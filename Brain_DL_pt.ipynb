{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from math import floor\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 1000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cpu')#cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour Utilities      ...       ScreenPorch PoolArea PoolQC Fence  \\\n",
       "0         Lvl    AllPub      ...                 0        0    NaN   NaN   \n",
       "1         Lvl    AllPub      ...                 0        0    NaN   NaN   \n",
       "2         Lvl    AllPub      ...                 0        0    NaN   NaN   \n",
       "3         Lvl    AllPub      ...                 0        0    NaN   NaN   \n",
       "4         Lvl    AllPub      ...                 0        0    NaN   NaN   \n",
       "\n",
       "  MiscFeature MiscVal MoSold  YrSold  SaleType  SaleCondition  \n",
       "0         NaN       0      2    2008        WD         Normal  \n",
       "1         NaN       0      5    2007        WD         Normal  \n",
       "2         NaN       0      9    2008        WD         Normal  \n",
       "3         NaN       0      2    2006        WD        Abnorml  \n",
       "4         NaN       0     12    2008        WD         Normal  \n",
       "\n",
       "[5 rows x 80 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sale_price = data['SalePrice']\n",
    "data = data.drop(['SalePrice'],axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id                  0\n",
       "MSSubClass          0\n",
       "MSZoning            0\n",
       "LotFrontage       259\n",
       "LotArea             0\n",
       "Street              0\n",
       "Alley            1369\n",
       "LotShape            0\n",
       "LandContour         0\n",
       "Utilities           0\n",
       "LotConfig           0\n",
       "LandSlope           0\n",
       "Neighborhood        0\n",
       "Condition1          0\n",
       "Condition2          0\n",
       "BldgType            0\n",
       "HouseStyle          0\n",
       "OverallQual         0\n",
       "OverallCond         0\n",
       "YearBuilt           0\n",
       "YearRemodAdd        0\n",
       "RoofStyle           0\n",
       "RoofMatl            0\n",
       "Exterior1st         0\n",
       "Exterior2nd         0\n",
       "MasVnrType          8\n",
       "MasVnrArea          8\n",
       "ExterQual           0\n",
       "ExterCond           0\n",
       "Foundation          0\n",
       "BsmtQual           37\n",
       "BsmtCond           37\n",
       "BsmtExposure       38\n",
       "BsmtFinType1       37\n",
       "BsmtFinSF1          0\n",
       "BsmtFinType2       38\n",
       "BsmtFinSF2          0\n",
       "BsmtUnfSF           0\n",
       "TotalBsmtSF         0\n",
       "Heating             0\n",
       "HeatingQC           0\n",
       "CentralAir          0\n",
       "Electrical          1\n",
       "1stFlrSF            0\n",
       "2ndFlrSF            0\n",
       "LowQualFinSF        0\n",
       "GrLivArea           0\n",
       "BsmtFullBath        0\n",
       "BsmtHalfBath        0\n",
       "FullBath            0\n",
       "HalfBath            0\n",
       "BedroomAbvGr        0\n",
       "KitchenAbvGr        0\n",
       "KitchenQual         0\n",
       "TotRmsAbvGrd        0\n",
       "Functional          0\n",
       "Fireplaces          0\n",
       "FireplaceQu       690\n",
       "GarageType         81\n",
       "GarageYrBlt        81\n",
       "GarageFinish       81\n",
       "GarageCars          0\n",
       "GarageArea          0\n",
       "GarageQual         81\n",
       "GarageCond         81\n",
       "PavedDrive          0\n",
       "WoodDeckSF          0\n",
       "OpenPorchSF         0\n",
       "EnclosedPorch       0\n",
       "3SsnPorch           0\n",
       "ScreenPorch         0\n",
       "PoolArea            0\n",
       "PoolQC           1453\n",
       "Fence            1179\n",
       "MiscFeature      1406\n",
       "MiscVal             0\n",
       "MoSold              0\n",
       "YrSold              0\n",
       "SaleType            0\n",
       "SaleCondition       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum() # Check number of NaN s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id                 int64\n",
       "MSSubClass         int64\n",
       "MSZoning          object\n",
       "LotFrontage      float64\n",
       "LotArea            int64\n",
       "Street            object\n",
       "Alley             object\n",
       "LotShape          object\n",
       "LandContour       object\n",
       "Utilities         object\n",
       "LotConfig         object\n",
       "LandSlope         object\n",
       "Neighborhood      object\n",
       "Condition1        object\n",
       "Condition2        object\n",
       "BldgType          object\n",
       "HouseStyle        object\n",
       "OverallQual        int64\n",
       "OverallCond        int64\n",
       "YearBuilt          int64\n",
       "YearRemodAdd       int64\n",
       "RoofStyle         object\n",
       "RoofMatl          object\n",
       "Exterior1st       object\n",
       "Exterior2nd       object\n",
       "MasVnrType        object\n",
       "MasVnrArea       float64\n",
       "ExterQual         object\n",
       "ExterCond         object\n",
       "Foundation        object\n",
       "BsmtQual          object\n",
       "BsmtCond          object\n",
       "BsmtExposure      object\n",
       "BsmtFinType1      object\n",
       "BsmtFinSF1         int64\n",
       "BsmtFinType2      object\n",
       "BsmtFinSF2         int64\n",
       "BsmtUnfSF          int64\n",
       "TotalBsmtSF        int64\n",
       "Heating           object\n",
       "HeatingQC         object\n",
       "CentralAir        object\n",
       "Electrical        object\n",
       "1stFlrSF           int64\n",
       "2ndFlrSF           int64\n",
       "LowQualFinSF       int64\n",
       "GrLivArea          int64\n",
       "BsmtFullBath       int64\n",
       "BsmtHalfBath       int64\n",
       "FullBath           int64\n",
       "HalfBath           int64\n",
       "BedroomAbvGr       int64\n",
       "KitchenAbvGr       int64\n",
       "KitchenQual       object\n",
       "TotRmsAbvGrd       int64\n",
       "Functional        object\n",
       "Fireplaces         int64\n",
       "FireplaceQu       object\n",
       "GarageType        object\n",
       "GarageYrBlt      float64\n",
       "GarageFinish      object\n",
       "GarageCars         int64\n",
       "GarageArea         int64\n",
       "GarageQual        object\n",
       "GarageCond        object\n",
       "PavedDrive        object\n",
       "WoodDeckSF         int64\n",
       "OpenPorchSF        int64\n",
       "EnclosedPorch      int64\n",
       "3SsnPorch          int64\n",
       "ScreenPorch        int64\n",
       "PoolArea           int64\n",
       "PoolQC            object\n",
       "Fence             object\n",
       "MiscFeature       object\n",
       "MiscVal            int64\n",
       "MoSold             int64\n",
       "YrSold             int64\n",
       "SaleType          object\n",
       "SaleCondition     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'MSSubClass', 'MSZoning', 'LotFrontage', 'LotArea', 'Street',\n",
       "       'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig',\n",
       "       'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType',\n",
       "       'HouseStyle', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd',\n",
       "       'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType',\n",
       "       'MasVnrArea', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual',\n",
       "       'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1',\n",
       "       'BsmtFinType2', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'Heating',\n",
       "       'HeatingQC', 'CentralAir', 'Electrical', '1stFlrSF', '2ndFlrSF',\n",
       "       'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath',\n",
       "       'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual',\n",
       "       'TotRmsAbvGrd', 'Functional', 'Fireplaces', 'FireplaceQu', 'GarageType',\n",
       "       'GarageYrBlt', 'GarageFinish', 'GarageCars', 'GarageArea', 'GarageQual',\n",
       "       'GarageCond', 'PavedDrive', 'WoodDeckSF', 'OpenPorchSF',\n",
       "       'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'PoolQC',\n",
       "       'Fence', 'MiscFeature', 'MiscVal', 'MoSold', 'YrSold', 'SaleType',\n",
       "       'SaleCondition'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MSZoning',\n",
       " 'Street',\n",
       " 'Alley',\n",
       " 'LotShape',\n",
       " 'LandContour',\n",
       " 'Utilities',\n",
       " 'LotConfig',\n",
       " 'LandSlope',\n",
       " 'Neighborhood',\n",
       " 'Condition1',\n",
       " 'Condition2',\n",
       " 'BldgType',\n",
       " 'HouseStyle',\n",
       " 'RoofStyle',\n",
       " 'RoofMatl',\n",
       " 'Exterior1st',\n",
       " 'Exterior2nd',\n",
       " 'MasVnrType',\n",
       " 'ExterQual',\n",
       " 'ExterCond',\n",
       " 'Foundation',\n",
       " 'BsmtQual',\n",
       " 'BsmtCond',\n",
       " 'BsmtExposure',\n",
       " 'BsmtFinType1',\n",
       " 'BsmtFinType2',\n",
       " 'Heating',\n",
       " 'HeatingQC',\n",
       " 'CentralAir',\n",
       " 'Electrical',\n",
       " 'KitchenQual',\n",
       " 'Functional',\n",
       " 'FireplaceQu',\n",
       " 'GarageType',\n",
       " 'GarageFinish',\n",
       " 'GarageQual',\n",
       " 'GarageCond',\n",
       " 'PavedDrive',\n",
       " 'PoolQC',\n",
       " 'Fence',\n",
       " 'MiscFeature',\n",
       " 'SaleType',\n",
       " 'SaleCondition']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_names = [i for i,v in (data.dtypes == object).items() if v]\n",
    "cat_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Id',\n",
       " 'MSSubClass',\n",
       " 'LotFrontage',\n",
       " 'LotArea',\n",
       " 'OverallQual',\n",
       " 'OverallCond',\n",
       " 'YearBuilt',\n",
       " 'YearRemodAdd',\n",
       " 'MasVnrArea',\n",
       " 'BsmtFinSF1',\n",
       " 'BsmtFinSF2',\n",
       " 'BsmtUnfSF',\n",
       " 'TotalBsmtSF',\n",
       " '1stFlrSF',\n",
       " '2ndFlrSF',\n",
       " 'LowQualFinSF',\n",
       " 'GrLivArea',\n",
       " 'BsmtFullBath',\n",
       " 'BsmtHalfBath',\n",
       " 'FullBath',\n",
       " 'HalfBath',\n",
       " 'BedroomAbvGr',\n",
       " 'KitchenAbvGr',\n",
       " 'TotRmsAbvGrd',\n",
       " 'Fireplaces',\n",
       " 'GarageYrBlt',\n",
       " 'GarageCars',\n",
       " 'GarageArea',\n",
       " 'WoodDeckSF',\n",
       " 'OpenPorchSF',\n",
       " 'EnclosedPorch',\n",
       " '3SsnPorch',\n",
       " 'ScreenPorch',\n",
       " 'PoolArea',\n",
       " 'MiscVal',\n",
       " 'MoSold',\n",
       " 'YrSold']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cont_names = [i for i,v in (data.dtypes == object).items() if not v]\n",
    "cont_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43, 37)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cat_names),len(cont_names)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id               int64\n",
       "MSSubClass       int64\n",
       "MSZoning        object\n",
       "LotFrontage    float64\n",
       "LotArea          int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in cat_names:\n",
    "    data[n] = data[n].astype('category').cat.as_ordered()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id                int64\n",
       "MSSubClass        int64\n",
       "MSZoning       category\n",
       "LotFrontage     float64\n",
       "LotArea           int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['C (all)', 'FV', 'RH', 'RL', 'RM'], dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['MSZoning'].cat.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       3\n",
       "1       3\n",
       "2       3\n",
       "3       3\n",
       "4       3\n",
       "5       3\n",
       "6       3\n",
       "7       3\n",
       "8       4\n",
       "9       3\n",
       "10      3\n",
       "11      3\n",
       "12      3\n",
       "13      3\n",
       "14      3\n",
       "15      4\n",
       "16      3\n",
       "17      3\n",
       "18      3\n",
       "19      3\n",
       "20      3\n",
       "21      4\n",
       "22      3\n",
       "23      4\n",
       "24      3\n",
       "25      3\n",
       "26      3\n",
       "27      3\n",
       "28      3\n",
       "29      4\n",
       "30      0\n",
       "31      3\n",
       "32      3\n",
       "33      3\n",
       "34      3\n",
       "35      3\n",
       "36      3\n",
       "37      3\n",
       "38      3\n",
       "39      3\n",
       "40      3\n",
       "41      3\n",
       "42      3\n",
       "43      3\n",
       "44      3\n",
       "45      3\n",
       "46      3\n",
       "47      1\n",
       "48      4\n",
       "49      3\n",
       "50      3\n",
       "51      4\n",
       "52      4\n",
       "53      3\n",
       "54      3\n",
       "55      3\n",
       "56      1\n",
       "57      3\n",
       "58      3\n",
       "59      3\n",
       "60      3\n",
       "61      4\n",
       "62      3\n",
       "63      4\n",
       "64      3\n",
       "65      3\n",
       "66      3\n",
       "67      3\n",
       "68      4\n",
       "69      3\n",
       "70      3\n",
       "71      3\n",
       "72      3\n",
       "73      3\n",
       "74      4\n",
       "75      4\n",
       "76      3\n",
       "77      4\n",
       "78      3\n",
       "79      4\n",
       "80      3\n",
       "81      4\n",
       "82      3\n",
       "83      3\n",
       "84      3\n",
       "85      3\n",
       "86      3\n",
       "87      1\n",
       "88      0\n",
       "89      3\n",
       "90      3\n",
       "91      3\n",
       "92      3\n",
       "93      0\n",
       "94      3\n",
       "95      3\n",
       "96      3\n",
       "97      3\n",
       "98      3\n",
       "99      3\n",
       "100     3\n",
       "101     3\n",
       "102     3\n",
       "103     3\n",
       "104     4\n",
       "105     1\n",
       "106     4\n",
       "107     4\n",
       "108     4\n",
       "109     3\n",
       "110     3\n",
       "111     3\n",
       "112     3\n",
       "113     3\n",
       "114     3\n",
       "115     1\n",
       "116     3\n",
       "117     3\n",
       "118     3\n",
       "119     3\n",
       "120     3\n",
       "121     4\n",
       "122     3\n",
       "123     3\n",
       "124     3\n",
       "125     4\n",
       "126     3\n",
       "127     4\n",
       "128     3\n",
       "129     3\n",
       "130     3\n",
       "131     3\n",
       "132     3\n",
       "133     3\n",
       "134     3\n",
       "135     3\n",
       "136     3\n",
       "137     3\n",
       "138     3\n",
       "139     3\n",
       "140     3\n",
       "141     3\n",
       "142     3\n",
       "143     3\n",
       "144     4\n",
       "145     4\n",
       "146     4\n",
       "147     3\n",
       "148     3\n",
       "149     4\n",
       "150     3\n",
       "151     3\n",
       "152     3\n",
       "153     3\n",
       "154     4\n",
       "155     3\n",
       "156     3\n",
       "157     3\n",
       "158     1\n",
       "159     3\n",
       "160     3\n",
       "161     3\n",
       "162     3\n",
       "163     3\n",
       "164     4\n",
       "165     3\n",
       "166     3\n",
       "167     3\n",
       "168     3\n",
       "169     3\n",
       "170     4\n",
       "171     3\n",
       "172     3\n",
       "173     3\n",
       "174     3\n",
       "175     3\n",
       "176     3\n",
       "177     3\n",
       "178     3\n",
       "179     4\n",
       "180     1\n",
       "181     3\n",
       "182     3\n",
       "183     4\n",
       "184     3\n",
       "185     4\n",
       "186     3\n",
       "187     3\n",
       "188     3\n",
       "189     3\n",
       "190     3\n",
       "191     3\n",
       "192     3\n",
       "193     4\n",
       "194     3\n",
       "195     3\n",
       "196     3\n",
       "197     3\n",
       "198     4\n",
       "199     3\n",
       "200     4\n",
       "201     3\n",
       "202     3\n",
       "203     4\n",
       "204     4\n",
       "205     3\n",
       "206     3\n",
       "207     3\n",
       "208     3\n",
       "209     3\n",
       "210     3\n",
       "211     3\n",
       "212     1\n",
       "213     3\n",
       "214     3\n",
       "215     3\n",
       "216     3\n",
       "217     4\n",
       "218     3\n",
       "219     3\n",
       "220     3\n",
       "221     3\n",
       "222     3\n",
       "223     3\n",
       "224     3\n",
       "225     4\n",
       "226     3\n",
       "227     4\n",
       "228     3\n",
       "229     3\n",
       "230     3\n",
       "231     3\n",
       "232     4\n",
       "233     3\n",
       "234     3\n",
       "235     4\n",
       "236     3\n",
       "237     3\n",
       "238     3\n",
       "239     3\n",
       "240     1\n",
       "241     4\n",
       "242     4\n",
       "243     3\n",
       "244     3\n",
       "245     3\n",
       "246     4\n",
       "247     3\n",
       "248     3\n",
       "249     3\n",
       "250     3\n",
       "251     4\n",
       "252     3\n",
       "253     3\n",
       "254     3\n",
       "255     3\n",
       "256     1\n",
       "257     3\n",
       "258     3\n",
       "259     4\n",
       "260     3\n",
       "261     3\n",
       "262     3\n",
       "263     4\n",
       "264     4\n",
       "265     3\n",
       "266     3\n",
       "267     3\n",
       "268     4\n",
       "269     3\n",
       "270     1\n",
       "271     3\n",
       "272     3\n",
       "273     3\n",
       "274     3\n",
       "275     3\n",
       "276     3\n",
       "277     3\n",
       "278     3\n",
       "279     3\n",
       "280     3\n",
       "281     1\n",
       "282     3\n",
       "283     3\n",
       "284     3\n",
       "285     1\n",
       "286     3\n",
       "287     3\n",
       "288     3\n",
       "289     3\n",
       "290     3\n",
       "291     3\n",
       "292     3\n",
       "293     3\n",
       "294     3\n",
       "295     3\n",
       "296     4\n",
       "297     1\n",
       "298     3\n",
       "299     3\n",
       "300     3\n",
       "301     3\n",
       "302     3\n",
       "303     3\n",
       "304     4\n",
       "305     3\n",
       "306     3\n",
       "307     4\n",
       "308     3\n",
       "309     3\n",
       "310     3\n",
       "311     3\n",
       "312     4\n",
       "313     3\n",
       "314     4\n",
       "315     3\n",
       "316     3\n",
       "317     1\n",
       "318     3\n",
       "319     3\n",
       "320     3\n",
       "321     3\n",
       "322     3\n",
       "323     4\n",
       "324     3\n",
       "325     4\n",
       "326     3\n",
       "327     3\n",
       "328     3\n",
       "329     4\n",
       "330     3\n",
       "331     3\n",
       "332     3\n",
       "333     4\n",
       "334     3\n",
       "335     3\n",
       "336     3\n",
       "337     3\n",
       "338     3\n",
       "339     3\n",
       "340     3\n",
       "341     2\n",
       "342     3\n",
       "343     3\n",
       "344     4\n",
       "345     3\n",
       "346     3\n",
       "347     3\n",
       "348     3\n",
       "349     3\n",
       "350     3\n",
       "351     3\n",
       "352     3\n",
       "353     4\n",
       "354     3\n",
       "355     3\n",
       "356     3\n",
       "357     4\n",
       "358     3\n",
       "359     3\n",
       "360     3\n",
       "361     3\n",
       "362     3\n",
       "363     4\n",
       "364     3\n",
       "365     4\n",
       "366     3\n",
       "367     3\n",
       "368     3\n",
       "369     3\n",
       "370     3\n",
       "371     3\n",
       "372     3\n",
       "373     3\n",
       "374     3\n",
       "375     3\n",
       "376     3\n",
       "377     1\n",
       "378     3\n",
       "379     3\n",
       "380     3\n",
       "381     1\n",
       "382     3\n",
       "383     2\n",
       "384     3\n",
       "385     3\n",
       "386     3\n",
       "387     3\n",
       "388     3\n",
       "389     3\n",
       "390     3\n",
       "391     3\n",
       "392     3\n",
       "393     3\n",
       "394     3\n",
       "395     3\n",
       "396     3\n",
       "397     3\n",
       "398     4\n",
       "399     1\n",
       "400     3\n",
       "401     3\n",
       "402     3\n",
       "403     3\n",
       "404     3\n",
       "405     3\n",
       "406     3\n",
       "407     3\n",
       "408     3\n",
       "409     1\n",
       "410     3\n",
       "411     3\n",
       "412     1\n",
       "413     4\n",
       "414     3\n",
       "415     3\n",
       "416     3\n",
       "417     3\n",
       "418     3\n",
       "419     3\n",
       "420     4\n",
       "421     3\n",
       "422     3\n",
       "423     3\n",
       "424     3\n",
       "425     4\n",
       "426     3\n",
       "427     3\n",
       "428     3\n",
       "429     3\n",
       "430     4\n",
       "431     4\n",
       "432     4\n",
       "433     3\n",
       "434     4\n",
       "435     3\n",
       "436     4\n",
       "437     4\n",
       "438     3\n",
       "439     3\n",
       "440     3\n",
       "441     3\n",
       "442     4\n",
       "443     3\n",
       "444     3\n",
       "445     3\n",
       "446     3\n",
       "447     3\n",
       "448     4\n",
       "449     4\n",
       "450     4\n",
       "451     3\n",
       "452     3\n",
       "453     1\n",
       "454     3\n",
       "455     3\n",
       "456     4\n",
       "457     3\n",
       "458     4\n",
       "459     3\n",
       "460     1\n",
       "461     3\n",
       "462     3\n",
       "463     3\n",
       "464     3\n",
       "465     4\n",
       "466     3\n",
       "467     3\n",
       "468     3\n",
       "469     3\n",
       "470     3\n",
       "471     3\n",
       "472     4\n",
       "473     3\n",
       "474     3\n",
       "475     3\n",
       "476     3\n",
       "477     3\n",
       "478     3\n",
       "479     4\n",
       "480     3\n",
       "481     3\n",
       "482     4\n",
       "483     4\n",
       "484     3\n",
       "485     3\n",
       "486     3\n",
       "487     3\n",
       "488     3\n",
       "489     4\n",
       "490     4\n",
       "491     3\n",
       "492     3\n",
       "493     3\n",
       "494     4\n",
       "495     0\n",
       "496     3\n",
       "497     3\n",
       "498     3\n",
       "499     3\n",
       "       ..\n",
       "960     3\n",
       "961     3\n",
       "962     3\n",
       "963     3\n",
       "964     3\n",
       "965     3\n",
       "966     3\n",
       "967     3\n",
       "968     4\n",
       "969     3\n",
       "970     3\n",
       "971     3\n",
       "972     3\n",
       "973     1\n",
       "974     3\n",
       "975     1\n",
       "976     3\n",
       "977     1\n",
       "978     3\n",
       "979     3\n",
       "980     3\n",
       "981     3\n",
       "982     3\n",
       "983     3\n",
       "984     3\n",
       "985     3\n",
       "986     4\n",
       "987     3\n",
       "988     3\n",
       "989     1\n",
       "990     3\n",
       "991     4\n",
       "992     3\n",
       "993     3\n",
       "994     3\n",
       "995     3\n",
       "996     3\n",
       "997     3\n",
       "998     4\n",
       "999     3\n",
       "1000    3\n",
       "1001    3\n",
       "1002    3\n",
       "1003    3\n",
       "1004    3\n",
       "1005    3\n",
       "1006    3\n",
       "1007    4\n",
       "1008    3\n",
       "1009    3\n",
       "1010    3\n",
       "1011    3\n",
       "1012    3\n",
       "1013    4\n",
       "1014    3\n",
       "1015    3\n",
       "1016    3\n",
       "1017    3\n",
       "1018    3\n",
       "1019    3\n",
       "1020    3\n",
       "1021    3\n",
       "1022    4\n",
       "1023    3\n",
       "1024    3\n",
       "1025    3\n",
       "1026    3\n",
       "1027    3\n",
       "1028    3\n",
       "1029    4\n",
       "1030    2\n",
       "1031    3\n",
       "1032    3\n",
       "1033    3\n",
       "1034    3\n",
       "1035    3\n",
       "1036    3\n",
       "1037    3\n",
       "1038    4\n",
       "1039    4\n",
       "1040    3\n",
       "1041    3\n",
       "1042    3\n",
       "1043    3\n",
       "1044    3\n",
       "1045    3\n",
       "1046    3\n",
       "1047    3\n",
       "1048    3\n",
       "1049    3\n",
       "1050    3\n",
       "1051    3\n",
       "1052    3\n",
       "1053    3\n",
       "1054    3\n",
       "1055    3\n",
       "1056    3\n",
       "1057    3\n",
       "1058    3\n",
       "1059    3\n",
       "1060    3\n",
       "1061    0\n",
       "1062    4\n",
       "1063    4\n",
       "1064    3\n",
       "1065    3\n",
       "1066    3\n",
       "1067    3\n",
       "1068    4\n",
       "1069    3\n",
       "1070    3\n",
       "1071    3\n",
       "1072    3\n",
       "1073    3\n",
       "1074    3\n",
       "1075    3\n",
       "1076    3\n",
       "1077    3\n",
       "1078    4\n",
       "1079    3\n",
       "1080    3\n",
       "1081    3\n",
       "1082    3\n",
       "1083    3\n",
       "1084    3\n",
       "1085    3\n",
       "1086    4\n",
       "1087    1\n",
       "1088    4\n",
       "1089    1\n",
       "1090    3\n",
       "1091    1\n",
       "1092    3\n",
       "1093    3\n",
       "1094    3\n",
       "1095    3\n",
       "1096    4\n",
       "1097    3\n",
       "1098    4\n",
       "1099    3\n",
       "1100    3\n",
       "1101    3\n",
       "1102    3\n",
       "1103    3\n",
       "1104    4\n",
       "1105    3\n",
       "1106    3\n",
       "1107    3\n",
       "1108    3\n",
       "1109    3\n",
       "1110    3\n",
       "1111    3\n",
       "1112    3\n",
       "1113    3\n",
       "1114    3\n",
       "1115    3\n",
       "1116    3\n",
       "1117    3\n",
       "1118    3\n",
       "1119    3\n",
       "1120    4\n",
       "1121    3\n",
       "1122    3\n",
       "1123    3\n",
       "1124    3\n",
       "1125    3\n",
       "1126    3\n",
       "1127    3\n",
       "1128    3\n",
       "1129    4\n",
       "1130    3\n",
       "1131    3\n",
       "1132    4\n",
       "1133    3\n",
       "1134    3\n",
       "1135    4\n",
       "1136    3\n",
       "1137    3\n",
       "1138    3\n",
       "1139    3\n",
       "1140    3\n",
       "1141    3\n",
       "1142    3\n",
       "1143    3\n",
       "1144    3\n",
       "1145    4\n",
       "1146    3\n",
       "1147    3\n",
       "1148    4\n",
       "1149    4\n",
       "1150    3\n",
       "1151    3\n",
       "1152    3\n",
       "1153    4\n",
       "1154    3\n",
       "1155    3\n",
       "1156    3\n",
       "1157    3\n",
       "1158    3\n",
       "1159    3\n",
       "1160    3\n",
       "1161    3\n",
       "1162    3\n",
       "1163    3\n",
       "1164    3\n",
       "1165    3\n",
       "1166    3\n",
       "1167    3\n",
       "1168    3\n",
       "1169    3\n",
       "1170    3\n",
       "1171    3\n",
       "1172    1\n",
       "1173    3\n",
       "1174    3\n",
       "1175    3\n",
       "1176    3\n",
       "1177    4\n",
       "1178    3\n",
       "1179    3\n",
       "1180    3\n",
       "1181    4\n",
       "1182    3\n",
       "1183    3\n",
       "1184    3\n",
       "1185    3\n",
       "1186    3\n",
       "1187    3\n",
       "1188    3\n",
       "1189    3\n",
       "1190    3\n",
       "1191    1\n",
       "1192    4\n",
       "1193    4\n",
       "1194    3\n",
       "1195    3\n",
       "1196    3\n",
       "1197    4\n",
       "1198    3\n",
       "1199    3\n",
       "1200    3\n",
       "1201    3\n",
       "1202    4\n",
       "1203    3\n",
       "1204    3\n",
       "1205    3\n",
       "1206    2\n",
       "1207    3\n",
       "1208    3\n",
       "1209    3\n",
       "1210    3\n",
       "1211    3\n",
       "1212    3\n",
       "1213    3\n",
       "1214    3\n",
       "1215    3\n",
       "1216    4\n",
       "1217    1\n",
       "1218    4\n",
       "1219    4\n",
       "1220    3\n",
       "1221    3\n",
       "1222    3\n",
       "1223    3\n",
       "1224    3\n",
       "1225    3\n",
       "1226    3\n",
       "1227    3\n",
       "1228    3\n",
       "1229    3\n",
       "1230    3\n",
       "1231    3\n",
       "1232    3\n",
       "1233    3\n",
       "1234    2\n",
       "1235    3\n",
       "1236    3\n",
       "1237    3\n",
       "1238    3\n",
       "1239    3\n",
       "1240    3\n",
       "1241    3\n",
       "1242    3\n",
       "1243    3\n",
       "1244    3\n",
       "1245    3\n",
       "1246    1\n",
       "1247    3\n",
       "1248    4\n",
       "1249    3\n",
       "1250    3\n",
       "1251    3\n",
       "1252    3\n",
       "1253    3\n",
       "1254    3\n",
       "1255    4\n",
       "1256    3\n",
       "1257    3\n",
       "1258    3\n",
       "1259    3\n",
       "1260    3\n",
       "1261    3\n",
       "1262    3\n",
       "1263    3\n",
       "1264    2\n",
       "1265    1\n",
       "1266    4\n",
       "1267    3\n",
       "1268    3\n",
       "1269    3\n",
       "1270    3\n",
       "1271    3\n",
       "1272    3\n",
       "1273    3\n",
       "1274    3\n",
       "1275    3\n",
       "1276    3\n",
       "1277    3\n",
       "1278    3\n",
       "1279    0\n",
       "1280    3\n",
       "1281    3\n",
       "1282    3\n",
       "1283    3\n",
       "1284    3\n",
       "1285    4\n",
       "1286    3\n",
       "1287    3\n",
       "1288    3\n",
       "1289    3\n",
       "1290    3\n",
       "1291    4\n",
       "1292    4\n",
       "1293    3\n",
       "1294    3\n",
       "1295    3\n",
       "1296    3\n",
       "1297    4\n",
       "1298    3\n",
       "1299    3\n",
       "1300    3\n",
       "1301    3\n",
       "1302    3\n",
       "1303    3\n",
       "1304    4\n",
       "1305    3\n",
       "1306    3\n",
       "1307    3\n",
       "1308    4\n",
       "1309    3\n",
       "1310    3\n",
       "1311    3\n",
       "1312    3\n",
       "1313    3\n",
       "1314    3\n",
       "1315    3\n",
       "1316    3\n",
       "1317    1\n",
       "1318    3\n",
       "1319    3\n",
       "1320    3\n",
       "1321    3\n",
       "1322    3\n",
       "1323    3\n",
       "1324    3\n",
       "1325    4\n",
       "1326    2\n",
       "1327    3\n",
       "1328    4\n",
       "1329    3\n",
       "1330    3\n",
       "1331    3\n",
       "1332    3\n",
       "1333    4\n",
       "1334    4\n",
       "1335    3\n",
       "1336    3\n",
       "1337    4\n",
       "1338    3\n",
       "1339    3\n",
       "1340    3\n",
       "1341    3\n",
       "1342    3\n",
       "1343    3\n",
       "1344    3\n",
       "1345    4\n",
       "1346    3\n",
       "1347    3\n",
       "1348    3\n",
       "1349    4\n",
       "1350    3\n",
       "1351    3\n",
       "1352    4\n",
       "1353    3\n",
       "1354    3\n",
       "1355    3\n",
       "1356    3\n",
       "1357    3\n",
       "1358    1\n",
       "1359    3\n",
       "1360    3\n",
       "1361    3\n",
       "1362    3\n",
       "1363    3\n",
       "1364    1\n",
       "1365    1\n",
       "1366    3\n",
       "1367    4\n",
       "1368    4\n",
       "1369    3\n",
       "1370    3\n",
       "1371    3\n",
       "1372    3\n",
       "1373    3\n",
       "1374    1\n",
       "1375    3\n",
       "1376    3\n",
       "1377    3\n",
       "1378    4\n",
       "1379    3\n",
       "1380    3\n",
       "1381    3\n",
       "1382    4\n",
       "1383    3\n",
       "1384    3\n",
       "1385    4\n",
       "1386    3\n",
       "1387    4\n",
       "1388    3\n",
       "1389    4\n",
       "1390    3\n",
       "1391    3\n",
       "1392    3\n",
       "1393    4\n",
       "1394    3\n",
       "1395    3\n",
       "1396    3\n",
       "1397    4\n",
       "1398    3\n",
       "1399    3\n",
       "1400    4\n",
       "1401    3\n",
       "1402    3\n",
       "1403    3\n",
       "1404    3\n",
       "1405    4\n",
       "1406    3\n",
       "1407    3\n",
       "1408    4\n",
       "1409    3\n",
       "1410    3\n",
       "1411    3\n",
       "1412    3\n",
       "1413    3\n",
       "1414    3\n",
       "1415    3\n",
       "1416    4\n",
       "1417    3\n",
       "1418    3\n",
       "1419    3\n",
       "1420    3\n",
       "1421    3\n",
       "1422    4\n",
       "1423    3\n",
       "1424    3\n",
       "1425    3\n",
       "1426    3\n",
       "1427    3\n",
       "1428    4\n",
       "1429    3\n",
       "1430    3\n",
       "1431    3\n",
       "1432    3\n",
       "1433    3\n",
       "1434    3\n",
       "1435    3\n",
       "1436    3\n",
       "1437    3\n",
       "1438    4\n",
       "1439    3\n",
       "1440    3\n",
       "1441    4\n",
       "1442    1\n",
       "1443    3\n",
       "1444    3\n",
       "1445    3\n",
       "1446    3\n",
       "1447    3\n",
       "1448    3\n",
       "1449    4\n",
       "1450    3\n",
       "1451    3\n",
       "1452    4\n",
       "1453    3\n",
       "1454    1\n",
       "1455    3\n",
       "1456    3\n",
       "1457    3\n",
       "1458    3\n",
       "1459    3\n",
       "Length: 1460, dtype: int8"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['MSZoning'].cat.codes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in cont_names:\n",
    "    if pd.isnull(data[n]).sum():\n",
    "        filler = data[n].median()\n",
    "        data[n] = data[n].fillna(filler)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sale_price = torch.tensor(np.log(sale_price).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([12.2477, 12.1090, 12.3172,  ..., 12.4931, 11.8645, 11.9016],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sale_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cat_names and len(cat_names) >= 1: # categorical data\n",
    "    cats = np.stack([c.cat.codes.values for n,c in data[cat_names].items()], 1) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 43)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = torch.LongTensor(cats.astype(np.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4, 2, 0,  ..., 0, 9, 5],\n",
       "        [4, 2, 0,  ..., 0, 9, 5],\n",
       "        [4, 2, 0,  ..., 0, 9, 5],\n",
       "        ...,\n",
       "        [4, 2, 0,  ..., 3, 9, 5],\n",
       "        [4, 2, 0,  ..., 0, 9, 5],\n",
       "        [4, 2, 0,  ..., 0, 9, 5]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cats"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cont_names and len(cont_names) >= 1:\n",
    "    conts = np.stack([c.astype('float32').values for n,c in data[cont_names].items()], 1)\n",
    "    means, stds = (conts.mean(0), conts.std(0))\n",
    "    conts = (conts - means[None]) / stds[None]\n",
    "    stats = means,stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 37)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "conts = torch.FloatTensor(conts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.7309,  0.0734, -0.2209,  ..., -0.0877, -1.5991,  0.1387],\n",
       "        [-1.7285, -0.8726,  0.4603,  ..., -0.0877, -0.4891, -0.6145],\n",
       "        [-1.7261,  0.0734, -0.0846,  ..., -0.0877,  0.9909,  0.1387],\n",
       "        ...,\n",
       "        [ 1.7261,  0.3099, -0.1755,  ...,  4.9531, -0.4891,  1.6452],\n",
       "        [ 1.7285, -0.8726, -0.0846,  ..., -0.0877, -0.8591,  1.6452],\n",
       "        [ 1.7309, -0.8726,  0.2333,  ..., -0.0877, -0.1191,  0.1387]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6, 3), (3, 2), (3, 2), (5, 3), (5, 3), (3, 2), (6, 3), (4, 2), (26, 13), (10, 5), (9, 5), (6, 3), (9, 5), (7, 4), (9, 5), (16, 8), (17, 9), (5, 3), (5, 3), (6, 3), (7, 4), (5, 3), (5, 3), (5, 3), (7, 4), (7, 4), (7, 4), (6, 3), (3, 2), (6, 3), (5, 3), (8, 4), (6, 3), (7, 4), (4, 2), (6, 3), (6, 3), (4, 2), (4, 2), (5, 3), (5, 3), (10, 5), (7, 4)]\n"
     ]
    }
   ],
   "source": [
    "cat_szs = [len(data[n].cat.categories)+1 for n in cat_names]\n",
    "emb_szs = [(c, min(50, (c+1)//2)) for c in cat_szs]\n",
    "print(emb_szs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data_conts = conts\n",
    "x_data_cats  = cats\n",
    "y_data = sale_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1460, 37])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data_conts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1460, 43])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data_cats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1460])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_cats: torch.Size([1168, 43])\n",
      "x_train_conts: torch.Size([1168, 37])\n",
      "y_train: torch.Size([1168])\n",
      "x_valid_cats: torch.Size([73, 43])\n",
      "x_valid_conts: torch.Size([73, 37])\n",
      "y_valid: torch.Size([73])\n",
      "x_test_cats: torch.Size([219, 43])\n",
      "x_test_conts: torch.Size([219, 37])\n",
      "y_test: torch.Size([219])\n"
     ]
    }
   ],
   "source": [
    "train_size = 0.8\n",
    "valid_size = 0.25\n",
    "\n",
    "train_cnt = floor(x_data_cats.shape[0] * train_size)\n",
    "\n",
    "x_train_cats  = x_data_cats[0:train_cnt]\n",
    "x_train_conts = x_data_conts[0:train_cnt]\n",
    "y_train = y_data[0:train_cnt]\n",
    "\n",
    "valid_cnt = floor((x_data_cats.shape[0] - train_cnt) * valid_size)\n",
    "\n",
    "x_valid_cats  = x_data_cats[train_cnt:train_cnt+valid_cnt]\n",
    "x_valid_conts = x_data_conts[train_cnt:train_cnt+valid_cnt]\n",
    "y_valid = y_data[train_cnt:train_cnt+valid_cnt]\n",
    "\n",
    "x_test_cats  = x_data_cats[train_cnt+valid_cnt:]\n",
    "x_test_conts = x_data_conts[train_cnt+valid_cnt:]\n",
    "y_test = y_data[train_cnt+valid_cnt:]\n",
    "\n",
    "print(\"x_train_cats:\",x_train_cats.shape)\n",
    "print(\"x_train_conts:\",x_train_conts.shape)\n",
    "print(\"y_train:\",y_train.shape)\n",
    "print(\"x_valid_cats:\",x_valid_cats.shape)\n",
    "print(\"x_valid_conts:\",x_valid_conts.shape)\n",
    "print(\"y_valid:\",y_valid.shape)\n",
    "print(\"x_test_cats:\",x_test_cats.shape)\n",
    "print(\"x_test_conts:\",x_test_conts.shape)\n",
    "print(\"y_test:\",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Parameters\n",
    "learning_rate = 1e-2\n",
    "epochs = 500\n",
    "batch_size = 64\n",
    "num_input = x_data_conts.shape[1]\n",
    "num_classes = 1\n",
    "num_hidden_1 = 100\n",
    "num_hidden_2 = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, num_classes, emb_szs, n_cont):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embeds = nn.ModuleList([nn.Embedding(ne,de) for ne,de in emb_szs])\n",
    "        n_emb = sum(e.embedding_dim for e in self.embeds)\n",
    "        self.n_emb,self.n_cont = n_emb,n_cont\n",
    "        \n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(self.n_emb+self.n_cont,num_hidden_1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(num_hidden_1),\n",
    "            nn.Dropout())\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(num_hidden_1,num_hidden_2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(num_hidden_2),\n",
    "            nn.Dropout())\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Linear(num_hidden_2,num_classes))\n",
    "        \n",
    "    def forward(self, x_cat, x_cont):\n",
    "        x = [e(x_cat[:,i]) for i,e in enumerate(self.embeds)]\n",
    "        x = torch.cat(x, 1)\n",
    "        x = torch.cat([x, x_cont], 1)\n",
    "        \n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(num_classes,emb_szs=emb_szs,n_cont=len(cont_names)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.utils.data.TensorDataset(x_train_conts,x_train_cats,y_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_data,batch_size=batch_size,shuffle=False)\n",
    "\n",
    "valid_data = torch.utils.data.TensorDataset(x_valid_conts,x_valid_cats,y_valid)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_data,batch_size=batch_size,shuffle=False)\n",
    "\n",
    "test_data = torch.utils.data.TensorDataset(x_test_conts,x_test_cats,y_test)\n",
    "test_loader = torch.utils.data.DataLoader(test_data,batch_size=batch_size,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs = []\n",
    "lrn_rate_sizes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], Step [1/18], Train_Loss: 145.2092, Valid_Loss: 145.5917\n",
      "Epoch [2/500], Step [1/18], Train_Loss: 80.2482, Valid_Loss: 81.0460\n",
      "Epoch [3/500], Step [1/18], Train_Loss: 5.4170, Valid_Loss: 5.4034\n",
      "Epoch [4/500], Step [1/18], Train_Loss: 3.1154, Valid_Loss: 2.9687\n",
      "Epoch [5/500], Step [1/18], Train_Loss: 2.7676, Valid_Loss: 3.0302\n",
      "Epoch [6/500], Step [1/18], Train_Loss: 1.9612, Valid_Loss: 2.3825\n",
      "Epoch [7/500], Step [1/18], Train_Loss: 1.5872, Valid_Loss: 1.7581\n",
      "Epoch [8/500], Step [1/18], Train_Loss: 1.6965, Valid_Loss: 1.6258\n",
      "Epoch [9/500], Step [1/18], Train_Loss: 2.4291, Valid_Loss: 2.6362\n",
      "Epoch [10/500], Step [1/18], Train_Loss: 2.4483, Valid_Loss: 2.5369\n",
      "Epoch [11/500], Step [1/18], Train_Loss: 1.6076, Valid_Loss: 1.6210\n",
      "Epoch [12/500], Step [1/18], Train_Loss: 1.8764, Valid_Loss: 1.9961\n",
      "Epoch [13/500], Step [1/18], Train_Loss: 2.0681, Valid_Loss: 2.4165\n",
      "Epoch [14/500], Step [1/18], Train_Loss: 1.5864, Valid_Loss: 1.6967\n",
      "Epoch [15/500], Step [1/18], Train_Loss: 1.4168, Valid_Loss: 1.6515\n",
      "Epoch [16/500], Step [1/18], Train_Loss: 1.2975, Valid_Loss: 1.3975\n",
      "Epoch [17/500], Step [1/18], Train_Loss: 1.7322, Valid_Loss: 1.7812\n",
      "Epoch [18/500], Step [1/18], Train_Loss: 1.7081, Valid_Loss: 1.8899\n",
      "Epoch [19/500], Step [1/18], Train_Loss: 1.2787, Valid_Loss: 1.6307\n",
      "Epoch [20/500], Step [1/18], Train_Loss: 1.3818, Valid_Loss: 1.9056\n",
      "Epoch [21/500], Step [1/18], Train_Loss: 1.3024, Valid_Loss: 1.5386\n",
      "Epoch [22/500], Step [1/18], Train_Loss: 1.2024, Valid_Loss: 1.2312\n",
      "Epoch [23/500], Step [1/18], Train_Loss: 1.3771, Valid_Loss: 1.5891\n",
      "Epoch [24/500], Step [1/18], Train_Loss: 1.4978, Valid_Loss: 1.5705\n",
      "Epoch [25/500], Step [1/18], Train_Loss: 1.6656, Valid_Loss: 1.7102\n",
      "Epoch [26/500], Step [1/18], Train_Loss: 1.5114, Valid_Loss: 1.6832\n",
      "Epoch [27/500], Step [1/18], Train_Loss: 0.9800, Valid_Loss: 1.3730\n",
      "Epoch [28/500], Step [1/18], Train_Loss: 1.4453, Valid_Loss: 1.7309\n",
      "Epoch [29/500], Step [1/18], Train_Loss: 0.9456, Valid_Loss: 1.2257\n",
      "Epoch [30/500], Step [1/18], Train_Loss: 1.2327, Valid_Loss: 1.1794\n",
      "Epoch [31/500], Step [1/18], Train_Loss: 1.0393, Valid_Loss: 1.1513\n",
      "Epoch [32/500], Step [1/18], Train_Loss: 1.1821, Valid_Loss: 1.3260\n",
      "Epoch [33/500], Step [1/18], Train_Loss: 1.4421, Valid_Loss: 1.8252\n",
      "Epoch [34/500], Step [1/18], Train_Loss: 1.4853, Valid_Loss: 1.4456\n",
      "Epoch [35/500], Step [1/18], Train_Loss: 1.4722, Valid_Loss: 1.5295\n",
      "Epoch [36/500], Step [1/18], Train_Loss: 1.0319, Valid_Loss: 1.2745\n",
      "Epoch [37/500], Step [1/18], Train_Loss: 1.4404, Valid_Loss: 1.5957\n",
      "Epoch [38/500], Step [1/18], Train_Loss: 1.4363, Valid_Loss: 2.0666\n",
      "Epoch [39/500], Step [1/18], Train_Loss: 1.0621, Valid_Loss: 1.2962\n",
      "Epoch [40/500], Step [1/18], Train_Loss: 1.7503, Valid_Loss: 2.0459\n",
      "Epoch [41/500], Step [1/18], Train_Loss: 0.9836, Valid_Loss: 1.2782\n",
      "Epoch [42/500], Step [1/18], Train_Loss: 1.2509, Valid_Loss: 1.1252\n",
      "Epoch [43/500], Step [1/18], Train_Loss: 1.2149, Valid_Loss: 1.3335\n",
      "Epoch [44/500], Step [1/18], Train_Loss: 1.3143, Valid_Loss: 1.6167\n",
      "Epoch [45/500], Step [1/18], Train_Loss: 1.0600, Valid_Loss: 1.3075\n",
      "Epoch [46/500], Step [1/18], Train_Loss: 0.9173, Valid_Loss: 0.8817\n",
      "Epoch [47/500], Step [1/18], Train_Loss: 1.6898, Valid_Loss: 1.9793\n",
      "Epoch [48/500], Step [1/18], Train_Loss: 1.3980, Valid_Loss: 1.5442\n",
      "Epoch [49/500], Step [1/18], Train_Loss: 1.2971, Valid_Loss: 1.4706\n",
      "Epoch [50/500], Step [1/18], Train_Loss: 1.1931, Valid_Loss: 1.6132\n",
      "Epoch [51/500], Step [1/18], Train_Loss: 1.3977, Valid_Loss: 1.4992\n",
      "Epoch [52/500], Step [1/18], Train_Loss: 1.2779, Valid_Loss: 1.5813\n",
      "Epoch [53/500], Step [1/18], Train_Loss: 1.4195, Valid_Loss: 1.9537\n",
      "Epoch [54/500], Step [1/18], Train_Loss: 1.1508, Valid_Loss: 1.5265\n",
      "Epoch [55/500], Step [1/18], Train_Loss: 1.0933, Valid_Loss: 1.4582\n",
      "Epoch [56/500], Step [1/18], Train_Loss: 1.0627, Valid_Loss: 1.2756\n",
      "Epoch [57/500], Step [1/18], Train_Loss: 1.4028, Valid_Loss: 1.3228\n",
      "Epoch [58/500], Step [1/18], Train_Loss: 1.3666, Valid_Loss: 1.6142\n",
      "Epoch [59/500], Step [1/18], Train_Loss: 0.9405, Valid_Loss: 1.3609\n",
      "Epoch [60/500], Step [1/18], Train_Loss: 1.3693, Valid_Loss: 1.6525\n",
      "Epoch [61/500], Step [1/18], Train_Loss: 1.2033, Valid_Loss: 1.4785\n",
      "Epoch [62/500], Step [1/18], Train_Loss: 1.5418, Valid_Loss: 1.6817\n",
      "Epoch [63/500], Step [1/18], Train_Loss: 1.5794, Valid_Loss: 1.7369\n",
      "Epoch [64/500], Step [1/18], Train_Loss: 1.4410, Valid_Loss: 1.7550\n",
      "Epoch [65/500], Step [1/18], Train_Loss: 1.2722, Valid_Loss: 1.6818\n",
      "Epoch [66/500], Step [1/18], Train_Loss: 1.5284, Valid_Loss: 1.8622\n",
      "Epoch [67/500], Step [1/18], Train_Loss: 1.4205, Valid_Loss: 1.6529\n",
      "Epoch [68/500], Step [1/18], Train_Loss: 1.2706, Valid_Loss: 1.3744\n",
      "Epoch [69/500], Step [1/18], Train_Loss: 1.2813, Valid_Loss: 1.8839\n",
      "Epoch [70/500], Step [1/18], Train_Loss: 1.1953, Valid_Loss: 1.2335\n",
      "Epoch [71/500], Step [1/18], Train_Loss: 1.4348, Valid_Loss: 1.5096\n",
      "Epoch [72/500], Step [1/18], Train_Loss: 1.0356, Valid_Loss: 1.1669\n",
      "Epoch [73/500], Step [1/18], Train_Loss: 1.4198, Valid_Loss: 1.6886\n",
      "Epoch [74/500], Step [1/18], Train_Loss: 0.8355, Valid_Loss: 1.1414\n",
      "Epoch [75/500], Step [1/18], Train_Loss: 0.7464, Valid_Loss: 0.9010\n",
      "Epoch [76/500], Step [1/18], Train_Loss: 1.0678, Valid_Loss: 1.2866\n",
      "Epoch [77/500], Step [1/18], Train_Loss: 1.5070, Valid_Loss: 1.7202\n",
      "Epoch [78/500], Step [1/18], Train_Loss: 1.2865, Valid_Loss: 1.6237\n",
      "Epoch [79/500], Step [1/18], Train_Loss: 1.0124, Valid_Loss: 1.0501\n",
      "Epoch [80/500], Step [1/18], Train_Loss: 1.0738, Valid_Loss: 1.2276\n",
      "Epoch [81/500], Step [1/18], Train_Loss: 0.9582, Valid_Loss: 1.3638\n",
      "Epoch [82/500], Step [1/18], Train_Loss: 0.9091, Valid_Loss: 1.2651\n",
      "Epoch [83/500], Step [1/18], Train_Loss: 1.3245, Valid_Loss: 1.9619\n",
      "Epoch [84/500], Step [1/18], Train_Loss: 1.2780, Valid_Loss: 1.1968\n",
      "Epoch [85/500], Step [1/18], Train_Loss: 1.3701, Valid_Loss: 1.4727\n",
      "Epoch [86/500], Step [1/18], Train_Loss: 1.1702, Valid_Loss: 1.5196\n",
      "Epoch [87/500], Step [1/18], Train_Loss: 1.2306, Valid_Loss: 1.5047\n",
      "Epoch [88/500], Step [1/18], Train_Loss: 1.4926, Valid_Loss: 2.1566\n",
      "Epoch [89/500], Step [1/18], Train_Loss: 0.9173, Valid_Loss: 1.1692\n",
      "Epoch [90/500], Step [1/18], Train_Loss: 1.2626, Valid_Loss: 1.7643\n",
      "Epoch [91/500], Step [1/18], Train_Loss: 0.9756, Valid_Loss: 1.2179\n",
      "Epoch [92/500], Step [1/18], Train_Loss: 1.0582, Valid_Loss: 1.2265\n",
      "Epoch [93/500], Step [1/18], Train_Loss: 1.3694, Valid_Loss: 1.7895\n",
      "Epoch [94/500], Step [1/18], Train_Loss: 1.1908, Valid_Loss: 1.1784\n",
      "Epoch [95/500], Step [1/18], Train_Loss: 1.0029, Valid_Loss: 1.2811\n",
      "Epoch [96/500], Step [1/18], Train_Loss: 1.0336, Valid_Loss: 1.2548\n",
      "Epoch [97/500], Step [1/18], Train_Loss: 0.8653, Valid_Loss: 1.0933\n",
      "Epoch [98/500], Step [1/18], Train_Loss: 0.8136, Valid_Loss: 1.1559\n",
      "Epoch [99/500], Step [1/18], Train_Loss: 1.2323, Valid_Loss: 1.5070\n",
      "Epoch [100/500], Step [1/18], Train_Loss: 1.1116, Valid_Loss: 1.1473\n",
      "Epoch [101/500], Step [1/18], Train_Loss: 1.2804, Valid_Loss: 1.2353\n",
      "Epoch [102/500], Step [1/18], Train_Loss: 1.1141, Valid_Loss: 1.1397\n",
      "Epoch [103/500], Step [1/18], Train_Loss: 1.1158, Valid_Loss: 1.4155\n",
      "Epoch [104/500], Step [1/18], Train_Loss: 0.9753, Valid_Loss: 1.1055\n",
      "Epoch [105/500], Step [1/18], Train_Loss: 1.1066, Valid_Loss: 1.2368\n",
      "Epoch [106/500], Step [1/18], Train_Loss: 1.1605, Valid_Loss: 1.5326\n",
      "Epoch [107/500], Step [1/18], Train_Loss: 1.1174, Valid_Loss: 1.4189\n",
      "Epoch [108/500], Step [1/18], Train_Loss: 1.0673, Valid_Loss: 1.3420\n",
      "Epoch [109/500], Step [1/18], Train_Loss: 0.9275, Valid_Loss: 1.2407\n",
      "Epoch [110/500], Step [1/18], Train_Loss: 0.7689, Valid_Loss: 1.0606\n",
      "Epoch [111/500], Step [1/18], Train_Loss: 1.2563, Valid_Loss: 1.7263\n",
      "Epoch [112/500], Step [1/18], Train_Loss: 1.2571, Valid_Loss: 1.3703\n",
      "Epoch [113/500], Step [1/18], Train_Loss: 1.1788, Valid_Loss: 1.4289\n",
      "Epoch [114/500], Step [1/18], Train_Loss: 0.9550, Valid_Loss: 1.1704\n",
      "Epoch [115/500], Step [1/18], Train_Loss: 1.0305, Valid_Loss: 1.0689\n",
      "Epoch [116/500], Step [1/18], Train_Loss: 0.9207, Valid_Loss: 1.0157\n",
      "Epoch [117/500], Step [1/18], Train_Loss: 1.1184, Valid_Loss: 1.3314\n",
      "Epoch [118/500], Step [1/18], Train_Loss: 1.1949, Valid_Loss: 1.2547\n",
      "Epoch [119/500], Step [1/18], Train_Loss: 0.7185, Valid_Loss: 1.1041\n",
      "Epoch [120/500], Step [1/18], Train_Loss: 0.7276, Valid_Loss: 0.8930\n",
      "Epoch [121/500], Step [1/18], Train_Loss: 0.7937, Valid_Loss: 0.9860\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [122/500], Step [1/18], Train_Loss: 0.7164, Valid_Loss: 0.9100\n",
      "Epoch [123/500], Step [1/18], Train_Loss: 0.9894, Valid_Loss: 1.1753\n",
      "Epoch [124/500], Step [1/18], Train_Loss: 1.4578, Valid_Loss: 1.9466\n",
      "Epoch [125/500], Step [1/18], Train_Loss: 1.1679, Valid_Loss: 1.4782\n",
      "Epoch [126/500], Step [1/18], Train_Loss: 0.8984, Valid_Loss: 1.1742\n",
      "Epoch [127/500], Step [1/18], Train_Loss: 0.7654, Valid_Loss: 0.9360\n",
      "Epoch [128/500], Step [1/18], Train_Loss: 0.8743, Valid_Loss: 1.1448\n",
      "Epoch [129/500], Step [1/18], Train_Loss: 1.0655, Valid_Loss: 1.2309\n",
      "Epoch [130/500], Step [1/18], Train_Loss: 0.9779, Valid_Loss: 1.0394\n",
      "Epoch [131/500], Step [1/18], Train_Loss: 0.9433, Valid_Loss: 0.9650\n",
      "Epoch [132/500], Step [1/18], Train_Loss: 1.0200, Valid_Loss: 1.3092\n",
      "Epoch [133/500], Step [1/18], Train_Loss: 0.9244, Valid_Loss: 1.2760\n",
      "Epoch [134/500], Step [1/18], Train_Loss: 0.9168, Valid_Loss: 1.0840\n",
      "Epoch [135/500], Step [1/18], Train_Loss: 1.0242, Valid_Loss: 1.5743\n",
      "Epoch [136/500], Step [1/18], Train_Loss: 0.9955, Valid_Loss: 1.2393\n",
      "Epoch [137/500], Step [1/18], Train_Loss: 0.8189, Valid_Loss: 1.1736\n",
      "Epoch [138/500], Step [1/18], Train_Loss: 0.8078, Valid_Loss: 1.1776\n",
      "Epoch [139/500], Step [1/18], Train_Loss: 0.5821, Valid_Loss: 0.7257\n",
      "Epoch [140/500], Step [1/18], Train_Loss: 0.8302, Valid_Loss: 1.1240\n",
      "Epoch [141/500], Step [1/18], Train_Loss: 0.9847, Valid_Loss: 1.0799\n",
      "Epoch [142/500], Step [1/18], Train_Loss: 0.7929, Valid_Loss: 0.9822\n",
      "Epoch [143/500], Step [1/18], Train_Loss: 0.9443, Valid_Loss: 1.4608\n",
      "Epoch [144/500], Step [1/18], Train_Loss: 0.8345, Valid_Loss: 1.1488\n",
      "Epoch [145/500], Step [1/18], Train_Loss: 0.8418, Valid_Loss: 0.9090\n",
      "Epoch [146/500], Step [1/18], Train_Loss: 0.8351, Valid_Loss: 0.7816\n",
      "Epoch [147/500], Step [1/18], Train_Loss: 0.8433, Valid_Loss: 1.0804\n",
      "Epoch [148/500], Step [1/18], Train_Loss: 0.6371, Valid_Loss: 1.1632\n",
      "Epoch [149/500], Step [1/18], Train_Loss: 0.6839, Valid_Loss: 1.0793\n",
      "Epoch [150/500], Step [1/18], Train_Loss: 0.6096, Valid_Loss: 0.7289\n",
      "Epoch [151/500], Step [1/18], Train_Loss: 0.6811, Valid_Loss: 1.0100\n",
      "Epoch [152/500], Step [1/18], Train_Loss: 0.6967, Valid_Loss: 1.1116\n",
      "Epoch [153/500], Step [1/18], Train_Loss: 0.5380, Valid_Loss: 0.8502\n",
      "Epoch [154/500], Step [1/18], Train_Loss: 1.2518, Valid_Loss: 1.3776\n",
      "Epoch [155/500], Step [1/18], Train_Loss: 0.7129, Valid_Loss: 0.7055\n",
      "Epoch [156/500], Step [1/18], Train_Loss: 0.7690, Valid_Loss: 0.8429\n",
      "Epoch [157/500], Step [1/18], Train_Loss: 0.6732, Valid_Loss: 0.9438\n",
      "Epoch [158/500], Step [1/18], Train_Loss: 0.7029, Valid_Loss: 0.7704\n",
      "Epoch [159/500], Step [1/18], Train_Loss: 0.7050, Valid_Loss: 0.9118\n",
      "Epoch [160/500], Step [1/18], Train_Loss: 0.9133, Valid_Loss: 1.2924\n",
      "Epoch [161/500], Step [1/18], Train_Loss: 0.9421, Valid_Loss: 1.1969\n",
      "Epoch [162/500], Step [1/18], Train_Loss: 0.6950, Valid_Loss: 0.7642\n",
      "Epoch [163/500], Step [1/18], Train_Loss: 0.6864, Valid_Loss: 0.9316\n",
      "Epoch [164/500], Step [1/18], Train_Loss: 0.7153, Valid_Loss: 0.9205\n",
      "Epoch [165/500], Step [1/18], Train_Loss: 0.6550, Valid_Loss: 1.0651\n",
      "Epoch [166/500], Step [1/18], Train_Loss: 0.7064, Valid_Loss: 0.8731\n",
      "Epoch [167/500], Step [1/18], Train_Loss: 0.6985, Valid_Loss: 0.9986\n",
      "Epoch [168/500], Step [1/18], Train_Loss: 0.6743, Valid_Loss: 1.0498\n",
      "Epoch [169/500], Step [1/18], Train_Loss: 0.6860, Valid_Loss: 0.9816\n",
      "Epoch [170/500], Step [1/18], Train_Loss: 0.7041, Valid_Loss: 0.9339\n",
      "Epoch [171/500], Step [1/18], Train_Loss: 0.8701, Valid_Loss: 1.1530\n",
      "Epoch [172/500], Step [1/18], Train_Loss: 0.5740, Valid_Loss: 0.8353\n",
      "Epoch [173/500], Step [1/18], Train_Loss: 0.8012, Valid_Loss: 0.8721\n",
      "Epoch [174/500], Step [1/18], Train_Loss: 0.8373, Valid_Loss: 1.0385\n",
      "Epoch [175/500], Step [1/18], Train_Loss: 0.5817, Valid_Loss: 0.9749\n",
      "Epoch [176/500], Step [1/18], Train_Loss: 0.6649, Valid_Loss: 0.8573\n",
      "Epoch [177/500], Step [1/18], Train_Loss: 0.7306, Valid_Loss: 1.1130\n",
      "Epoch [178/500], Step [1/18], Train_Loss: 0.6230, Valid_Loss: 0.7402\n",
      "Epoch [179/500], Step [1/18], Train_Loss: 0.5008, Valid_Loss: 0.9239\n",
      "Epoch [180/500], Step [1/18], Train_Loss: 0.8188, Valid_Loss: 1.0478\n",
      "Epoch [181/500], Step [1/18], Train_Loss: 0.6232, Valid_Loss: 0.7540\n",
      "Epoch [182/500], Step [1/18], Train_Loss: 0.5443, Valid_Loss: 0.7775\n",
      "Epoch [183/500], Step [1/18], Train_Loss: 0.4835, Valid_Loss: 0.8226\n",
      "Epoch [184/500], Step [1/18], Train_Loss: 0.4398, Valid_Loss: 0.7364\n",
      "Epoch [185/500], Step [1/18], Train_Loss: 0.5451, Valid_Loss: 0.7671\n",
      "Epoch [186/500], Step [1/18], Train_Loss: 0.7063, Valid_Loss: 1.0701\n",
      "Epoch [187/500], Step [1/18], Train_Loss: 0.4650, Valid_Loss: 0.5351\n",
      "Epoch [188/500], Step [1/18], Train_Loss: 0.6474, Valid_Loss: 0.8016\n",
      "Epoch [189/500], Step [1/18], Train_Loss: 0.6956, Valid_Loss: 0.8400\n",
      "Epoch [190/500], Step [1/18], Train_Loss: 0.4687, Valid_Loss: 0.7543\n",
      "Epoch [191/500], Step [1/18], Train_Loss: 0.5904, Valid_Loss: 0.9679\n",
      "Epoch [192/500], Step [1/18], Train_Loss: 0.6353, Valid_Loss: 1.0014\n",
      "Epoch [193/500], Step [1/18], Train_Loss: 0.4701, Valid_Loss: 0.6651\n",
      "Epoch [194/500], Step [1/18], Train_Loss: 0.3967, Valid_Loss: 0.7270\n",
      "Epoch [195/500], Step [1/18], Train_Loss: 0.6894, Valid_Loss: 0.9699\n",
      "Epoch [196/500], Step [1/18], Train_Loss: 0.3735, Valid_Loss: 0.5348\n",
      "Epoch [197/500], Step [1/18], Train_Loss: 0.7227, Valid_Loss: 1.1996\n",
      "Epoch [198/500], Step [1/18], Train_Loss: 0.4175, Valid_Loss: 0.6540\n",
      "Epoch [199/500], Step [1/18], Train_Loss: 0.4524, Valid_Loss: 0.6369\n",
      "Epoch [200/500], Step [1/18], Train_Loss: 0.4389, Valid_Loss: 0.8333\n",
      "Epoch [201/500], Step [1/18], Train_Loss: 0.4821, Valid_Loss: 0.7821\n",
      "Epoch [202/500], Step [1/18], Train_Loss: 0.4966, Valid_Loss: 0.6077\n",
      "Epoch [203/500], Step [1/18], Train_Loss: 0.5097, Valid_Loss: 0.8018\n",
      "Epoch [204/500], Step [1/18], Train_Loss: 0.4246, Valid_Loss: 0.7874\n",
      "Epoch [205/500], Step [1/18], Train_Loss: 0.3851, Valid_Loss: 0.7418\n",
      "Epoch [206/500], Step [1/18], Train_Loss: 0.4377, Valid_Loss: 0.8207\n",
      "Epoch [207/500], Step [1/18], Train_Loss: 0.5154, Valid_Loss: 0.7203\n",
      "Epoch [208/500], Step [1/18], Train_Loss: 0.5278, Valid_Loss: 0.8429\n",
      "Epoch [209/500], Step [1/18], Train_Loss: 0.5059, Valid_Loss: 0.6764\n",
      "Epoch [210/500], Step [1/18], Train_Loss: 0.5803, Valid_Loss: 0.8960\n",
      "Epoch [211/500], Step [1/18], Train_Loss: 0.3481, Valid_Loss: 0.5297\n",
      "Epoch [212/500], Step [1/18], Train_Loss: 0.4418, Valid_Loss: 0.5614\n",
      "Epoch [213/500], Step [1/18], Train_Loss: 0.5123, Valid_Loss: 0.8049\n",
      "Epoch [214/500], Step [1/18], Train_Loss: 0.3533, Valid_Loss: 0.6031\n",
      "Epoch [215/500], Step [1/18], Train_Loss: 0.4696, Valid_Loss: 0.8340\n",
      "Epoch [216/500], Step [1/18], Train_Loss: 0.4229, Valid_Loss: 0.6283\n",
      "Epoch [217/500], Step [1/18], Train_Loss: 0.2436, Valid_Loss: 0.5794\n",
      "Epoch [218/500], Step [1/18], Train_Loss: 0.5589, Valid_Loss: 0.6806\n",
      "Epoch [219/500], Step [1/18], Train_Loss: 0.4604, Valid_Loss: 0.7856\n",
      "Epoch [220/500], Step [1/18], Train_Loss: 0.5012, Valid_Loss: 0.7520\n",
      "Epoch [221/500], Step [1/18], Train_Loss: 0.4267, Valid_Loss: 0.6791\n",
      "Epoch [222/500], Step [1/18], Train_Loss: 0.3765, Valid_Loss: 0.7862\n",
      "Epoch [223/500], Step [1/18], Train_Loss: 0.4598, Valid_Loss: 0.7954\n",
      "Epoch [224/500], Step [1/18], Train_Loss: 0.3290, Valid_Loss: 0.7271\n",
      "Epoch [225/500], Step [1/18], Train_Loss: 0.3585, Valid_Loss: 0.5444\n",
      "Epoch [226/500], Step [1/18], Train_Loss: 0.3345, Valid_Loss: 0.6734\n",
      "Epoch [227/500], Step [1/18], Train_Loss: 0.3325, Valid_Loss: 0.6471\n",
      "Epoch [228/500], Step [1/18], Train_Loss: 0.3206, Valid_Loss: 0.5514\n",
      "Epoch [229/500], Step [1/18], Train_Loss: 0.3107, Valid_Loss: 0.4730\n",
      "Epoch [230/500], Step [1/18], Train_Loss: 0.4003, Valid_Loss: 0.6739\n",
      "Epoch [231/500], Step [1/18], Train_Loss: 0.2990, Valid_Loss: 0.4667\n",
      "Epoch [232/500], Step [1/18], Train_Loss: 0.4120, Valid_Loss: 0.6221\n",
      "Epoch [233/500], Step [1/18], Train_Loss: 0.3646, Valid_Loss: 0.6027\n",
      "Epoch [234/500], Step [1/18], Train_Loss: 0.3852, Valid_Loss: 0.6134\n",
      "Epoch [235/500], Step [1/18], Train_Loss: 0.2998, Valid_Loss: 0.5331\n",
      "Epoch [236/500], Step [1/18], Train_Loss: 0.4061, Valid_Loss: 0.6318\n",
      "Epoch [237/500], Step [1/18], Train_Loss: 0.2483, Valid_Loss: 0.4914\n",
      "Epoch [238/500], Step [1/18], Train_Loss: 0.2548, Valid_Loss: 0.3976\n",
      "Epoch [239/500], Step [1/18], Train_Loss: 0.2498, Valid_Loss: 0.4762\n",
      "Epoch [240/500], Step [1/18], Train_Loss: 0.3131, Valid_Loss: 0.6603\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [241/500], Step [1/18], Train_Loss: 0.2667, Valid_Loss: 0.5983\n",
      "Epoch [242/500], Step [1/18], Train_Loss: 0.3086, Valid_Loss: 0.4986\n",
      "Epoch [243/500], Step [1/18], Train_Loss: 0.3522, Valid_Loss: 0.6439\n",
      "Epoch [244/500], Step [1/18], Train_Loss: 0.2978, Valid_Loss: 0.6289\n",
      "Epoch [245/500], Step [1/18], Train_Loss: 0.3441, Valid_Loss: 0.5544\n",
      "Epoch [246/500], Step [1/18], Train_Loss: 0.2742, Valid_Loss: 0.4175\n",
      "Epoch [247/500], Step [1/18], Train_Loss: 0.2529, Valid_Loss: 0.4785\n",
      "Epoch [248/500], Step [1/18], Train_Loss: 0.3235, Valid_Loss: 0.5832\n",
      "Epoch [249/500], Step [1/18], Train_Loss: 0.2469, Valid_Loss: 0.4711\n",
      "Epoch [250/500], Step [1/18], Train_Loss: 0.2532, Valid_Loss: 0.5959\n",
      "Epoch [251/500], Step [1/18], Train_Loss: 0.1703, Valid_Loss: 0.4538\n",
      "Epoch [252/500], Step [1/18], Train_Loss: 0.1658, Valid_Loss: 0.4774\n",
      "Epoch [253/500], Step [1/18], Train_Loss: 0.1860, Valid_Loss: 0.3117\n",
      "Epoch [254/500], Step [1/18], Train_Loss: 0.2574, Valid_Loss: 0.4705\n",
      "Epoch [255/500], Step [1/18], Train_Loss: 0.2273, Valid_Loss: 0.5233\n",
      "Epoch [256/500], Step [1/18], Train_Loss: 0.2732, Valid_Loss: 0.5342\n",
      "Epoch [257/500], Step [1/18], Train_Loss: 0.2411, Valid_Loss: 0.5100\n",
      "Epoch [258/500], Step [1/18], Train_Loss: 0.2736, Valid_Loss: 0.5390\n",
      "Epoch [259/500], Step [1/18], Train_Loss: 0.2019, Valid_Loss: 0.5335\n",
      "Epoch [260/500], Step [1/18], Train_Loss: 0.1341, Valid_Loss: 0.3385\n",
      "Epoch [261/500], Step [1/18], Train_Loss: 0.2356, Valid_Loss: 0.4722\n",
      "Epoch [262/500], Step [1/18], Train_Loss: 0.1305, Valid_Loss: 0.3584\n",
      "Epoch [263/500], Step [1/18], Train_Loss: 0.2205, Valid_Loss: 0.3889\n",
      "Epoch [264/500], Step [1/18], Train_Loss: 0.1658, Valid_Loss: 0.3809\n",
      "Epoch [265/500], Step [1/18], Train_Loss: 0.2060, Valid_Loss: 0.5315\n",
      "Epoch [266/500], Step [1/18], Train_Loss: 0.1721, Valid_Loss: 0.4132\n",
      "Epoch [267/500], Step [1/18], Train_Loss: 0.1864, Valid_Loss: 0.5371\n",
      "Epoch [268/500], Step [1/18], Train_Loss: 0.1488, Valid_Loss: 0.4158\n",
      "Epoch [269/500], Step [1/18], Train_Loss: 0.1833, Valid_Loss: 0.4100\n",
      "Epoch [270/500], Step [1/18], Train_Loss: 0.1870, Valid_Loss: 0.4689\n",
      "Epoch [271/500], Step [1/18], Train_Loss: 0.1724, Valid_Loss: 0.3677\n",
      "Epoch [272/500], Step [1/18], Train_Loss: 0.1915, Valid_Loss: 0.5335\n",
      "Epoch [273/500], Step [1/18], Train_Loss: 0.1119, Valid_Loss: 0.4692\n",
      "Epoch [274/500], Step [1/18], Train_Loss: 0.1852, Valid_Loss: 0.5282\n",
      "Epoch [275/500], Step [1/18], Train_Loss: 0.1368, Valid_Loss: 0.3728\n",
      "Epoch [276/500], Step [1/18], Train_Loss: 0.1339, Valid_Loss: 0.5238\n",
      "Epoch [277/500], Step [1/18], Train_Loss: 0.1433, Valid_Loss: 0.4018\n",
      "Epoch [278/500], Step [1/18], Train_Loss: 0.1623, Valid_Loss: 0.4458\n",
      "Epoch [279/500], Step [1/18], Train_Loss: 0.1993, Valid_Loss: 0.5001\n",
      "Epoch [280/500], Step [1/18], Train_Loss: 0.1707, Valid_Loss: 0.3961\n",
      "Epoch [281/500], Step [1/18], Train_Loss: 0.1328, Valid_Loss: 0.3990\n",
      "Epoch [282/500], Step [1/18], Train_Loss: 0.1989, Valid_Loss: 0.4116\n",
      "Epoch [283/500], Step [1/18], Train_Loss: 0.1895, Valid_Loss: 0.3953\n",
      "Epoch [284/500], Step [1/18], Train_Loss: 0.1740, Valid_Loss: 0.4292\n",
      "Epoch [285/500], Step [1/18], Train_Loss: 0.1466, Valid_Loss: 0.3979\n",
      "Epoch [286/500], Step [1/18], Train_Loss: 0.1177, Valid_Loss: 0.4508\n",
      "Epoch [287/500], Step [1/18], Train_Loss: 0.1497, Valid_Loss: 0.4240\n",
      "Epoch [288/500], Step [1/18], Train_Loss: 0.1485, Valid_Loss: 0.5273\n",
      "Epoch [289/500], Step [1/18], Train_Loss: 0.1552, Valid_Loss: 0.3913\n",
      "Epoch [290/500], Step [1/18], Train_Loss: 0.1184, Valid_Loss: 0.4117\n",
      "Epoch [291/500], Step [1/18], Train_Loss: 0.1781, Valid_Loss: 0.4276\n",
      "Epoch [292/500], Step [1/18], Train_Loss: 0.1240, Valid_Loss: 0.4209\n",
      "Epoch [293/500], Step [1/18], Train_Loss: 0.1076, Valid_Loss: 0.3781\n",
      "Epoch [294/500], Step [1/18], Train_Loss: 0.1432, Valid_Loss: 0.3994\n",
      "Epoch [295/500], Step [1/18], Train_Loss: 0.0999, Valid_Loss: 0.3413\n",
      "Epoch [296/500], Step [1/18], Train_Loss: 0.1319, Valid_Loss: 0.4029\n",
      "Epoch [297/500], Step [1/18], Train_Loss: 0.1068, Valid_Loss: 0.3397\n",
      "Epoch [298/500], Step [1/18], Train_Loss: 0.1114, Valid_Loss: 0.4684\n",
      "Epoch [299/500], Step [1/18], Train_Loss: 0.1237, Valid_Loss: 0.3721\n",
      "Epoch [300/500], Step [1/18], Train_Loss: 0.0809, Valid_Loss: 0.3586\n",
      "Epoch [301/500], Step [1/18], Train_Loss: 0.0883, Valid_Loss: 0.3802\n",
      "Epoch [302/500], Step [1/18], Train_Loss: 0.0851, Valid_Loss: 0.3904\n",
      "Epoch [303/500], Step [1/18], Train_Loss: 0.1043, Valid_Loss: 0.3463\n",
      "Epoch [304/500], Step [1/18], Train_Loss: 0.0749, Valid_Loss: 0.3693\n",
      "Epoch [305/500], Step [1/18], Train_Loss: 0.1256, Valid_Loss: 0.4058\n",
      "Epoch [306/500], Step [1/18], Train_Loss: 0.0976, Valid_Loss: 0.3426\n",
      "Epoch [307/500], Step [1/18], Train_Loss: 0.1253, Valid_Loss: 0.3646\n",
      "Epoch [308/500], Step [1/18], Train_Loss: 0.0865, Valid_Loss: 0.4252\n",
      "Epoch [309/500], Step [1/18], Train_Loss: 0.0958, Valid_Loss: 0.3643\n",
      "Epoch [310/500], Step [1/18], Train_Loss: 0.0675, Valid_Loss: 0.3393\n",
      "Epoch [311/500], Step [1/18], Train_Loss: 0.1087, Valid_Loss: 0.3964\n",
      "Epoch [312/500], Step [1/18], Train_Loss: 0.0716, Valid_Loss: 0.3276\n",
      "Epoch [313/500], Step [1/18], Train_Loss: 0.0780, Valid_Loss: 0.2953\n",
      "Epoch [314/500], Step [1/18], Train_Loss: 0.0758, Valid_Loss: 0.3365\n",
      "Epoch [315/500], Step [1/18], Train_Loss: 0.0911, Valid_Loss: 0.3265\n",
      "Epoch [316/500], Step [1/18], Train_Loss: 0.0635, Valid_Loss: 0.3312\n",
      "Epoch [317/500], Step [1/18], Train_Loss: 0.0794, Valid_Loss: 0.3671\n",
      "Epoch [318/500], Step [1/18], Train_Loss: 0.0641, Valid_Loss: 0.3323\n",
      "Epoch [319/500], Step [1/18], Train_Loss: 0.0715, Valid_Loss: 0.3233\n",
      "Epoch [320/500], Step [1/18], Train_Loss: 0.0857, Valid_Loss: 0.3199\n",
      "Epoch [321/500], Step [1/18], Train_Loss: 0.0792, Valid_Loss: 0.2874\n",
      "Epoch [322/500], Step [1/18], Train_Loss: 0.0750, Valid_Loss: 0.3857\n",
      "Epoch [323/500], Step [1/18], Train_Loss: 0.0620, Valid_Loss: 0.3217\n",
      "Epoch [324/500], Step [1/18], Train_Loss: 0.0562, Valid_Loss: 0.2853\n",
      "Epoch [325/500], Step [1/18], Train_Loss: 0.0528, Valid_Loss: 0.3460\n",
      "Epoch [326/500], Step [1/18], Train_Loss: 0.0640, Valid_Loss: 0.2789\n",
      "Epoch [327/500], Step [1/18], Train_Loss: 0.0448, Valid_Loss: 0.2993\n",
      "Epoch [328/500], Step [1/18], Train_Loss: 0.0636, Valid_Loss: 0.3240\n",
      "Epoch [329/500], Step [1/18], Train_Loss: 0.0594, Valid_Loss: 0.3188\n",
      "Epoch [330/500], Step [1/18], Train_Loss: 0.0693, Valid_Loss: 0.3266\n",
      "Epoch [331/500], Step [1/18], Train_Loss: 0.0508, Valid_Loss: 0.2611\n",
      "Epoch [332/500], Step [1/18], Train_Loss: 0.0681, Valid_Loss: 0.3162\n",
      "Epoch [333/500], Step [1/18], Train_Loss: 0.0634, Valid_Loss: 0.3012\n",
      "Epoch [334/500], Step [1/18], Train_Loss: 0.0480, Valid_Loss: 0.3010\n",
      "Epoch [335/500], Step [1/18], Train_Loss: 0.0467, Valid_Loss: 0.3004\n",
      "Epoch [336/500], Step [1/18], Train_Loss: 0.0545, Valid_Loss: 0.3486\n",
      "Epoch [337/500], Step [1/18], Train_Loss: 0.0602, Valid_Loss: 0.3354\n",
      "Epoch [338/500], Step [1/18], Train_Loss: 0.0461, Valid_Loss: 0.3108\n",
      "Epoch [339/500], Step [1/18], Train_Loss: 0.0493, Valid_Loss: 0.2959\n",
      "Epoch [340/500], Step [1/18], Train_Loss: 0.0374, Valid_Loss: 0.3280\n",
      "Epoch [341/500], Step [1/18], Train_Loss: 0.0658, Valid_Loss: 0.2727\n",
      "Epoch [342/500], Step [1/18], Train_Loss: 0.0400, Valid_Loss: 0.3319\n",
      "Epoch [343/500], Step [1/18], Train_Loss: 0.0386, Valid_Loss: 0.2646\n",
      "Epoch [344/500], Step [1/18], Train_Loss: 0.0470, Valid_Loss: 0.3045\n",
      "Epoch [345/500], Step [1/18], Train_Loss: 0.0417, Valid_Loss: 0.3430\n",
      "Epoch [346/500], Step [1/18], Train_Loss: 0.0583, Valid_Loss: 0.3208\n",
      "Epoch [347/500], Step [1/18], Train_Loss: 0.0423, Valid_Loss: 0.2679\n",
      "Epoch [348/500], Step [1/18], Train_Loss: 0.0382, Valid_Loss: 0.2945\n",
      "Epoch [349/500], Step [1/18], Train_Loss: 0.0428, Valid_Loss: 0.3555\n",
      "Epoch [350/500], Step [1/18], Train_Loss: 0.0452, Valid_Loss: 0.3069\n",
      "Epoch [351/500], Step [1/18], Train_Loss: 0.0365, Valid_Loss: 0.2906\n",
      "Epoch [352/500], Step [1/18], Train_Loss: 0.0313, Valid_Loss: 0.2895\n",
      "Epoch [353/500], Step [1/18], Train_Loss: 0.0408, Valid_Loss: 0.3134\n",
      "Epoch [354/500], Step [1/18], Train_Loss: 0.0451, Valid_Loss: 0.2453\n",
      "Epoch [355/500], Step [1/18], Train_Loss: 0.0505, Valid_Loss: 0.2612\n",
      "Epoch [356/500], Step [1/18], Train_Loss: 0.0333, Valid_Loss: 0.2670\n",
      "Epoch [357/500], Step [1/18], Train_Loss: 0.0348, Valid_Loss: 0.3005\n",
      "Epoch [358/500], Step [1/18], Train_Loss: 0.0429, Valid_Loss: 0.3105\n",
      "Epoch [359/500], Step [1/18], Train_Loss: 0.0288, Valid_Loss: 0.2823\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [360/500], Step [1/18], Train_Loss: 0.0297, Valid_Loss: 0.2904\n",
      "Epoch [361/500], Step [1/18], Train_Loss: 0.0288, Valid_Loss: 0.2960\n",
      "Epoch [362/500], Step [1/18], Train_Loss: 0.0320, Valid_Loss: 0.2751\n",
      "Epoch [363/500], Step [1/18], Train_Loss: 0.0581, Valid_Loss: 0.3163\n",
      "Epoch [364/500], Step [1/18], Train_Loss: 0.0316, Valid_Loss: 0.3149\n",
      "Epoch [365/500], Step [1/18], Train_Loss: 0.0333, Valid_Loss: 0.2907\n",
      "Epoch [366/500], Step [1/18], Train_Loss: 0.0260, Valid_Loss: 0.2981\n",
      "Epoch [367/500], Step [1/18], Train_Loss: 0.0254, Valid_Loss: 0.2821\n",
      "Epoch [368/500], Step [1/18], Train_Loss: 0.0199, Valid_Loss: 0.2923\n",
      "Epoch [369/500], Step [1/18], Train_Loss: 0.0200, Valid_Loss: 0.2702\n",
      "Epoch [370/500], Step [1/18], Train_Loss: 0.0326, Valid_Loss: 0.2657\n",
      "Epoch [371/500], Step [1/18], Train_Loss: 0.0205, Valid_Loss: 0.2650\n",
      "Epoch [372/500], Step [1/18], Train_Loss: 0.0275, Valid_Loss: 0.3030\n",
      "Epoch [373/500], Step [1/18], Train_Loss: 0.0224, Valid_Loss: 0.3048\n",
      "Epoch [374/500], Step [1/18], Train_Loss: 0.0247, Valid_Loss: 0.3188\n",
      "Epoch [375/500], Step [1/18], Train_Loss: 0.0292, Valid_Loss: 0.3166\n",
      "Epoch [376/500], Step [1/18], Train_Loss: 0.0308, Valid_Loss: 0.2768\n",
      "Epoch [377/500], Step [1/18], Train_Loss: 0.0262, Valid_Loss: 0.2889\n",
      "Epoch [378/500], Step [1/18], Train_Loss: 0.0291, Valid_Loss: 0.2997\n",
      "Epoch [379/500], Step [1/18], Train_Loss: 0.0325, Valid_Loss: 0.2814\n",
      "Epoch [380/500], Step [1/18], Train_Loss: 0.0213, Valid_Loss: 0.3056\n",
      "Epoch [381/500], Step [1/18], Train_Loss: 0.0237, Valid_Loss: 0.2844\n",
      "Epoch [382/500], Step [1/18], Train_Loss: 0.0315, Valid_Loss: 0.2653\n",
      "Epoch [383/500], Step [1/18], Train_Loss: 0.0242, Valid_Loss: 0.2597\n",
      "Epoch [384/500], Step [1/18], Train_Loss: 0.0182, Valid_Loss: 0.2993\n",
      "Epoch [385/500], Step [1/18], Train_Loss: 0.0272, Valid_Loss: 0.2667\n",
      "Epoch [386/500], Step [1/18], Train_Loss: 0.0318, Valid_Loss: 0.2563\n",
      "Epoch [387/500], Step [1/18], Train_Loss: 0.0268, Valid_Loss: 0.2646\n",
      "Epoch [388/500], Step [1/18], Train_Loss: 0.0191, Valid_Loss: 0.2876\n",
      "Epoch [389/500], Step [1/18], Train_Loss: 0.0259, Valid_Loss: 0.2998\n",
      "Epoch [390/500], Step [1/18], Train_Loss: 0.0255, Valid_Loss: 0.2829\n",
      "Epoch [391/500], Step [1/18], Train_Loss: 0.0214, Valid_Loss: 0.2916\n",
      "Epoch [392/500], Step [1/18], Train_Loss: 0.0211, Valid_Loss: 0.2537\n",
      "Epoch [393/500], Step [1/18], Train_Loss: 0.0186, Valid_Loss: 0.3033\n",
      "Epoch [394/500], Step [1/18], Train_Loss: 0.0173, Valid_Loss: 0.2745\n",
      "Epoch [395/500], Step [1/18], Train_Loss: 0.0218, Valid_Loss: 0.2562\n",
      "Epoch [396/500], Step [1/18], Train_Loss: 0.0198, Valid_Loss: 0.2849\n",
      "Epoch [397/500], Step [1/18], Train_Loss: 0.0234, Valid_Loss: 0.2704\n",
      "Epoch [398/500], Step [1/18], Train_Loss: 0.0238, Valid_Loss: 0.2758\n",
      "Epoch [399/500], Step [1/18], Train_Loss: 0.0192, Valid_Loss: 0.2912\n",
      "Epoch [400/500], Step [1/18], Train_Loss: 0.0323, Valid_Loss: 0.2484\n",
      "Epoch [401/500], Step [1/18], Train_Loss: 0.0242, Valid_Loss: 0.2660\n",
      "Epoch [402/500], Step [1/18], Train_Loss: 0.0187, Valid_Loss: 0.2688\n",
      "Epoch [403/500], Step [1/18], Train_Loss: 0.0309, Valid_Loss: 0.2817\n",
      "Epoch [404/500], Step [1/18], Train_Loss: 0.0178, Valid_Loss: 0.2667\n",
      "Epoch [405/500], Step [1/18], Train_Loss: 0.0200, Valid_Loss: 0.2796\n",
      "Epoch [406/500], Step [1/18], Train_Loss: 0.0232, Valid_Loss: 0.2665\n",
      "Epoch [407/500], Step [1/18], Train_Loss: 0.0204, Valid_Loss: 0.2688\n",
      "Epoch [408/500], Step [1/18], Train_Loss: 0.0232, Valid_Loss: 0.3019\n",
      "Epoch [409/500], Step [1/18], Train_Loss: 0.0288, Valid_Loss: 0.2361\n",
      "Epoch [410/500], Step [1/18], Train_Loss: 0.0281, Valid_Loss: 0.2347\n",
      "Epoch [411/500], Step [1/18], Train_Loss: 0.0225, Valid_Loss: 0.2814\n",
      "Epoch [412/500], Step [1/18], Train_Loss: 0.0232, Valid_Loss: 0.2620\n",
      "Epoch [413/500], Step [1/18], Train_Loss: 0.0256, Valid_Loss: 0.2560\n",
      "Epoch [414/500], Step [1/18], Train_Loss: 0.0186, Valid_Loss: 0.2715\n",
      "Epoch [415/500], Step [1/18], Train_Loss: 0.0301, Valid_Loss: 0.2543\n",
      "Epoch [416/500], Step [1/18], Train_Loss: 0.0316, Valid_Loss: 0.2479\n",
      "Epoch [417/500], Step [1/18], Train_Loss: 0.0217, Valid_Loss: 0.2543\n",
      "Epoch [418/500], Step [1/18], Train_Loss: 0.0195, Valid_Loss: 0.2543\n",
      "Epoch [419/500], Step [1/18], Train_Loss: 0.0196, Valid_Loss: 0.2737\n",
      "Epoch [420/500], Step [1/18], Train_Loss: 0.0263, Valid_Loss: 0.2337\n",
      "Epoch [421/500], Step [1/18], Train_Loss: 0.0191, Valid_Loss: 0.2660\n",
      "Epoch [422/500], Step [1/18], Train_Loss: 0.0266, Valid_Loss: 0.2412\n",
      "Epoch [423/500], Step [1/18], Train_Loss: 0.0308, Valid_Loss: 0.2702\n",
      "Epoch [424/500], Step [1/18], Train_Loss: 0.0175, Valid_Loss: 0.2608\n",
      "Epoch [425/500], Step [1/18], Train_Loss: 0.0182, Valid_Loss: 0.2693\n",
      "Epoch [426/500], Step [1/18], Train_Loss: 0.0181, Valid_Loss: 0.2682\n",
      "Epoch [427/500], Step [1/18], Train_Loss: 0.0145, Valid_Loss: 0.2592\n",
      "Epoch [428/500], Step [1/18], Train_Loss: 0.0116, Valid_Loss: 0.2873\n",
      "Epoch [429/500], Step [1/18], Train_Loss: 0.0172, Valid_Loss: 0.2528\n",
      "Epoch [430/500], Step [1/18], Train_Loss: 0.0145, Valid_Loss: 0.2615\n",
      "Epoch [431/500], Step [1/18], Train_Loss: 0.0160, Valid_Loss: 0.2703\n",
      "Epoch [432/500], Step [1/18], Train_Loss: 0.0149, Valid_Loss: 0.2655\n",
      "Epoch [433/500], Step [1/18], Train_Loss: 0.0103, Valid_Loss: 0.2759\n",
      "Epoch [434/500], Step [1/18], Train_Loss: 0.0110, Valid_Loss: 0.2916\n",
      "Epoch [435/500], Step [1/18], Train_Loss: 0.0164, Valid_Loss: 0.2793\n",
      "Epoch [436/500], Step [1/18], Train_Loss: 0.0146, Valid_Loss: 0.3092\n",
      "Epoch [437/500], Step [1/18], Train_Loss: 0.0177, Valid_Loss: 0.2801\n",
      "Epoch [438/500], Step [1/18], Train_Loss: 0.0201, Valid_Loss: 0.2779\n",
      "Epoch [439/500], Step [1/18], Train_Loss: 0.0101, Valid_Loss: 0.2761\n",
      "Epoch [440/500], Step [1/18], Train_Loss: 0.0130, Valid_Loss: 0.2779\n",
      "Epoch [441/500], Step [1/18], Train_Loss: 0.0122, Valid_Loss: 0.2632\n",
      "Epoch [442/500], Step [1/18], Train_Loss: 0.0092, Valid_Loss: 0.3060\n",
      "Epoch [443/500], Step [1/18], Train_Loss: 0.0118, Valid_Loss: 0.2868\n",
      "Epoch [444/500], Step [1/18], Train_Loss: 0.0226, Valid_Loss: 0.2769\n",
      "Epoch [445/500], Step [1/18], Train_Loss: 0.0125, Valid_Loss: 0.2989\n",
      "Epoch [446/500], Step [1/18], Train_Loss: 0.0114, Valid_Loss: 0.2868\n",
      "Epoch [447/500], Step [1/18], Train_Loss: 0.0123, Valid_Loss: 0.2897\n",
      "Epoch [448/500], Step [1/18], Train_Loss: 0.0150, Valid_Loss: 0.2920\n",
      "Epoch [449/500], Step [1/18], Train_Loss: 0.0108, Valid_Loss: 0.2808\n",
      "Epoch [450/500], Step [1/18], Train_Loss: 0.0176, Valid_Loss: 0.2800\n",
      "Epoch [451/500], Step [1/18], Train_Loss: 0.0122, Valid_Loss: 0.2832\n",
      "Epoch [452/500], Step [1/18], Train_Loss: 0.0148, Valid_Loss: 0.3072\n",
      "Epoch [453/500], Step [1/18], Train_Loss: 0.0098, Valid_Loss: 0.2983\n",
      "Epoch [454/500], Step [1/18], Train_Loss: 0.0117, Valid_Loss: 0.2702\n",
      "Epoch [455/500], Step [1/18], Train_Loss: 0.0099, Valid_Loss: 0.2791\n",
      "Epoch [456/500], Step [1/18], Train_Loss: 0.0134, Valid_Loss: 0.2827\n",
      "Epoch [457/500], Step [1/18], Train_Loss: 0.0112, Valid_Loss: 0.3155\n",
      "Epoch [458/500], Step [1/18], Train_Loss: 0.0168, Valid_Loss: 0.2990\n",
      "Epoch [459/500], Step [1/18], Train_Loss: 0.0117, Valid_Loss: 0.2815\n",
      "Epoch [460/500], Step [1/18], Train_Loss: 0.0135, Valid_Loss: 0.2680\n",
      "Epoch [461/500], Step [1/18], Train_Loss: 0.0123, Valid_Loss: 0.2982\n",
      "Epoch [462/500], Step [1/18], Train_Loss: 0.0132, Valid_Loss: 0.2943\n",
      "Epoch [463/500], Step [1/18], Train_Loss: 0.0116, Valid_Loss: 0.3000\n",
      "Epoch [464/500], Step [1/18], Train_Loss: 0.0167, Valid_Loss: 0.2889\n",
      "Epoch [465/500], Step [1/18], Train_Loss: 0.0146, Valid_Loss: 0.2880\n",
      "Epoch [466/500], Step [1/18], Train_Loss: 0.0123, Valid_Loss: 0.2726\n",
      "Epoch [467/500], Step [1/18], Train_Loss: 0.0178, Valid_Loss: 0.2840\n",
      "Epoch [468/500], Step [1/18], Train_Loss: 0.0167, Valid_Loss: 0.2802\n",
      "Epoch [469/500], Step [1/18], Train_Loss: 0.0119, Valid_Loss: 0.2849\n",
      "Epoch [470/500], Step [1/18], Train_Loss: 0.0121, Valid_Loss: 0.2986\n",
      "Epoch [471/500], Step [1/18], Train_Loss: 0.0100, Valid_Loss: 0.2795\n",
      "Epoch [472/500], Step [1/18], Train_Loss: 0.0082, Valid_Loss: 0.3062\n",
      "Epoch [473/500], Step [1/18], Train_Loss: 0.0169, Valid_Loss: 0.2695\n",
      "Epoch [474/500], Step [1/18], Train_Loss: 0.0145, Valid_Loss: 0.2886\n",
      "Epoch [475/500], Step [1/18], Train_Loss: 0.0137, Valid_Loss: 0.3055\n",
      "Epoch [476/500], Step [1/18], Train_Loss: 0.0116, Valid_Loss: 0.3061\n",
      "Epoch [477/500], Step [1/18], Train_Loss: 0.0147, Valid_Loss: 0.2734\n",
      "Epoch [478/500], Step [1/18], Train_Loss: 0.0085, Valid_Loss: 0.2965\n",
      "Epoch [479/500], Step [1/18], Train_Loss: 0.0146, Valid_Loss: 0.2989\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [480/500], Step [1/18], Train_Loss: 0.0124, Valid_Loss: 0.3092\n",
      "Epoch [481/500], Step [1/18], Train_Loss: 0.0140, Valid_Loss: 0.3123\n",
      "Epoch [482/500], Step [1/18], Train_Loss: 0.0128, Valid_Loss: 0.2951\n",
      "Epoch [483/500], Step [1/18], Train_Loss: 0.0156, Valid_Loss: 0.2958\n",
      "Epoch [484/500], Step [1/18], Train_Loss: 0.0119, Valid_Loss: 0.2832\n",
      "Epoch [485/500], Step [1/18], Train_Loss: 0.0192, Valid_Loss: 0.2645\n",
      "Epoch [486/500], Step [1/18], Train_Loss: 0.0182, Valid_Loss: 0.2759\n",
      "Epoch [487/500], Step [1/18], Train_Loss: 0.0091, Valid_Loss: 0.2886\n",
      "Epoch [488/500], Step [1/18], Train_Loss: 0.0096, Valid_Loss: 0.2931\n",
      "Epoch [489/500], Step [1/18], Train_Loss: 0.0155, Valid_Loss: 0.3269\n",
      "Epoch [490/500], Step [1/18], Train_Loss: 0.0111, Valid_Loss: 0.3235\n",
      "Epoch [491/500], Step [1/18], Train_Loss: 0.0123, Valid_Loss: 0.2884\n",
      "Epoch [492/500], Step [1/18], Train_Loss: 0.0111, Valid_Loss: 0.2908\n",
      "Epoch [493/500], Step [1/18], Train_Loss: 0.0095, Valid_Loss: 0.3111\n",
      "Epoch [494/500], Step [1/18], Train_Loss: 0.0107, Valid_Loss: 0.2951\n",
      "Epoch [495/500], Step [1/18], Train_Loss: 0.0158, Valid_Loss: 0.2906\n",
      "Epoch [496/500], Step [1/18], Train_Loss: 0.0157, Valid_Loss: 0.2884\n",
      "Epoch [497/500], Step [1/18], Train_Loss: 0.0094, Valid_Loss: 0.3134\n",
      "Epoch [498/500], Step [1/18], Train_Loss: 0.0095, Valid_Loss: 0.3238\n",
      "Epoch [499/500], Step [1/18], Train_Loss: 0.0131, Valid_Loss: 0.3106\n",
      "Epoch [500/500], Step [1/18], Train_Loss: 0.0094, Valid_Loss: 0.2954\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "total_step = int(len(x_train_conts) / batch_size)\n",
    "for epoch in range(epochs):\n",
    "    for i, (x_conts,x_cats,y) in enumerate(train_loader):\n",
    "        # Forward pass\n",
    "        outputs = model(x_cats,x_conts)\n",
    "        loss = criterion(outputs, y[:,None].float())\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i) % 100 == 0:\n",
    "            for i2, (x2_conts,x2_cats,y2) in enumerate(valid_loader):\n",
    "                loss2 = criterion(outputs, y2[:,None].float())\n",
    "                break\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Train_Loss: {:.4f}, Valid_Loss: {:.4f}' \n",
    "                   .format(epoch+1, epochs, i+1, total_step, loss.item(), loss2.item()))\n",
    "print(\"done!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.0170)\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "model.eval()\n",
    "total = 0\n",
    "nums = 0\n",
    "with torch.no_grad():\n",
    "    for x_conts, x_cats, y in test_loader:\n",
    "        outputs = model(x_cats,x_conts)\n",
    "        \n",
    "        y = y[:,None].float().reshape((-1,1))\n",
    "        loss = criterion(outputs, y.float())\n",
    "        nums += 1\n",
    "        total += loss\n",
    "\n",
    "    print('Loss:',total/nums)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
