{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Imports."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from math import floor\n",
    "\n",
    "import torch\n",
    "import torch.utils.data\n",
    "import torch.nn as nn\n",
    "from torch.autograd import Variable\n",
    "\n",
    "import numpy as np\n",
    "from sklearn import preprocessing\n",
    "from sklearn.preprocessing import normalize"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Calling this row Jupyter Notebook shows us all rows."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.options.display.max_rows = 1000"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Some reason GPU is not working. I think problem is that then I should move all things to GPU."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Device configuration\n",
    "device = torch.device('cpu')#cuda:0' if torch.cuda.is_available() else 'cpu')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "device(type='cpu')"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "device"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Import data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = pd.read_csv('train.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Split data to y and x."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Id</th>\n",
       "      <th>MSSubClass</th>\n",
       "      <th>MSZoning</th>\n",
       "      <th>LotFrontage</th>\n",
       "      <th>LotArea</th>\n",
       "      <th>Street</th>\n",
       "      <th>Alley</th>\n",
       "      <th>LotShape</th>\n",
       "      <th>LandContour</th>\n",
       "      <th>Utilities</th>\n",
       "      <th>...</th>\n",
       "      <th>ScreenPorch</th>\n",
       "      <th>PoolArea</th>\n",
       "      <th>PoolQC</th>\n",
       "      <th>Fence</th>\n",
       "      <th>MiscFeature</th>\n",
       "      <th>MiscVal</th>\n",
       "      <th>MoSold</th>\n",
       "      <th>YrSold</th>\n",
       "      <th>SaleType</th>\n",
       "      <th>SaleCondition</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>65.0</td>\n",
       "      <td>8450</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2</td>\n",
       "      <td>20</td>\n",
       "      <td>RL</td>\n",
       "      <td>80.0</td>\n",
       "      <td>9600</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>Reg</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>5</td>\n",
       "      <td>2007</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>3</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>68.0</td>\n",
       "      <td>11250</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>9</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4</td>\n",
       "      <td>70</td>\n",
       "      <td>RL</td>\n",
       "      <td>60.0</td>\n",
       "      <td>9550</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>2</td>\n",
       "      <td>2006</td>\n",
       "      <td>WD</td>\n",
       "      <td>Abnorml</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5</td>\n",
       "      <td>60</td>\n",
       "      <td>RL</td>\n",
       "      <td>84.0</td>\n",
       "      <td>14260</td>\n",
       "      <td>Pave</td>\n",
       "      <td>NaN</td>\n",
       "      <td>IR1</td>\n",
       "      <td>Lvl</td>\n",
       "      <td>AllPub</td>\n",
       "      <td>...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>0</td>\n",
       "      <td>12</td>\n",
       "      <td>2008</td>\n",
       "      <td>WD</td>\n",
       "      <td>Normal</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 80 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   Id  MSSubClass MSZoning  LotFrontage  LotArea Street Alley LotShape  \\\n",
       "0   1          60       RL         65.0     8450   Pave   NaN      Reg   \n",
       "1   2          20       RL         80.0     9600   Pave   NaN      Reg   \n",
       "2   3          60       RL         68.0    11250   Pave   NaN      IR1   \n",
       "3   4          70       RL         60.0     9550   Pave   NaN      IR1   \n",
       "4   5          60       RL         84.0    14260   Pave   NaN      IR1   \n",
       "\n",
       "  LandContour Utilities      ...       ScreenPorch PoolArea PoolQC Fence  \\\n",
       "0         Lvl    AllPub      ...                 0        0    NaN   NaN   \n",
       "1         Lvl    AllPub      ...                 0        0    NaN   NaN   \n",
       "2         Lvl    AllPub      ...                 0        0    NaN   NaN   \n",
       "3         Lvl    AllPub      ...                 0        0    NaN   NaN   \n",
       "4         Lvl    AllPub      ...                 0        0    NaN   NaN   \n",
       "\n",
       "  MiscFeature MiscVal MoSold  YrSold  SaleType  SaleCondition  \n",
       "0         NaN       0      2    2008        WD         Normal  \n",
       "1         NaN       0      5    2007        WD         Normal  \n",
       "2         NaN       0      9    2008        WD         Normal  \n",
       "3         NaN       0      2    2006        WD        Abnorml  \n",
       "4         NaN       0     12    2008        WD         Normal  \n",
       "\n",
       "[5 rows x 80 columns]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sale_price = data['SalePrice']\n",
    "data = data.drop(['SalePrice'],axis=1)\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Analyzing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id                  0\n",
       "MSSubClass          0\n",
       "MSZoning            0\n",
       "LotFrontage       259\n",
       "LotArea             0\n",
       "Street              0\n",
       "Alley            1369\n",
       "LotShape            0\n",
       "LandContour         0\n",
       "Utilities           0\n",
       "LotConfig           0\n",
       "LandSlope           0\n",
       "Neighborhood        0\n",
       "Condition1          0\n",
       "Condition2          0\n",
       "BldgType            0\n",
       "HouseStyle          0\n",
       "OverallQual         0\n",
       "OverallCond         0\n",
       "YearBuilt           0\n",
       "YearRemodAdd        0\n",
       "RoofStyle           0\n",
       "RoofMatl            0\n",
       "Exterior1st         0\n",
       "Exterior2nd         0\n",
       "MasVnrType          8\n",
       "MasVnrArea          8\n",
       "ExterQual           0\n",
       "ExterCond           0\n",
       "Foundation          0\n",
       "BsmtQual           37\n",
       "BsmtCond           37\n",
       "BsmtExposure       38\n",
       "BsmtFinType1       37\n",
       "BsmtFinSF1          0\n",
       "BsmtFinType2       38\n",
       "BsmtFinSF2          0\n",
       "BsmtUnfSF           0\n",
       "TotalBsmtSF         0\n",
       "Heating             0\n",
       "HeatingQC           0\n",
       "CentralAir          0\n",
       "Electrical          1\n",
       "1stFlrSF            0\n",
       "2ndFlrSF            0\n",
       "LowQualFinSF        0\n",
       "GrLivArea           0\n",
       "BsmtFullBath        0\n",
       "BsmtHalfBath        0\n",
       "FullBath            0\n",
       "HalfBath            0\n",
       "BedroomAbvGr        0\n",
       "KitchenAbvGr        0\n",
       "KitchenQual         0\n",
       "TotRmsAbvGrd        0\n",
       "Functional          0\n",
       "Fireplaces          0\n",
       "FireplaceQu       690\n",
       "GarageType         81\n",
       "GarageYrBlt        81\n",
       "GarageFinish       81\n",
       "GarageCars          0\n",
       "GarageArea          0\n",
       "GarageQual         81\n",
       "GarageCond         81\n",
       "PavedDrive          0\n",
       "WoodDeckSF          0\n",
       "OpenPorchSF         0\n",
       "EnclosedPorch       0\n",
       "3SsnPorch           0\n",
       "ScreenPorch         0\n",
       "PoolArea            0\n",
       "PoolQC           1453\n",
       "Fence            1179\n",
       "MiscFeature      1406\n",
       "MiscVal             0\n",
       "MoSold              0\n",
       "YrSold              0\n",
       "SaleType            0\n",
       "SaleCondition       0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.isnull().sum() # Check number of NaN s"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id                 int64\n",
       "MSSubClass         int64\n",
       "MSZoning          object\n",
       "LotFrontage      float64\n",
       "LotArea            int64\n",
       "Street            object\n",
       "Alley             object\n",
       "LotShape          object\n",
       "LandContour       object\n",
       "Utilities         object\n",
       "LotConfig         object\n",
       "LandSlope         object\n",
       "Neighborhood      object\n",
       "Condition1        object\n",
       "Condition2        object\n",
       "BldgType          object\n",
       "HouseStyle        object\n",
       "OverallQual        int64\n",
       "OverallCond        int64\n",
       "YearBuilt          int64\n",
       "YearRemodAdd       int64\n",
       "RoofStyle         object\n",
       "RoofMatl          object\n",
       "Exterior1st       object\n",
       "Exterior2nd       object\n",
       "MasVnrType        object\n",
       "MasVnrArea       float64\n",
       "ExterQual         object\n",
       "ExterCond         object\n",
       "Foundation        object\n",
       "BsmtQual          object\n",
       "BsmtCond          object\n",
       "BsmtExposure      object\n",
       "BsmtFinType1      object\n",
       "BsmtFinSF1         int64\n",
       "BsmtFinType2      object\n",
       "BsmtFinSF2         int64\n",
       "BsmtUnfSF          int64\n",
       "TotalBsmtSF        int64\n",
       "Heating           object\n",
       "HeatingQC         object\n",
       "CentralAir        object\n",
       "Electrical        object\n",
       "1stFlrSF           int64\n",
       "2ndFlrSF           int64\n",
       "LowQualFinSF       int64\n",
       "GrLivArea          int64\n",
       "BsmtFullBath       int64\n",
       "BsmtHalfBath       int64\n",
       "FullBath           int64\n",
       "HalfBath           int64\n",
       "BedroomAbvGr       int64\n",
       "KitchenAbvGr       int64\n",
       "KitchenQual       object\n",
       "TotRmsAbvGrd       int64\n",
       "Functional        object\n",
       "Fireplaces         int64\n",
       "FireplaceQu       object\n",
       "GarageType        object\n",
       "GarageYrBlt      float64\n",
       "GarageFinish      object\n",
       "GarageCars         int64\n",
       "GarageArea         int64\n",
       "GarageQual        object\n",
       "GarageCond        object\n",
       "PavedDrive        object\n",
       "WoodDeckSF         int64\n",
       "OpenPorchSF        int64\n",
       "EnclosedPorch      int64\n",
       "3SsnPorch          int64\n",
       "ScreenPorch        int64\n",
       "PoolArea           int64\n",
       "PoolQC            object\n",
       "Fence             object\n",
       "MiscFeature       object\n",
       "MiscVal            int64\n",
       "MoSold             int64\n",
       "YrSold             int64\n",
       "SaleType          object\n",
       "SaleCondition     object\n",
       "dtype: object"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['Id', 'MSSubClass', 'MSZoning', 'LotFrontage', 'LotArea', 'Street',\n",
       "       'Alley', 'LotShape', 'LandContour', 'Utilities', 'LotConfig',\n",
       "       'LandSlope', 'Neighborhood', 'Condition1', 'Condition2', 'BldgType',\n",
       "       'HouseStyle', 'OverallQual', 'OverallCond', 'YearBuilt', 'YearRemodAdd',\n",
       "       'RoofStyle', 'RoofMatl', 'Exterior1st', 'Exterior2nd', 'MasVnrType',\n",
       "       'MasVnrArea', 'ExterQual', 'ExterCond', 'Foundation', 'BsmtQual',\n",
       "       'BsmtCond', 'BsmtExposure', 'BsmtFinType1', 'BsmtFinSF1',\n",
       "       'BsmtFinType2', 'BsmtFinSF2', 'BsmtUnfSF', 'TotalBsmtSF', 'Heating',\n",
       "       'HeatingQC', 'CentralAir', 'Electrical', '1stFlrSF', '2ndFlrSF',\n",
       "       'LowQualFinSF', 'GrLivArea', 'BsmtFullBath', 'BsmtHalfBath', 'FullBath',\n",
       "       'HalfBath', 'BedroomAbvGr', 'KitchenAbvGr', 'KitchenQual',\n",
       "       'TotRmsAbvGrd', 'Functional', 'Fireplaces', 'FireplaceQu', 'GarageType',\n",
       "       'GarageYrBlt', 'GarageFinish', 'GarageCars', 'GarageArea', 'GarageQual',\n",
       "       'GarageCond', 'PavedDrive', 'WoodDeckSF', 'OpenPorchSF',\n",
       "       'EnclosedPorch', '3SsnPorch', 'ScreenPorch', 'PoolArea', 'PoolQC',\n",
       "       'Fence', 'MiscFeature', 'MiscVal', 'MoSold', 'YrSold', 'SaleType',\n",
       "       'SaleCondition'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['MSZoning',\n",
       " 'Street',\n",
       " 'Alley',\n",
       " 'LotShape',\n",
       " 'LandContour',\n",
       " 'Utilities',\n",
       " 'LotConfig',\n",
       " 'LandSlope',\n",
       " 'Neighborhood',\n",
       " 'Condition1',\n",
       " 'Condition2',\n",
       " 'BldgType',\n",
       " 'HouseStyle',\n",
       " 'RoofStyle',\n",
       " 'RoofMatl',\n",
       " 'Exterior1st',\n",
       " 'Exterior2nd',\n",
       " 'MasVnrType',\n",
       " 'ExterQual',\n",
       " 'ExterCond',\n",
       " 'Foundation',\n",
       " 'BsmtQual',\n",
       " 'BsmtCond',\n",
       " 'BsmtExposure',\n",
       " 'BsmtFinType1',\n",
       " 'BsmtFinType2',\n",
       " 'Heating',\n",
       " 'HeatingQC',\n",
       " 'CentralAir',\n",
       " 'Electrical',\n",
       " 'KitchenQual',\n",
       " 'Functional',\n",
       " 'FireplaceQu',\n",
       " 'GarageType',\n",
       " 'GarageFinish',\n",
       " 'GarageQual',\n",
       " 'GarageCond',\n",
       " 'PavedDrive',\n",
       " 'PoolQC',\n",
       " 'Fence',\n",
       " 'MiscFeature',\n",
       " 'SaleType',\n",
       " 'SaleCondition']"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cat_names = [i for i,v in (data.dtypes == object).items() if v]\n",
    "cat_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['Id',\n",
       " 'MSSubClass',\n",
       " 'LotFrontage',\n",
       " 'LotArea',\n",
       " 'OverallQual',\n",
       " 'OverallCond',\n",
       " 'YearBuilt',\n",
       " 'YearRemodAdd',\n",
       " 'MasVnrArea',\n",
       " 'BsmtFinSF1',\n",
       " 'BsmtFinSF2',\n",
       " 'BsmtUnfSF',\n",
       " 'TotalBsmtSF',\n",
       " '1stFlrSF',\n",
       " '2ndFlrSF',\n",
       " 'LowQualFinSF',\n",
       " 'GrLivArea',\n",
       " 'BsmtFullBath',\n",
       " 'BsmtHalfBath',\n",
       " 'FullBath',\n",
       " 'HalfBath',\n",
       " 'BedroomAbvGr',\n",
       " 'KitchenAbvGr',\n",
       " 'TotRmsAbvGrd',\n",
       " 'Fireplaces',\n",
       " 'GarageYrBlt',\n",
       " 'GarageCars',\n",
       " 'GarageArea',\n",
       " 'WoodDeckSF',\n",
       " 'OpenPorchSF',\n",
       " 'EnclosedPorch',\n",
       " '3SsnPorch',\n",
       " 'ScreenPorch',\n",
       " 'PoolArea',\n",
       " 'MiscVal',\n",
       " 'MoSold',\n",
       " 'YrSold']"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cont_names = [i for i,v in (data.dtypes == object).items() if not v]\n",
    "cont_names"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(43, 37)"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(cat_names),len(cont_names)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Change strings to categorical data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id               int64\n",
       "MSSubClass       int64\n",
       "MSZoning        object\n",
       "LotFrontage    float64\n",
       "LotArea          int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in cat_names:\n",
    "    data[n] = data[n].astype('category').cat.as_ordered()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Id                int64\n",
       "MSSubClass        int64\n",
       "MSZoning       category\n",
       "LotFrontage     float64\n",
       "LotArea           int64\n",
       "dtype: object"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['C (all)', 'FV', 'RH', 'RL', 'RM'], dtype='object')"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['MSZoning'].cat.categories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0       3\n",
       "1       3\n",
       "2       3\n",
       "3       3\n",
       "4       3\n",
       "5       3\n",
       "6       3\n",
       "7       3\n",
       "8       4\n",
       "9       3\n",
       "10      3\n",
       "11      3\n",
       "12      3\n",
       "13      3\n",
       "14      3\n",
       "15      4\n",
       "16      3\n",
       "17      3\n",
       "18      3\n",
       "19      3\n",
       "20      3\n",
       "21      4\n",
       "22      3\n",
       "23      4\n",
       "24      3\n",
       "25      3\n",
       "26      3\n",
       "27      3\n",
       "28      3\n",
       "29      4\n",
       "30      0\n",
       "31      3\n",
       "32      3\n",
       "33      3\n",
       "34      3\n",
       "35      3\n",
       "36      3\n",
       "37      3\n",
       "38      3\n",
       "39      3\n",
       "40      3\n",
       "41      3\n",
       "42      3\n",
       "43      3\n",
       "44      3\n",
       "45      3\n",
       "46      3\n",
       "47      1\n",
       "48      4\n",
       "49      3\n",
       "50      3\n",
       "51      4\n",
       "52      4\n",
       "53      3\n",
       "54      3\n",
       "55      3\n",
       "56      1\n",
       "57      3\n",
       "58      3\n",
       "59      3\n",
       "60      3\n",
       "61      4\n",
       "62      3\n",
       "63      4\n",
       "64      3\n",
       "65      3\n",
       "66      3\n",
       "67      3\n",
       "68      4\n",
       "69      3\n",
       "70      3\n",
       "71      3\n",
       "72      3\n",
       "73      3\n",
       "74      4\n",
       "75      4\n",
       "76      3\n",
       "77      4\n",
       "78      3\n",
       "79      4\n",
       "80      3\n",
       "81      4\n",
       "82      3\n",
       "83      3\n",
       "84      3\n",
       "85      3\n",
       "86      3\n",
       "87      1\n",
       "88      0\n",
       "89      3\n",
       "90      3\n",
       "91      3\n",
       "92      3\n",
       "93      0\n",
       "94      3\n",
       "95      3\n",
       "96      3\n",
       "97      3\n",
       "98      3\n",
       "99      3\n",
       "100     3\n",
       "101     3\n",
       "102     3\n",
       "103     3\n",
       "104     4\n",
       "105     1\n",
       "106     4\n",
       "107     4\n",
       "108     4\n",
       "109     3\n",
       "110     3\n",
       "111     3\n",
       "112     3\n",
       "113     3\n",
       "114     3\n",
       "115     1\n",
       "116     3\n",
       "117     3\n",
       "118     3\n",
       "119     3\n",
       "120     3\n",
       "121     4\n",
       "122     3\n",
       "123     3\n",
       "124     3\n",
       "125     4\n",
       "126     3\n",
       "127     4\n",
       "128     3\n",
       "129     3\n",
       "130     3\n",
       "131     3\n",
       "132     3\n",
       "133     3\n",
       "134     3\n",
       "135     3\n",
       "136     3\n",
       "137     3\n",
       "138     3\n",
       "139     3\n",
       "140     3\n",
       "141     3\n",
       "142     3\n",
       "143     3\n",
       "144     4\n",
       "145     4\n",
       "146     4\n",
       "147     3\n",
       "148     3\n",
       "149     4\n",
       "150     3\n",
       "151     3\n",
       "152     3\n",
       "153     3\n",
       "154     4\n",
       "155     3\n",
       "156     3\n",
       "157     3\n",
       "158     1\n",
       "159     3\n",
       "160     3\n",
       "161     3\n",
       "162     3\n",
       "163     3\n",
       "164     4\n",
       "165     3\n",
       "166     3\n",
       "167     3\n",
       "168     3\n",
       "169     3\n",
       "170     4\n",
       "171     3\n",
       "172     3\n",
       "173     3\n",
       "174     3\n",
       "175     3\n",
       "176     3\n",
       "177     3\n",
       "178     3\n",
       "179     4\n",
       "180     1\n",
       "181     3\n",
       "182     3\n",
       "183     4\n",
       "184     3\n",
       "185     4\n",
       "186     3\n",
       "187     3\n",
       "188     3\n",
       "189     3\n",
       "190     3\n",
       "191     3\n",
       "192     3\n",
       "193     4\n",
       "194     3\n",
       "195     3\n",
       "196     3\n",
       "197     3\n",
       "198     4\n",
       "199     3\n",
       "200     4\n",
       "201     3\n",
       "202     3\n",
       "203     4\n",
       "204     4\n",
       "205     3\n",
       "206     3\n",
       "207     3\n",
       "208     3\n",
       "209     3\n",
       "210     3\n",
       "211     3\n",
       "212     1\n",
       "213     3\n",
       "214     3\n",
       "215     3\n",
       "216     3\n",
       "217     4\n",
       "218     3\n",
       "219     3\n",
       "220     3\n",
       "221     3\n",
       "222     3\n",
       "223     3\n",
       "224     3\n",
       "225     4\n",
       "226     3\n",
       "227     4\n",
       "228     3\n",
       "229     3\n",
       "230     3\n",
       "231     3\n",
       "232     4\n",
       "233     3\n",
       "234     3\n",
       "235     4\n",
       "236     3\n",
       "237     3\n",
       "238     3\n",
       "239     3\n",
       "240     1\n",
       "241     4\n",
       "242     4\n",
       "243     3\n",
       "244     3\n",
       "245     3\n",
       "246     4\n",
       "247     3\n",
       "248     3\n",
       "249     3\n",
       "250     3\n",
       "251     4\n",
       "252     3\n",
       "253     3\n",
       "254     3\n",
       "255     3\n",
       "256     1\n",
       "257     3\n",
       "258     3\n",
       "259     4\n",
       "260     3\n",
       "261     3\n",
       "262     3\n",
       "263     4\n",
       "264     4\n",
       "265     3\n",
       "266     3\n",
       "267     3\n",
       "268     4\n",
       "269     3\n",
       "270     1\n",
       "271     3\n",
       "272     3\n",
       "273     3\n",
       "274     3\n",
       "275     3\n",
       "276     3\n",
       "277     3\n",
       "278     3\n",
       "279     3\n",
       "280     3\n",
       "281     1\n",
       "282     3\n",
       "283     3\n",
       "284     3\n",
       "285     1\n",
       "286     3\n",
       "287     3\n",
       "288     3\n",
       "289     3\n",
       "290     3\n",
       "291     3\n",
       "292     3\n",
       "293     3\n",
       "294     3\n",
       "295     3\n",
       "296     4\n",
       "297     1\n",
       "298     3\n",
       "299     3\n",
       "300     3\n",
       "301     3\n",
       "302     3\n",
       "303     3\n",
       "304     4\n",
       "305     3\n",
       "306     3\n",
       "307     4\n",
       "308     3\n",
       "309     3\n",
       "310     3\n",
       "311     3\n",
       "312     4\n",
       "313     3\n",
       "314     4\n",
       "315     3\n",
       "316     3\n",
       "317     1\n",
       "318     3\n",
       "319     3\n",
       "320     3\n",
       "321     3\n",
       "322     3\n",
       "323     4\n",
       "324     3\n",
       "325     4\n",
       "326     3\n",
       "327     3\n",
       "328     3\n",
       "329     4\n",
       "330     3\n",
       "331     3\n",
       "332     3\n",
       "333     4\n",
       "334     3\n",
       "335     3\n",
       "336     3\n",
       "337     3\n",
       "338     3\n",
       "339     3\n",
       "340     3\n",
       "341     2\n",
       "342     3\n",
       "343     3\n",
       "344     4\n",
       "345     3\n",
       "346     3\n",
       "347     3\n",
       "348     3\n",
       "349     3\n",
       "350     3\n",
       "351     3\n",
       "352     3\n",
       "353     4\n",
       "354     3\n",
       "355     3\n",
       "356     3\n",
       "357     4\n",
       "358     3\n",
       "359     3\n",
       "360     3\n",
       "361     3\n",
       "362     3\n",
       "363     4\n",
       "364     3\n",
       "365     4\n",
       "366     3\n",
       "367     3\n",
       "368     3\n",
       "369     3\n",
       "370     3\n",
       "371     3\n",
       "372     3\n",
       "373     3\n",
       "374     3\n",
       "375     3\n",
       "376     3\n",
       "377     1\n",
       "378     3\n",
       "379     3\n",
       "380     3\n",
       "381     1\n",
       "382     3\n",
       "383     2\n",
       "384     3\n",
       "385     3\n",
       "386     3\n",
       "387     3\n",
       "388     3\n",
       "389     3\n",
       "390     3\n",
       "391     3\n",
       "392     3\n",
       "393     3\n",
       "394     3\n",
       "395     3\n",
       "396     3\n",
       "397     3\n",
       "398     4\n",
       "399     1\n",
       "400     3\n",
       "401     3\n",
       "402     3\n",
       "403     3\n",
       "404     3\n",
       "405     3\n",
       "406     3\n",
       "407     3\n",
       "408     3\n",
       "409     1\n",
       "410     3\n",
       "411     3\n",
       "412     1\n",
       "413     4\n",
       "414     3\n",
       "415     3\n",
       "416     3\n",
       "417     3\n",
       "418     3\n",
       "419     3\n",
       "420     4\n",
       "421     3\n",
       "422     3\n",
       "423     3\n",
       "424     3\n",
       "425     4\n",
       "426     3\n",
       "427     3\n",
       "428     3\n",
       "429     3\n",
       "430     4\n",
       "431     4\n",
       "432     4\n",
       "433     3\n",
       "434     4\n",
       "435     3\n",
       "436     4\n",
       "437     4\n",
       "438     3\n",
       "439     3\n",
       "440     3\n",
       "441     3\n",
       "442     4\n",
       "443     3\n",
       "444     3\n",
       "445     3\n",
       "446     3\n",
       "447     3\n",
       "448     4\n",
       "449     4\n",
       "450     4\n",
       "451     3\n",
       "452     3\n",
       "453     1\n",
       "454     3\n",
       "455     3\n",
       "456     4\n",
       "457     3\n",
       "458     4\n",
       "459     3\n",
       "460     1\n",
       "461     3\n",
       "462     3\n",
       "463     3\n",
       "464     3\n",
       "465     4\n",
       "466     3\n",
       "467     3\n",
       "468     3\n",
       "469     3\n",
       "470     3\n",
       "471     3\n",
       "472     4\n",
       "473     3\n",
       "474     3\n",
       "475     3\n",
       "476     3\n",
       "477     3\n",
       "478     3\n",
       "479     4\n",
       "480     3\n",
       "481     3\n",
       "482     4\n",
       "483     4\n",
       "484     3\n",
       "485     3\n",
       "486     3\n",
       "487     3\n",
       "488     3\n",
       "489     4\n",
       "490     4\n",
       "491     3\n",
       "492     3\n",
       "493     3\n",
       "494     4\n",
       "495     0\n",
       "496     3\n",
       "497     3\n",
       "498     3\n",
       "499     3\n",
       "       ..\n",
       "960     3\n",
       "961     3\n",
       "962     3\n",
       "963     3\n",
       "964     3\n",
       "965     3\n",
       "966     3\n",
       "967     3\n",
       "968     4\n",
       "969     3\n",
       "970     3\n",
       "971     3\n",
       "972     3\n",
       "973     1\n",
       "974     3\n",
       "975     1\n",
       "976     3\n",
       "977     1\n",
       "978     3\n",
       "979     3\n",
       "980     3\n",
       "981     3\n",
       "982     3\n",
       "983     3\n",
       "984     3\n",
       "985     3\n",
       "986     4\n",
       "987     3\n",
       "988     3\n",
       "989     1\n",
       "990     3\n",
       "991     4\n",
       "992     3\n",
       "993     3\n",
       "994     3\n",
       "995     3\n",
       "996     3\n",
       "997     3\n",
       "998     4\n",
       "999     3\n",
       "1000    3\n",
       "1001    3\n",
       "1002    3\n",
       "1003    3\n",
       "1004    3\n",
       "1005    3\n",
       "1006    3\n",
       "1007    4\n",
       "1008    3\n",
       "1009    3\n",
       "1010    3\n",
       "1011    3\n",
       "1012    3\n",
       "1013    4\n",
       "1014    3\n",
       "1015    3\n",
       "1016    3\n",
       "1017    3\n",
       "1018    3\n",
       "1019    3\n",
       "1020    3\n",
       "1021    3\n",
       "1022    4\n",
       "1023    3\n",
       "1024    3\n",
       "1025    3\n",
       "1026    3\n",
       "1027    3\n",
       "1028    3\n",
       "1029    4\n",
       "1030    2\n",
       "1031    3\n",
       "1032    3\n",
       "1033    3\n",
       "1034    3\n",
       "1035    3\n",
       "1036    3\n",
       "1037    3\n",
       "1038    4\n",
       "1039    4\n",
       "1040    3\n",
       "1041    3\n",
       "1042    3\n",
       "1043    3\n",
       "1044    3\n",
       "1045    3\n",
       "1046    3\n",
       "1047    3\n",
       "1048    3\n",
       "1049    3\n",
       "1050    3\n",
       "1051    3\n",
       "1052    3\n",
       "1053    3\n",
       "1054    3\n",
       "1055    3\n",
       "1056    3\n",
       "1057    3\n",
       "1058    3\n",
       "1059    3\n",
       "1060    3\n",
       "1061    0\n",
       "1062    4\n",
       "1063    4\n",
       "1064    3\n",
       "1065    3\n",
       "1066    3\n",
       "1067    3\n",
       "1068    4\n",
       "1069    3\n",
       "1070    3\n",
       "1071    3\n",
       "1072    3\n",
       "1073    3\n",
       "1074    3\n",
       "1075    3\n",
       "1076    3\n",
       "1077    3\n",
       "1078    4\n",
       "1079    3\n",
       "1080    3\n",
       "1081    3\n",
       "1082    3\n",
       "1083    3\n",
       "1084    3\n",
       "1085    3\n",
       "1086    4\n",
       "1087    1\n",
       "1088    4\n",
       "1089    1\n",
       "1090    3\n",
       "1091    1\n",
       "1092    3\n",
       "1093    3\n",
       "1094    3\n",
       "1095    3\n",
       "1096    4\n",
       "1097    3\n",
       "1098    4\n",
       "1099    3\n",
       "1100    3\n",
       "1101    3\n",
       "1102    3\n",
       "1103    3\n",
       "1104    4\n",
       "1105    3\n",
       "1106    3\n",
       "1107    3\n",
       "1108    3\n",
       "1109    3\n",
       "1110    3\n",
       "1111    3\n",
       "1112    3\n",
       "1113    3\n",
       "1114    3\n",
       "1115    3\n",
       "1116    3\n",
       "1117    3\n",
       "1118    3\n",
       "1119    3\n",
       "1120    4\n",
       "1121    3\n",
       "1122    3\n",
       "1123    3\n",
       "1124    3\n",
       "1125    3\n",
       "1126    3\n",
       "1127    3\n",
       "1128    3\n",
       "1129    4\n",
       "1130    3\n",
       "1131    3\n",
       "1132    4\n",
       "1133    3\n",
       "1134    3\n",
       "1135    4\n",
       "1136    3\n",
       "1137    3\n",
       "1138    3\n",
       "1139    3\n",
       "1140    3\n",
       "1141    3\n",
       "1142    3\n",
       "1143    3\n",
       "1144    3\n",
       "1145    4\n",
       "1146    3\n",
       "1147    3\n",
       "1148    4\n",
       "1149    4\n",
       "1150    3\n",
       "1151    3\n",
       "1152    3\n",
       "1153    4\n",
       "1154    3\n",
       "1155    3\n",
       "1156    3\n",
       "1157    3\n",
       "1158    3\n",
       "1159    3\n",
       "1160    3\n",
       "1161    3\n",
       "1162    3\n",
       "1163    3\n",
       "1164    3\n",
       "1165    3\n",
       "1166    3\n",
       "1167    3\n",
       "1168    3\n",
       "1169    3\n",
       "1170    3\n",
       "1171    3\n",
       "1172    1\n",
       "1173    3\n",
       "1174    3\n",
       "1175    3\n",
       "1176    3\n",
       "1177    4\n",
       "1178    3\n",
       "1179    3\n",
       "1180    3\n",
       "1181    4\n",
       "1182    3\n",
       "1183    3\n",
       "1184    3\n",
       "1185    3\n",
       "1186    3\n",
       "1187    3\n",
       "1188    3\n",
       "1189    3\n",
       "1190    3\n",
       "1191    1\n",
       "1192    4\n",
       "1193    4\n",
       "1194    3\n",
       "1195    3\n",
       "1196    3\n",
       "1197    4\n",
       "1198    3\n",
       "1199    3\n",
       "1200    3\n",
       "1201    3\n",
       "1202    4\n",
       "1203    3\n",
       "1204    3\n",
       "1205    3\n",
       "1206    2\n",
       "1207    3\n",
       "1208    3\n",
       "1209    3\n",
       "1210    3\n",
       "1211    3\n",
       "1212    3\n",
       "1213    3\n",
       "1214    3\n",
       "1215    3\n",
       "1216    4\n",
       "1217    1\n",
       "1218    4\n",
       "1219    4\n",
       "1220    3\n",
       "1221    3\n",
       "1222    3\n",
       "1223    3\n",
       "1224    3\n",
       "1225    3\n",
       "1226    3\n",
       "1227    3\n",
       "1228    3\n",
       "1229    3\n",
       "1230    3\n",
       "1231    3\n",
       "1232    3\n",
       "1233    3\n",
       "1234    2\n",
       "1235    3\n",
       "1236    3\n",
       "1237    3\n",
       "1238    3\n",
       "1239    3\n",
       "1240    3\n",
       "1241    3\n",
       "1242    3\n",
       "1243    3\n",
       "1244    3\n",
       "1245    3\n",
       "1246    1\n",
       "1247    3\n",
       "1248    4\n",
       "1249    3\n",
       "1250    3\n",
       "1251    3\n",
       "1252    3\n",
       "1253    3\n",
       "1254    3\n",
       "1255    4\n",
       "1256    3\n",
       "1257    3\n",
       "1258    3\n",
       "1259    3\n",
       "1260    3\n",
       "1261    3\n",
       "1262    3\n",
       "1263    3\n",
       "1264    2\n",
       "1265    1\n",
       "1266    4\n",
       "1267    3\n",
       "1268    3\n",
       "1269    3\n",
       "1270    3\n",
       "1271    3\n",
       "1272    3\n",
       "1273    3\n",
       "1274    3\n",
       "1275    3\n",
       "1276    3\n",
       "1277    3\n",
       "1278    3\n",
       "1279    0\n",
       "1280    3\n",
       "1281    3\n",
       "1282    3\n",
       "1283    3\n",
       "1284    3\n",
       "1285    4\n",
       "1286    3\n",
       "1287    3\n",
       "1288    3\n",
       "1289    3\n",
       "1290    3\n",
       "1291    4\n",
       "1292    4\n",
       "1293    3\n",
       "1294    3\n",
       "1295    3\n",
       "1296    3\n",
       "1297    4\n",
       "1298    3\n",
       "1299    3\n",
       "1300    3\n",
       "1301    3\n",
       "1302    3\n",
       "1303    3\n",
       "1304    4\n",
       "1305    3\n",
       "1306    3\n",
       "1307    3\n",
       "1308    4\n",
       "1309    3\n",
       "1310    3\n",
       "1311    3\n",
       "1312    3\n",
       "1313    3\n",
       "1314    3\n",
       "1315    3\n",
       "1316    3\n",
       "1317    1\n",
       "1318    3\n",
       "1319    3\n",
       "1320    3\n",
       "1321    3\n",
       "1322    3\n",
       "1323    3\n",
       "1324    3\n",
       "1325    4\n",
       "1326    2\n",
       "1327    3\n",
       "1328    4\n",
       "1329    3\n",
       "1330    3\n",
       "1331    3\n",
       "1332    3\n",
       "1333    4\n",
       "1334    4\n",
       "1335    3\n",
       "1336    3\n",
       "1337    4\n",
       "1338    3\n",
       "1339    3\n",
       "1340    3\n",
       "1341    3\n",
       "1342    3\n",
       "1343    3\n",
       "1344    3\n",
       "1345    4\n",
       "1346    3\n",
       "1347    3\n",
       "1348    3\n",
       "1349    4\n",
       "1350    3\n",
       "1351    3\n",
       "1352    4\n",
       "1353    3\n",
       "1354    3\n",
       "1355    3\n",
       "1356    3\n",
       "1357    3\n",
       "1358    1\n",
       "1359    3\n",
       "1360    3\n",
       "1361    3\n",
       "1362    3\n",
       "1363    3\n",
       "1364    1\n",
       "1365    1\n",
       "1366    3\n",
       "1367    4\n",
       "1368    4\n",
       "1369    3\n",
       "1370    3\n",
       "1371    3\n",
       "1372    3\n",
       "1373    3\n",
       "1374    1\n",
       "1375    3\n",
       "1376    3\n",
       "1377    3\n",
       "1378    4\n",
       "1379    3\n",
       "1380    3\n",
       "1381    3\n",
       "1382    4\n",
       "1383    3\n",
       "1384    3\n",
       "1385    4\n",
       "1386    3\n",
       "1387    4\n",
       "1388    3\n",
       "1389    4\n",
       "1390    3\n",
       "1391    3\n",
       "1392    3\n",
       "1393    4\n",
       "1394    3\n",
       "1395    3\n",
       "1396    3\n",
       "1397    4\n",
       "1398    3\n",
       "1399    3\n",
       "1400    4\n",
       "1401    3\n",
       "1402    3\n",
       "1403    3\n",
       "1404    3\n",
       "1405    4\n",
       "1406    3\n",
       "1407    3\n",
       "1408    4\n",
       "1409    3\n",
       "1410    3\n",
       "1411    3\n",
       "1412    3\n",
       "1413    3\n",
       "1414    3\n",
       "1415    3\n",
       "1416    4\n",
       "1417    3\n",
       "1418    3\n",
       "1419    3\n",
       "1420    3\n",
       "1421    3\n",
       "1422    4\n",
       "1423    3\n",
       "1424    3\n",
       "1425    3\n",
       "1426    3\n",
       "1427    3\n",
       "1428    4\n",
       "1429    3\n",
       "1430    3\n",
       "1431    3\n",
       "1432    3\n",
       "1433    3\n",
       "1434    3\n",
       "1435    3\n",
       "1436    3\n",
       "1437    3\n",
       "1438    4\n",
       "1439    3\n",
       "1440    3\n",
       "1441    4\n",
       "1442    1\n",
       "1443    3\n",
       "1444    3\n",
       "1445    3\n",
       "1446    3\n",
       "1447    3\n",
       "1448    3\n",
       "1449    4\n",
       "1450    3\n",
       "1451    3\n",
       "1452    4\n",
       "1453    3\n",
       "1454    1\n",
       "1455    3\n",
       "1456    3\n",
       "1457    3\n",
       "1458    3\n",
       "1459    3\n",
       "Length: 1460, dtype: int8"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['MSZoning'].cat.codes"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fill all NaNs with median when continues data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "for n in cont_names:\n",
    "    if pd.isnull(data[n]).sum():\n",
    "        filler = data[n].median()\n",
    "        data[n] = data[n].fillna(filler)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Turn dtata into torch format. We take log of sale price because evaluation of competition demand us to do so."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "sale_price = torch.tensor(np.log(sale_price).values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([12.2477, 12.1090, 12.3172,  ..., 12.4931, 11.8645, 11.9016],\n",
       "       dtype=torch.float64)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sale_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cat_names and len(cat_names) >= 1: # categorical data\n",
    "    cats = np.stack([c.cat.codes.values for n,c in data[cat_names].items()], 1) + 1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 43)"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "cats = torch.LongTensor(cats.astype(np.int64))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[4, 2, 0,  ..., 0, 9, 5],\n",
       "        [4, 2, 0,  ..., 0, 9, 5],\n",
       "        [4, 2, 0,  ..., 0, 9, 5],\n",
       "        ...,\n",
       "        [4, 2, 0,  ..., 3, 9, 5],\n",
       "        [4, 2, 0,  ..., 0, 9, 5],\n",
       "        [4, 2, 0,  ..., 0, 9, 5]])"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Normalizing data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "if cont_names and len(cont_names) >= 1:\n",
    "    conts = np.stack([c.astype('float32').values for n,c in data[cont_names].items()], 1)\n",
    "    means, stds = (conts.mean(0), conts.std(0))\n",
    "    conts = (conts - means[None]) / stds[None]\n",
    "    stats = means,stds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(1460, 37)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "conts = torch.FloatTensor(conts)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-1.7309,  0.0734, -0.2209,  ..., -0.0877, -1.5991,  0.1387],\n",
       "        [-1.7285, -0.8726,  0.4603,  ..., -0.0877, -0.4891, -0.6145],\n",
       "        [-1.7261,  0.0734, -0.0846,  ..., -0.0877,  0.9909,  0.1387],\n",
       "        ...,\n",
       "        [ 1.7261,  0.3099, -0.1755,  ...,  4.9531, -0.4891,  1.6452],\n",
       "        [ 1.7285, -0.8726, -0.0846,  ..., -0.0877, -0.8591,  1.6452],\n",
       "        [ 1.7309, -0.8726,  0.2333,  ..., -0.0877, -0.1191,  0.1387]])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "conts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(6, 3), (3, 2), (3, 2), (5, 3), (5, 3), (3, 2), (6, 3), (4, 2), (26, 13), (10, 5), (9, 5), (6, 3), (9, 5), (7, 4), (9, 5), (16, 8), (17, 9), (5, 3), (5, 3), (6, 3), (7, 4), (5, 3), (5, 3), (5, 3), (7, 4), (7, 4), (7, 4), (6, 3), (3, 2), (6, 3), (5, 3), (8, 4), (6, 3), (7, 4), (4, 2), (6, 3), (6, 3), (4, 2), (4, 2), (5, 3), (5, 3), (10, 5), (7, 4)]\n"
     ]
    }
   ],
   "source": [
    "cat_szs = [len(data[n].cat.categories)+1 for n in cat_names]\n",
    "emb_szs = [(c, min(50, (c+1)//2)) for c in cat_szs]\n",
    "print(emb_szs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "x_data_conts = conts\n",
    "x_data_cats  = cats\n",
    "y_data = sale_price"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1460, 37])"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data_conts.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1460, 43])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data_cats.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([1460])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_data.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And then we split data into train, valid and test sets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "x_train_cats: torch.Size([1168, 43])\n",
      "x_train_conts: torch.Size([1168, 37])\n",
      "y_train: torch.Size([1168])\n",
      "x_valid_cats: torch.Size([73, 43])\n",
      "x_valid_conts: torch.Size([73, 37])\n",
      "y_valid: torch.Size([73])\n",
      "x_test_cats: torch.Size([219, 43])\n",
      "x_test_conts: torch.Size([219, 37])\n",
      "y_test: torch.Size([219])\n"
     ]
    }
   ],
   "source": [
    "train_size = 0.8\n",
    "valid_size = 0.25\n",
    "\n",
    "train_cnt = floor(x_data_cats.shape[0] * train_size)\n",
    "\n",
    "x_train_cats  = x_data_cats[0:train_cnt]\n",
    "x_train_conts = x_data_conts[0:train_cnt]\n",
    "y_train = y_data[0:train_cnt]\n",
    "\n",
    "valid_cnt = floor((x_data_cats.shape[0] - train_cnt) * valid_size)\n",
    "\n",
    "x_valid_cats  = x_data_cats[train_cnt:train_cnt+valid_cnt]\n",
    "x_valid_conts = x_data_conts[train_cnt:train_cnt+valid_cnt]\n",
    "y_valid = y_data[train_cnt:train_cnt+valid_cnt]\n",
    "\n",
    "x_test_cats  = x_data_cats[train_cnt+valid_cnt:]\n",
    "x_test_conts = x_data_conts[train_cnt+valid_cnt:]\n",
    "y_test = y_data[train_cnt+valid_cnt:]\n",
    "\n",
    "print(\"x_train_cats:\",x_train_cats.shape)\n",
    "print(\"x_train_conts:\",x_train_conts.shape)\n",
    "print(\"y_train:\",y_train.shape)\n",
    "print(\"x_valid_cats:\",x_valid_cats.shape)\n",
    "print(\"x_valid_conts:\",x_valid_conts.shape)\n",
    "print(\"y_valid:\",y_valid.shape)\n",
    "print(\"x_test_cats:\",x_test_cats.shape)\n",
    "print(\"x_test_conts:\",x_test_conts.shape)\n",
    "print(\"y_test:\",y_test.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Network Parameters\n",
    "learning_rate = 1e-2\n",
    "epochs = 500\n",
    "batch_size = 64\n",
    "num_input = x_data_conts.shape[1]\n",
    "num_classes = 1\n",
    "num_hidden_1 = 200\n",
    "num_hidden_2 = 200\n",
    "num_hidden_3 = 100\n",
    "num_hidden_4 = 100"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [],
   "source": [
    "class NeuralNetwork(nn.Module):\n",
    "    def __init__(self, num_classes, emb_szs, n_cont):\n",
    "        super().__init__()\n",
    "        \n",
    "        self.embeds = nn.ModuleList([nn.Embedding(ne,de) for ne,de in emb_szs])\n",
    "        n_emb = sum(e.embedding_dim for e in self.embeds)\n",
    "        self.n_emb,self.n_cont = n_emb,n_cont\n",
    "        \n",
    "        \n",
    "        self.layer1 = nn.Sequential(\n",
    "            nn.Linear(self.n_emb+self.n_cont,num_hidden_1),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(num_hidden_1),\n",
    "            nn.Dropout())\n",
    "        self.layer2 = nn.Sequential(\n",
    "            nn.Linear(num_hidden_1,num_hidden_2),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(num_hidden_2),\n",
    "            nn.Dropout())\n",
    "        self.layer3 = nn.Sequential(\n",
    "            nn.Linear(num_hidden_2,num_hidden_3),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(num_hidden_3),\n",
    "            nn.Dropout())\n",
    "        self.layer4 = nn.Sequential(\n",
    "            nn.Linear(num_hidden_3,num_hidden_4),\n",
    "            nn.ReLU(),\n",
    "            nn.BatchNorm1d(num_hidden_4),\n",
    "            nn.Dropout())\n",
    "        self.layer5 = nn.Sequential(\n",
    "            nn.Linear(num_hidden_4,num_classes))\n",
    "        \n",
    "    def forward(self, x_cat, x_cont):\n",
    "        x = [e(x_cat[:,i]) for i,e in enumerate(self.embeds)]\n",
    "        x = torch.cat(x, 1)\n",
    "        x = torch.cat([x, x_cont], 1)\n",
    "        \n",
    "        out = self.layer1(x)\n",
    "        out = self.layer2(out)\n",
    "        out = self.layer3(out)\n",
    "        out = self.layer4(out)\n",
    "        out = self.layer5(out)\n",
    "        return out"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = NeuralNetwork(num_classes,emb_szs=emb_szs,n_cont=len(cont_names)).to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = torch.utils.data.TensorDataset(x_train_conts,x_train_cats,y_train)\n",
    "train_loader = torch.utils.data.DataLoader(train_data,batch_size=batch_size,shuffle=False)\n",
    "\n",
    "valid_data = torch.utils.data.TensorDataset(x_valid_conts,x_valid_cats,y_valid)\n",
    "valid_loader = torch.utils.data.DataLoader(valid_data,batch_size=batch_size,shuffle=False)\n",
    "\n",
    "test_data = torch.utils.data.TensorDataset(x_test_conts,x_test_cats,y_test)\n",
    "test_loader = torch.utils.data.DataLoader(test_data,batch_size=batch_size,shuffle=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss and optimizer\n",
    "criterion = nn.MSELoss()\n",
    "optimizer = torch.optim.Adam(model.parameters(), lr=learning_rate)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "costs = []\n",
    "lrn_rate_sizes = []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/500], Step [1/18], Train_Loss: 142.9024, Valid_Loss: 143.3448\n",
      "Epoch [2/500], Step [1/18], Train_Loss: 74.9722, Valid_Loss: 75.5546\n",
      "Epoch [3/500], Step [1/18], Train_Loss: 4.1440, Valid_Loss: 4.3436\n",
      "Epoch [4/500], Step [1/18], Train_Loss: 2.7536, Valid_Loss: 3.0897\n",
      "Epoch [5/500], Step [1/18], Train_Loss: 2.5735, Valid_Loss: 2.5031\n",
      "Epoch [6/500], Step [1/18], Train_Loss: 2.4933, Valid_Loss: 2.2625\n",
      "Epoch [7/500], Step [1/18], Train_Loss: 2.2255, Valid_Loss: 2.6660\n",
      "Epoch [8/500], Step [1/18], Train_Loss: 2.1227, Valid_Loss: 2.2382\n",
      "Epoch [9/500], Step [1/18], Train_Loss: 2.0156, Valid_Loss: 2.4030\n",
      "Epoch [10/500], Step [1/18], Train_Loss: 1.4478, Valid_Loss: 1.6835\n",
      "Epoch [11/500], Step [1/18], Train_Loss: 2.0659, Valid_Loss: 2.4636\n",
      "Epoch [12/500], Step [1/18], Train_Loss: 1.3461, Valid_Loss: 1.7174\n",
      "Epoch [13/500], Step [1/18], Train_Loss: 1.5762, Valid_Loss: 1.6120\n",
      "Epoch [14/500], Step [1/18], Train_Loss: 1.3575, Valid_Loss: 1.6430\n",
      "Epoch [15/500], Step [1/18], Train_Loss: 1.7657, Valid_Loss: 2.1028\n",
      "Epoch [16/500], Step [1/18], Train_Loss: 1.6778, Valid_Loss: 1.8930\n",
      "Epoch [17/500], Step [1/18], Train_Loss: 1.0907, Valid_Loss: 1.1368\n",
      "Epoch [18/500], Step [1/18], Train_Loss: 2.4132, Valid_Loss: 2.0337\n",
      "Epoch [19/500], Step [1/18], Train_Loss: 1.5793, Valid_Loss: 1.7066\n",
      "Epoch [20/500], Step [1/18], Train_Loss: 1.0268, Valid_Loss: 1.2770\n",
      "Epoch [21/500], Step [1/18], Train_Loss: 1.1646, Valid_Loss: 1.6301\n",
      "Epoch [22/500], Step [1/18], Train_Loss: 1.4560, Valid_Loss: 1.6316\n",
      "Epoch [23/500], Step [1/18], Train_Loss: 1.1925, Valid_Loss: 1.6733\n",
      "Epoch [24/500], Step [1/18], Train_Loss: 1.4432, Valid_Loss: 1.5252\n",
      "Epoch [25/500], Step [1/18], Train_Loss: 1.9223, Valid_Loss: 2.0760\n",
      "Epoch [26/500], Step [1/18], Train_Loss: 1.3570, Valid_Loss: 1.5679\n",
      "Epoch [27/500], Step [1/18], Train_Loss: 1.3090, Valid_Loss: 1.3851\n",
      "Epoch [28/500], Step [1/18], Train_Loss: 1.1089, Valid_Loss: 1.2599\n",
      "Epoch [29/500], Step [1/18], Train_Loss: 1.1734, Valid_Loss: 1.2620\n",
      "Epoch [30/500], Step [1/18], Train_Loss: 1.0763, Valid_Loss: 1.3135\n",
      "Epoch [31/500], Step [1/18], Train_Loss: 1.8247, Valid_Loss: 1.9684\n",
      "Epoch [32/500], Step [1/18], Train_Loss: 1.0660, Valid_Loss: 1.4102\n",
      "Epoch [33/500], Step [1/18], Train_Loss: 1.4893, Valid_Loss: 1.5684\n",
      "Epoch [34/500], Step [1/18], Train_Loss: 1.0345, Valid_Loss: 1.2616\n",
      "Epoch [35/500], Step [1/18], Train_Loss: 1.2315, Valid_Loss: 1.8088\n",
      "Epoch [36/500], Step [1/18], Train_Loss: 1.4430, Valid_Loss: 1.6453\n",
      "Epoch [37/500], Step [1/18], Train_Loss: 1.0250, Valid_Loss: 1.0430\n",
      "Epoch [38/500], Step [1/18], Train_Loss: 1.5289, Valid_Loss: 2.1917\n",
      "Epoch [39/500], Step [1/18], Train_Loss: 2.1046, Valid_Loss: 2.1003\n",
      "Epoch [40/500], Step [1/18], Train_Loss: 0.9996, Valid_Loss: 1.2372\n",
      "Epoch [41/500], Step [1/18], Train_Loss: 1.9197, Valid_Loss: 2.0699\n",
      "Epoch [42/500], Step [1/18], Train_Loss: 1.3322, Valid_Loss: 1.4342\n",
      "Epoch [43/500], Step [1/18], Train_Loss: 1.3310, Valid_Loss: 1.6690\n",
      "Epoch [44/500], Step [1/18], Train_Loss: 0.9715, Valid_Loss: 1.2791\n",
      "Epoch [45/500], Step [1/18], Train_Loss: 1.4035, Valid_Loss: 1.5546\n",
      "Epoch [46/500], Step [1/18], Train_Loss: 2.0781, Valid_Loss: 2.1512\n",
      "Epoch [47/500], Step [1/18], Train_Loss: 1.4226, Valid_Loss: 1.5867\n",
      "Epoch [48/500], Step [1/18], Train_Loss: 1.2365, Valid_Loss: 1.1783\n",
      "Epoch [49/500], Step [1/18], Train_Loss: 1.4378, Valid_Loss: 1.7920\n",
      "Epoch [50/500], Step [1/18], Train_Loss: 1.9424, Valid_Loss: 1.8118\n",
      "Epoch [51/500], Step [1/18], Train_Loss: 1.4824, Valid_Loss: 1.9020\n",
      "Epoch [52/500], Step [1/18], Train_Loss: 0.7535, Valid_Loss: 1.2354\n",
      "Epoch [53/500], Step [1/18], Train_Loss: 1.2191, Valid_Loss: 1.7784\n",
      "Epoch [54/500], Step [1/18], Train_Loss: 1.4194, Valid_Loss: 1.5576\n",
      "Epoch [55/500], Step [1/18], Train_Loss: 1.4475, Valid_Loss: 1.6241\n",
      "Epoch [56/500], Step [1/18], Train_Loss: 1.0847, Valid_Loss: 1.3689\n",
      "Epoch [57/500], Step [1/18], Train_Loss: 1.2191, Valid_Loss: 1.3712\n",
      "Epoch [58/500], Step [1/18], Train_Loss: 1.3511, Valid_Loss: 1.4653\n",
      "Epoch [59/500], Step [1/18], Train_Loss: 1.5519, Valid_Loss: 1.6403\n",
      "Epoch [60/500], Step [1/18], Train_Loss: 1.2108, Valid_Loss: 1.6808\n",
      "Epoch [61/500], Step [1/18], Train_Loss: 1.3537, Valid_Loss: 1.6599\n",
      "Epoch [62/500], Step [1/18], Train_Loss: 1.1505, Valid_Loss: 1.2990\n",
      "Epoch [63/500], Step [1/18], Train_Loss: 1.2425, Valid_Loss: 1.4581\n",
      "Epoch [64/500], Step [1/18], Train_Loss: 1.3902, Valid_Loss: 1.4524\n",
      "Epoch [65/500], Step [1/18], Train_Loss: 1.3006, Valid_Loss: 1.4807\n",
      "Epoch [66/500], Step [1/18], Train_Loss: 1.2070, Valid_Loss: 1.5485\n",
      "Epoch [67/500], Step [1/18], Train_Loss: 1.0914, Valid_Loss: 1.4905\n",
      "Epoch [68/500], Step [1/18], Train_Loss: 1.2460, Valid_Loss: 1.4765\n",
      "Epoch [69/500], Step [1/18], Train_Loss: 1.4644, Valid_Loss: 1.7841\n",
      "Epoch [70/500], Step [1/18], Train_Loss: 1.2293, Valid_Loss: 1.6729\n",
      "Epoch [71/500], Step [1/18], Train_Loss: 1.0408, Valid_Loss: 1.3694\n",
      "Epoch [72/500], Step [1/18], Train_Loss: 0.9024, Valid_Loss: 1.1548\n",
      "Epoch [73/500], Step [1/18], Train_Loss: 1.1399, Valid_Loss: 1.4704\n",
      "Epoch [74/500], Step [1/18], Train_Loss: 1.3167, Valid_Loss: 1.5120\n",
      "Epoch [75/500], Step [1/18], Train_Loss: 1.2737, Valid_Loss: 1.7978\n",
      "Epoch [76/500], Step [1/18], Train_Loss: 1.0396, Valid_Loss: 1.2033\n",
      "Epoch [77/500], Step [1/18], Train_Loss: 1.0400, Valid_Loss: 1.3960\n",
      "Epoch [78/500], Step [1/18], Train_Loss: 1.6372, Valid_Loss: 1.7693\n",
      "Epoch [79/500], Step [1/18], Train_Loss: 1.4825, Valid_Loss: 1.4337\n",
      "Epoch [80/500], Step [1/18], Train_Loss: 1.5751, Valid_Loss: 1.9063\n",
      "Epoch [81/500], Step [1/18], Train_Loss: 1.5289, Valid_Loss: 1.8643\n",
      "Epoch [82/500], Step [1/18], Train_Loss: 0.9659, Valid_Loss: 0.9969\n",
      "Epoch [83/500], Step [1/18], Train_Loss: 1.3665, Valid_Loss: 1.7124\n",
      "Epoch [84/500], Step [1/18], Train_Loss: 1.1493, Valid_Loss: 1.5350\n",
      "Epoch [85/500], Step [1/18], Train_Loss: 1.2109, Valid_Loss: 1.5010\n",
      "Epoch [86/500], Step [1/18], Train_Loss: 1.0351, Valid_Loss: 1.2761\n",
      "Epoch [87/500], Step [1/18], Train_Loss: 1.7677, Valid_Loss: 2.0086\n",
      "Epoch [88/500], Step [1/18], Train_Loss: 0.9725, Valid_Loss: 0.9216\n",
      "Epoch [89/500], Step [1/18], Train_Loss: 1.1151, Valid_Loss: 1.4672\n",
      "Epoch [90/500], Step [1/18], Train_Loss: 0.9848, Valid_Loss: 1.4033\n",
      "Epoch [91/500], Step [1/18], Train_Loss: 1.0917, Valid_Loss: 1.4003\n",
      "Epoch [92/500], Step [1/18], Train_Loss: 0.9192, Valid_Loss: 0.9058\n",
      "Epoch [93/500], Step [1/18], Train_Loss: 1.2040, Valid_Loss: 1.3567\n",
      "Epoch [94/500], Step [1/18], Train_Loss: 1.0286, Valid_Loss: 1.2902\n",
      "Epoch [95/500], Step [1/18], Train_Loss: 1.0027, Valid_Loss: 1.3619\n",
      "Epoch [96/500], Step [1/18], Train_Loss: 1.0679, Valid_Loss: 1.1021\n",
      "Epoch [97/500], Step [1/18], Train_Loss: 0.7723, Valid_Loss: 0.9775\n",
      "Epoch [98/500], Step [1/18], Train_Loss: 1.1125, Valid_Loss: 1.5673\n",
      "Epoch [99/500], Step [1/18], Train_Loss: 0.9054, Valid_Loss: 1.4173\n",
      "Epoch [100/500], Step [1/18], Train_Loss: 0.8563, Valid_Loss: 1.2614\n",
      "Epoch [101/500], Step [1/18], Train_Loss: 1.1487, Valid_Loss: 1.7874\n",
      "Epoch [102/500], Step [1/18], Train_Loss: 1.2256, Valid_Loss: 1.3260\n",
      "Epoch [103/500], Step [1/18], Train_Loss: 1.2263, Valid_Loss: 1.4990\n",
      "Epoch [104/500], Step [1/18], Train_Loss: 0.9275, Valid_Loss: 1.2913\n",
      "Epoch [105/500], Step [1/18], Train_Loss: 1.0900, Valid_Loss: 1.2641\n",
      "Epoch [106/500], Step [1/18], Train_Loss: 1.1348, Valid_Loss: 1.1229\n",
      "Epoch [107/500], Step [1/18], Train_Loss: 0.9743, Valid_Loss: 1.2194\n",
      "Epoch [108/500], Step [1/18], Train_Loss: 1.0227, Valid_Loss: 1.2993\n",
      "Epoch [109/500], Step [1/18], Train_Loss: 0.9186, Valid_Loss: 1.1772\n",
      "Epoch [110/500], Step [1/18], Train_Loss: 1.1659, Valid_Loss: 1.7304\n",
      "Epoch [111/500], Step [1/18], Train_Loss: 0.9643, Valid_Loss: 1.2050\n",
      "Epoch [112/500], Step [1/18], Train_Loss: 0.8796, Valid_Loss: 1.0758\n",
      "Epoch [113/500], Step [1/18], Train_Loss: 1.1497, Valid_Loss: 1.2941\n",
      "Epoch [114/500], Step [1/18], Train_Loss: 0.9608, Valid_Loss: 1.1097\n",
      "Epoch [115/500], Step [1/18], Train_Loss: 0.9943, Valid_Loss: 1.6349\n",
      "Epoch [116/500], Step [1/18], Train_Loss: 0.7807, Valid_Loss: 1.1826\n",
      "Epoch [117/500], Step [1/18], Train_Loss: 0.8921, Valid_Loss: 1.0427\n",
      "Epoch [118/500], Step [1/18], Train_Loss: 0.8638, Valid_Loss: 1.2874\n",
      "Epoch [119/500], Step [1/18], Train_Loss: 1.0026, Valid_Loss: 1.2223\n",
      "Epoch [120/500], Step [1/18], Train_Loss: 0.9372, Valid_Loss: 1.3051\n",
      "Epoch [121/500], Step [1/18], Train_Loss: 0.8828, Valid_Loss: 1.0975\n",
      "Epoch [122/500], Step [1/18], Train_Loss: 1.2181, Valid_Loss: 1.5416\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [123/500], Step [1/18], Train_Loss: 0.9359, Valid_Loss: 1.1462\n",
      "Epoch [124/500], Step [1/18], Train_Loss: 1.1123, Valid_Loss: 1.3895\n",
      "Epoch [125/500], Step [1/18], Train_Loss: 0.8048, Valid_Loss: 1.2964\n",
      "Epoch [126/500], Step [1/18], Train_Loss: 1.1161, Valid_Loss: 1.3777\n",
      "Epoch [127/500], Step [1/18], Train_Loss: 1.0360, Valid_Loss: 1.1782\n",
      "Epoch [128/500], Step [1/18], Train_Loss: 0.9543, Valid_Loss: 1.3801\n",
      "Epoch [129/500], Step [1/18], Train_Loss: 1.0009, Valid_Loss: 1.3306\n",
      "Epoch [130/500], Step [1/18], Train_Loss: 0.7179, Valid_Loss: 1.1020\n",
      "Epoch [131/500], Step [1/18], Train_Loss: 0.8475, Valid_Loss: 1.1381\n",
      "Epoch [132/500], Step [1/18], Train_Loss: 0.7518, Valid_Loss: 0.8494\n",
      "Epoch [133/500], Step [1/18], Train_Loss: 0.7189, Valid_Loss: 0.9718\n",
      "Epoch [134/500], Step [1/18], Train_Loss: 0.8670, Valid_Loss: 1.1537\n",
      "Epoch [135/500], Step [1/18], Train_Loss: 0.6132, Valid_Loss: 0.9571\n",
      "Epoch [136/500], Step [1/18], Train_Loss: 1.0271, Valid_Loss: 1.3703\n",
      "Epoch [137/500], Step [1/18], Train_Loss: 1.0721, Valid_Loss: 1.3090\n",
      "Epoch [138/500], Step [1/18], Train_Loss: 0.8700, Valid_Loss: 1.2421\n",
      "Epoch [139/500], Step [1/18], Train_Loss: 0.8224, Valid_Loss: 1.3292\n",
      "Epoch [140/500], Step [1/18], Train_Loss: 0.6634, Valid_Loss: 0.8002\n",
      "Epoch [141/500], Step [1/18], Train_Loss: 0.8587, Valid_Loss: 1.0318\n",
      "Epoch [142/500], Step [1/18], Train_Loss: 0.8397, Valid_Loss: 1.1478\n",
      "Epoch [143/500], Step [1/18], Train_Loss: 0.7874, Valid_Loss: 1.0996\n",
      "Epoch [144/500], Step [1/18], Train_Loss: 0.8511, Valid_Loss: 1.1384\n",
      "Epoch [145/500], Step [1/18], Train_Loss: 0.8020, Valid_Loss: 1.0286\n",
      "Epoch [146/500], Step [1/18], Train_Loss: 0.7391, Valid_Loss: 1.0330\n",
      "Epoch [147/500], Step [1/18], Train_Loss: 0.8718, Valid_Loss: 1.2014\n",
      "Epoch [148/500], Step [1/18], Train_Loss: 0.7144, Valid_Loss: 1.0415\n",
      "Epoch [149/500], Step [1/18], Train_Loss: 0.8154, Valid_Loss: 1.1067\n",
      "Epoch [150/500], Step [1/18], Train_Loss: 0.8891, Valid_Loss: 1.2338\n",
      "Epoch [151/500], Step [1/18], Train_Loss: 0.7916, Valid_Loss: 1.0036\n",
      "Epoch [152/500], Step [1/18], Train_Loss: 0.7147, Valid_Loss: 1.0462\n",
      "Epoch [153/500], Step [1/18], Train_Loss: 0.6521, Valid_Loss: 0.9508\n",
      "Epoch [154/500], Step [1/18], Train_Loss: 0.9420, Valid_Loss: 1.3076\n",
      "Epoch [155/500], Step [1/18], Train_Loss: 0.8766, Valid_Loss: 1.1353\n",
      "Epoch [156/500], Step [1/18], Train_Loss: 0.6679, Valid_Loss: 0.7917\n",
      "Epoch [157/500], Step [1/18], Train_Loss: 0.7154, Valid_Loss: 0.9323\n",
      "Epoch [158/500], Step [1/18], Train_Loss: 0.6404, Valid_Loss: 0.8762\n",
      "Epoch [159/500], Step [1/18], Train_Loss: 0.8839, Valid_Loss: 1.2226\n",
      "Epoch [160/500], Step [1/18], Train_Loss: 0.9764, Valid_Loss: 1.2553\n",
      "Epoch [161/500], Step [1/18], Train_Loss: 0.7485, Valid_Loss: 0.8468\n",
      "Epoch [162/500], Step [1/18], Train_Loss: 0.7113, Valid_Loss: 0.8326\n",
      "Epoch [163/500], Step [1/18], Train_Loss: 0.9870, Valid_Loss: 1.3163\n",
      "Epoch [164/500], Step [1/18], Train_Loss: 0.6799, Valid_Loss: 1.0504\n",
      "Epoch [165/500], Step [1/18], Train_Loss: 0.4555, Valid_Loss: 0.7094\n",
      "Epoch [166/500], Step [1/18], Train_Loss: 0.6268, Valid_Loss: 0.7261\n",
      "Epoch [167/500], Step [1/18], Train_Loss: 0.6360, Valid_Loss: 0.9844\n",
      "Epoch [168/500], Step [1/18], Train_Loss: 0.7288, Valid_Loss: 1.0841\n",
      "Epoch [169/500], Step [1/18], Train_Loss: 0.5603, Valid_Loss: 0.9101\n",
      "Epoch [170/500], Step [1/18], Train_Loss: 0.5281, Valid_Loss: 0.6344\n",
      "Epoch [171/500], Step [1/18], Train_Loss: 0.7529, Valid_Loss: 1.0493\n",
      "Epoch [172/500], Step [1/18], Train_Loss: 0.7806, Valid_Loss: 1.1383\n",
      "Epoch [173/500], Step [1/18], Train_Loss: 0.5171, Valid_Loss: 0.6184\n",
      "Epoch [174/500], Step [1/18], Train_Loss: 0.6551, Valid_Loss: 0.9904\n",
      "Epoch [175/500], Step [1/18], Train_Loss: 0.5805, Valid_Loss: 0.8490\n",
      "Epoch [176/500], Step [1/18], Train_Loss: 0.4819, Valid_Loss: 0.6866\n",
      "Epoch [177/500], Step [1/18], Train_Loss: 0.5055, Valid_Loss: 0.7868\n",
      "Epoch [178/500], Step [1/18], Train_Loss: 0.7155, Valid_Loss: 0.9074\n",
      "Epoch [179/500], Step [1/18], Train_Loss: 0.6265, Valid_Loss: 0.8272\n",
      "Epoch [180/500], Step [1/18], Train_Loss: 0.7060, Valid_Loss: 1.0010\n",
      "Epoch [181/500], Step [1/18], Train_Loss: 0.5657, Valid_Loss: 0.7246\n",
      "Epoch [182/500], Step [1/18], Train_Loss: 0.6570, Valid_Loss: 0.8447\n",
      "Epoch [183/500], Step [1/18], Train_Loss: 0.7603, Valid_Loss: 0.9213\n",
      "Epoch [184/500], Step [1/18], Train_Loss: 0.5527, Valid_Loss: 0.8009\n",
      "Epoch [185/500], Step [1/18], Train_Loss: 0.7699, Valid_Loss: 1.0710\n",
      "Epoch [186/500], Step [1/18], Train_Loss: 0.6959, Valid_Loss: 0.9701\n",
      "Epoch [187/500], Step [1/18], Train_Loss: 0.7467, Valid_Loss: 0.8131\n",
      "Epoch [188/500], Step [1/18], Train_Loss: 0.5601, Valid_Loss: 0.6715\n",
      "Epoch [189/500], Step [1/18], Train_Loss: 0.6977, Valid_Loss: 1.0886\n",
      "Epoch [190/500], Step [1/18], Train_Loss: 0.6286, Valid_Loss: 0.7777\n",
      "Epoch [191/500], Step [1/18], Train_Loss: 0.6112, Valid_Loss: 0.8997\n",
      "Epoch [192/500], Step [1/18], Train_Loss: 0.5211, Valid_Loss: 0.9190\n",
      "Epoch [193/500], Step [1/18], Train_Loss: 0.5347, Valid_Loss: 0.7619\n",
      "Epoch [194/500], Step [1/18], Train_Loss: 0.5079, Valid_Loss: 0.7036\n",
      "Epoch [195/500], Step [1/18], Train_Loss: 0.6433, Valid_Loss: 0.8583\n",
      "Epoch [196/500], Step [1/18], Train_Loss: 0.4198, Valid_Loss: 0.6534\n",
      "Epoch [197/500], Step [1/18], Train_Loss: 0.4473, Valid_Loss: 0.5588\n",
      "Epoch [198/500], Step [1/18], Train_Loss: 0.7607, Valid_Loss: 1.0441\n",
      "Epoch [199/500], Step [1/18], Train_Loss: 0.5877, Valid_Loss: 0.7180\n",
      "Epoch [200/500], Step [1/18], Train_Loss: 0.4550, Valid_Loss: 0.7821\n",
      "Epoch [201/500], Step [1/18], Train_Loss: 0.4215, Valid_Loss: 0.8195\n",
      "Epoch [202/500], Step [1/18], Train_Loss: 0.5920, Valid_Loss: 0.9155\n",
      "Epoch [203/500], Step [1/18], Train_Loss: 0.5543, Valid_Loss: 0.9480\n",
      "Epoch [204/500], Step [1/18], Train_Loss: 0.5380, Valid_Loss: 0.9003\n",
      "Epoch [205/500], Step [1/18], Train_Loss: 0.4346, Valid_Loss: 0.7109\n",
      "Epoch [206/500], Step [1/18], Train_Loss: 0.3934, Valid_Loss: 0.8365\n",
      "Epoch [207/500], Step [1/18], Train_Loss: 0.3669, Valid_Loss: 0.6423\n",
      "Epoch [208/500], Step [1/18], Train_Loss: 0.4639, Valid_Loss: 0.7592\n",
      "Epoch [209/500], Step [1/18], Train_Loss: 0.4136, Valid_Loss: 0.7076\n",
      "Epoch [210/500], Step [1/18], Train_Loss: 0.5453, Valid_Loss: 0.7763\n",
      "Epoch [211/500], Step [1/18], Train_Loss: 0.4330, Valid_Loss: 0.7989\n",
      "Epoch [212/500], Step [1/18], Train_Loss: 0.5032, Valid_Loss: 0.7881\n",
      "Epoch [213/500], Step [1/18], Train_Loss: 0.4478, Valid_Loss: 0.6429\n",
      "Epoch [214/500], Step [1/18], Train_Loss: 0.3923, Valid_Loss: 0.6475\n",
      "Epoch [215/500], Step [1/18], Train_Loss: 0.5215, Valid_Loss: 0.6713\n",
      "Epoch [216/500], Step [1/18], Train_Loss: 0.2458, Valid_Loss: 0.6472\n",
      "Epoch [217/500], Step [1/18], Train_Loss: 0.3870, Valid_Loss: 0.5676\n",
      "Epoch [218/500], Step [1/18], Train_Loss: 0.3623, Valid_Loss: 0.6047\n",
      "Epoch [219/500], Step [1/18], Train_Loss: 0.4403, Valid_Loss: 0.6424\n",
      "Epoch [220/500], Step [1/18], Train_Loss: 0.3558, Valid_Loss: 0.5990\n",
      "Epoch [221/500], Step [1/18], Train_Loss: 0.4560, Valid_Loss: 0.6264\n",
      "Epoch [222/500], Step [1/18], Train_Loss: 0.3504, Valid_Loss: 0.5169\n",
      "Epoch [223/500], Step [1/18], Train_Loss: 0.3697, Valid_Loss: 0.4472\n",
      "Epoch [224/500], Step [1/18], Train_Loss: 0.3682, Valid_Loss: 0.4562\n",
      "Epoch [225/500], Step [1/18], Train_Loss: 0.3389, Valid_Loss: 0.6461\n",
      "Epoch [226/500], Step [1/18], Train_Loss: 0.2696, Valid_Loss: 0.4828\n",
      "Epoch [227/500], Step [1/18], Train_Loss: 0.3243, Valid_Loss: 0.6365\n",
      "Epoch [228/500], Step [1/18], Train_Loss: 0.4031, Valid_Loss: 0.6667\n",
      "Epoch [229/500], Step [1/18], Train_Loss: 0.2757, Valid_Loss: 0.5418\n",
      "Epoch [230/500], Step [1/18], Train_Loss: 0.3037, Valid_Loss: 0.6124\n",
      "Epoch [231/500], Step [1/18], Train_Loss: 0.4013, Valid_Loss: 0.6337\n",
      "Epoch [232/500], Step [1/18], Train_Loss: 0.3372, Valid_Loss: 0.5555\n",
      "Epoch [233/500], Step [1/18], Train_Loss: 0.3820, Valid_Loss: 0.7044\n",
      "Epoch [234/500], Step [1/18], Train_Loss: 0.3463, Valid_Loss: 0.6331\n",
      "Epoch [235/500], Step [1/18], Train_Loss: 0.2274, Valid_Loss: 0.4387\n",
      "Epoch [236/500], Step [1/18], Train_Loss: 0.3746, Valid_Loss: 0.6441\n",
      "Epoch [237/500], Step [1/18], Train_Loss: 0.3619, Valid_Loss: 0.6046\n",
      "Epoch [238/500], Step [1/18], Train_Loss: 0.3571, Valid_Loss: 0.5572\n",
      "Epoch [239/500], Step [1/18], Train_Loss: 0.2930, Valid_Loss: 0.4687\n",
      "Epoch [240/500], Step [1/18], Train_Loss: 0.4002, Valid_Loss: 0.6501\n",
      "Epoch [241/500], Step [1/18], Train_Loss: 0.1844, Valid_Loss: 0.4121\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [242/500], Step [1/18], Train_Loss: 0.3975, Valid_Loss: 0.6996\n",
      "Epoch [243/500], Step [1/18], Train_Loss: 0.2359, Valid_Loss: 0.4711\n",
      "Epoch [244/500], Step [1/18], Train_Loss: 0.3501, Valid_Loss: 0.6716\n",
      "Epoch [245/500], Step [1/18], Train_Loss: 0.2575, Valid_Loss: 0.4919\n",
      "Epoch [246/500], Step [1/18], Train_Loss: 0.3289, Valid_Loss: 0.5457\n",
      "Epoch [247/500], Step [1/18], Train_Loss: 0.2436, Valid_Loss: 0.5504\n",
      "Epoch [248/500], Step [1/18], Train_Loss: 0.2833, Valid_Loss: 0.4561\n",
      "Epoch [249/500], Step [1/18], Train_Loss: 0.2941, Valid_Loss: 0.4964\n",
      "Epoch [250/500], Step [1/18], Train_Loss: 0.2270, Valid_Loss: 0.3971\n",
      "Epoch [251/500], Step [1/18], Train_Loss: 0.2965, Valid_Loss: 0.5358\n",
      "Epoch [252/500], Step [1/18], Train_Loss: 0.1902, Valid_Loss: 0.4176\n",
      "Epoch [253/500], Step [1/18], Train_Loss: 0.2751, Valid_Loss: 0.5074\n",
      "Epoch [254/500], Step [1/18], Train_Loss: 0.2240, Valid_Loss: 0.5642\n",
      "Epoch [255/500], Step [1/18], Train_Loss: 0.2296, Valid_Loss: 0.4254\n",
      "Epoch [256/500], Step [1/18], Train_Loss: 0.3967, Valid_Loss: 0.5017\n",
      "Epoch [257/500], Step [1/18], Train_Loss: 0.2603, Valid_Loss: 0.4066\n",
      "Epoch [258/500], Step [1/18], Train_Loss: 0.2336, Valid_Loss: 0.5004\n",
      "Epoch [259/500], Step [1/18], Train_Loss: 0.2008, Valid_Loss: 0.6068\n",
      "Epoch [260/500], Step [1/18], Train_Loss: 0.2187, Valid_Loss: 0.4250\n",
      "Epoch [261/500], Step [1/18], Train_Loss: 0.2035, Valid_Loss: 0.4680\n",
      "Epoch [262/500], Step [1/18], Train_Loss: 0.2204, Valid_Loss: 0.4510\n",
      "Epoch [263/500], Step [1/18], Train_Loss: 0.1584, Valid_Loss: 0.5271\n",
      "Epoch [264/500], Step [1/18], Train_Loss: 0.2506, Valid_Loss: 0.6477\n",
      "Epoch [265/500], Step [1/18], Train_Loss: 0.1733, Valid_Loss: 0.3746\n",
      "Epoch [266/500], Step [1/18], Train_Loss: 0.2307, Valid_Loss: 0.5070\n",
      "Epoch [267/500], Step [1/18], Train_Loss: 0.2741, Valid_Loss: 0.4949\n",
      "Epoch [268/500], Step [1/18], Train_Loss: 0.2050, Valid_Loss: 0.4552\n",
      "Epoch [269/500], Step [1/18], Train_Loss: 0.1643, Valid_Loss: 0.5222\n",
      "Epoch [270/500], Step [1/18], Train_Loss: 0.1609, Valid_Loss: 0.4504\n",
      "Epoch [271/500], Step [1/18], Train_Loss: 0.2137, Valid_Loss: 0.3323\n",
      "Epoch [272/500], Step [1/18], Train_Loss: 0.2265, Valid_Loss: 0.4903\n",
      "Epoch [273/500], Step [1/18], Train_Loss: 0.1309, Valid_Loss: 0.4862\n",
      "Epoch [274/500], Step [1/18], Train_Loss: 0.1849, Valid_Loss: 0.3883\n",
      "Epoch [275/500], Step [1/18], Train_Loss: 0.1863, Valid_Loss: 0.4203\n",
      "Epoch [276/500], Step [1/18], Train_Loss: 0.1396, Valid_Loss: 0.3808\n",
      "Epoch [277/500], Step [1/18], Train_Loss: 0.2115, Valid_Loss: 0.4787\n",
      "Epoch [278/500], Step [1/18], Train_Loss: 0.1696, Valid_Loss: 0.4322\n",
      "Epoch [279/500], Step [1/18], Train_Loss: 0.1096, Valid_Loss: 0.3558\n",
      "Epoch [280/500], Step [1/18], Train_Loss: 0.1968, Valid_Loss: 0.3713\n",
      "Epoch [281/500], Step [1/18], Train_Loss: 0.1809, Valid_Loss: 0.3072\n",
      "Epoch [282/500], Step [1/18], Train_Loss: 0.1282, Valid_Loss: 0.3071\n",
      "Epoch [283/500], Step [1/18], Train_Loss: 0.1464, Valid_Loss: 0.3549\n",
      "Epoch [284/500], Step [1/18], Train_Loss: 0.1428, Valid_Loss: 0.4644\n",
      "Epoch [285/500], Step [1/18], Train_Loss: 0.1501, Valid_Loss: 0.4394\n",
      "Epoch [286/500], Step [1/18], Train_Loss: 0.0878, Valid_Loss: 0.3607\n",
      "Epoch [287/500], Step [1/18], Train_Loss: 0.1231, Valid_Loss: 0.3789\n",
      "Epoch [288/500], Step [1/18], Train_Loss: 0.1306, Valid_Loss: 0.4051\n",
      "Epoch [289/500], Step [1/18], Train_Loss: 0.1348, Valid_Loss: 0.4212\n",
      "Epoch [290/500], Step [1/18], Train_Loss: 0.1432, Valid_Loss: 0.3679\n",
      "Epoch [291/500], Step [1/18], Train_Loss: 0.1041, Valid_Loss: 0.3578\n",
      "Epoch [292/500], Step [1/18], Train_Loss: 0.1552, Valid_Loss: 0.3382\n",
      "Epoch [293/500], Step [1/18], Train_Loss: 0.0893, Valid_Loss: 0.3809\n",
      "Epoch [294/500], Step [1/18], Train_Loss: 0.1078, Valid_Loss: 0.4746\n",
      "Epoch [295/500], Step [1/18], Train_Loss: 0.1513, Valid_Loss: 0.3331\n",
      "Epoch [296/500], Step [1/18], Train_Loss: 0.1191, Valid_Loss: 0.4174\n",
      "Epoch [297/500], Step [1/18], Train_Loss: 0.1303, Valid_Loss: 0.3791\n",
      "Epoch [298/500], Step [1/18], Train_Loss: 0.0989, Valid_Loss: 0.4063\n",
      "Epoch [299/500], Step [1/18], Train_Loss: 0.1084, Valid_Loss: 0.3263\n",
      "Epoch [300/500], Step [1/18], Train_Loss: 0.0621, Valid_Loss: 0.2951\n",
      "Epoch [301/500], Step [1/18], Train_Loss: 0.1050, Valid_Loss: 0.4023\n",
      "Epoch [302/500], Step [1/18], Train_Loss: 0.1035, Valid_Loss: 0.3107\n",
      "Epoch [303/500], Step [1/18], Train_Loss: 0.1058, Valid_Loss: 0.3740\n",
      "Epoch [304/500], Step [1/18], Train_Loss: 0.1132, Valid_Loss: 0.3314\n",
      "Epoch [305/500], Step [1/18], Train_Loss: 0.0748, Valid_Loss: 0.2820\n",
      "Epoch [306/500], Step [1/18], Train_Loss: 0.0962, Valid_Loss: 0.3100\n",
      "Epoch [307/500], Step [1/18], Train_Loss: 0.0790, Valid_Loss: 0.2982\n",
      "Epoch [308/500], Step [1/18], Train_Loss: 0.1246, Valid_Loss: 0.3634\n",
      "Epoch [309/500], Step [1/18], Train_Loss: 0.0970, Valid_Loss: 0.3163\n",
      "Epoch [310/500], Step [1/18], Train_Loss: 0.0885, Valid_Loss: 0.3080\n",
      "Epoch [311/500], Step [1/18], Train_Loss: 0.1270, Valid_Loss: 0.2952\n",
      "Epoch [312/500], Step [1/18], Train_Loss: 0.0817, Valid_Loss: 0.3456\n",
      "Epoch [313/500], Step [1/18], Train_Loss: 0.0960, Valid_Loss: 0.3235\n",
      "Epoch [314/500], Step [1/18], Train_Loss: 0.0793, Valid_Loss: 0.3109\n",
      "Epoch [315/500], Step [1/18], Train_Loss: 0.0721, Valid_Loss: 0.3037\n",
      "Epoch [316/500], Step [1/18], Train_Loss: 0.0756, Valid_Loss: 0.3374\n",
      "Epoch [317/500], Step [1/18], Train_Loss: 0.0781, Valid_Loss: 0.3025\n",
      "Epoch [318/500], Step [1/18], Train_Loss: 0.0613, Valid_Loss: 0.2873\n",
      "Epoch [319/500], Step [1/18], Train_Loss: 0.0705, Valid_Loss: 0.3429\n",
      "Epoch [320/500], Step [1/18], Train_Loss: 0.0581, Valid_Loss: 0.3649\n",
      "Epoch [321/500], Step [1/18], Train_Loss: 0.0708, Valid_Loss: 0.3616\n",
      "Epoch [322/500], Step [1/18], Train_Loss: 0.0462, Valid_Loss: 0.3290\n",
      "Epoch [323/500], Step [1/18], Train_Loss: 0.0823, Valid_Loss: 0.3451\n",
      "Epoch [324/500], Step [1/18], Train_Loss: 0.0768, Valid_Loss: 0.4296\n",
      "Epoch [325/500], Step [1/18], Train_Loss: 0.0637, Valid_Loss: 0.2889\n",
      "Epoch [326/500], Step [1/18], Train_Loss: 0.0806, Valid_Loss: 0.2447\n",
      "Epoch [327/500], Step [1/18], Train_Loss: 0.0701, Valid_Loss: 0.2642\n",
      "Epoch [328/500], Step [1/18], Train_Loss: 0.0664, Valid_Loss: 0.3281\n",
      "Epoch [329/500], Step [1/18], Train_Loss: 0.0966, Valid_Loss: 0.3962\n",
      "Epoch [330/500], Step [1/18], Train_Loss: 0.0627, Valid_Loss: 0.3213\n",
      "Epoch [331/500], Step [1/18], Train_Loss: 0.0570, Valid_Loss: 0.3436\n",
      "Epoch [332/500], Step [1/18], Train_Loss: 0.0589, Valid_Loss: 0.2838\n",
      "Epoch [333/500], Step [1/18], Train_Loss: 0.0408, Valid_Loss: 0.3099\n",
      "Epoch [334/500], Step [1/18], Train_Loss: 0.0355, Valid_Loss: 0.3341\n",
      "Epoch [335/500], Step [1/18], Train_Loss: 0.0479, Valid_Loss: 0.3046\n",
      "Epoch [336/500], Step [1/18], Train_Loss: 0.0540, Valid_Loss: 0.3359\n",
      "Epoch [337/500], Step [1/18], Train_Loss: 0.0567, Valid_Loss: 0.3163\n",
      "Epoch [338/500], Step [1/18], Train_Loss: 0.0519, Valid_Loss: 0.3119\n",
      "Epoch [339/500], Step [1/18], Train_Loss: 0.0425, Valid_Loss: 0.3050\n",
      "Epoch [340/500], Step [1/18], Train_Loss: 0.0587, Valid_Loss: 0.3014\n",
      "Epoch [341/500], Step [1/18], Train_Loss: 0.0483, Valid_Loss: 0.3408\n",
      "Epoch [342/500], Step [1/18], Train_Loss: 0.0425, Valid_Loss: 0.2698\n",
      "Epoch [343/500], Step [1/18], Train_Loss: 0.0511, Valid_Loss: 0.3123\n",
      "Epoch [344/500], Step [1/18], Train_Loss: 0.0359, Valid_Loss: 0.2937\n",
      "Epoch [345/500], Step [1/18], Train_Loss: 0.0359, Valid_Loss: 0.2911\n",
      "Epoch [346/500], Step [1/18], Train_Loss: 0.0600, Valid_Loss: 0.3063\n",
      "Epoch [347/500], Step [1/18], Train_Loss: 0.0377, Valid_Loss: 0.3160\n",
      "Epoch [348/500], Step [1/18], Train_Loss: 0.0447, Valid_Loss: 0.2676\n",
      "Epoch [349/500], Step [1/18], Train_Loss: 0.0448, Valid_Loss: 0.3090\n",
      "Epoch [350/500], Step [1/18], Train_Loss: 0.0366, Valid_Loss: 0.2713\n",
      "Epoch [351/500], Step [1/18], Train_Loss: 0.0466, Valid_Loss: 0.2974\n",
      "Epoch [352/500], Step [1/18], Train_Loss: 0.0314, Valid_Loss: 0.3199\n",
      "Epoch [353/500], Step [1/18], Train_Loss: 0.0531, Valid_Loss: 0.3196\n",
      "Epoch [354/500], Step [1/18], Train_Loss: 0.0253, Valid_Loss: 0.2815\n",
      "Epoch [355/500], Step [1/18], Train_Loss: 0.0292, Valid_Loss: 0.2936\n",
      "Epoch [356/500], Step [1/18], Train_Loss: 0.0493, Valid_Loss: 0.3292\n",
      "Epoch [357/500], Step [1/18], Train_Loss: 0.0438, Valid_Loss: 0.2890\n",
      "Epoch [358/500], Step [1/18], Train_Loss: 0.0408, Valid_Loss: 0.3110\n",
      "Epoch [359/500], Step [1/18], Train_Loss: 0.0337, Valid_Loss: 0.2810\n",
      "Epoch [360/500], Step [1/18], Train_Loss: 0.0402, Valid_Loss: 0.2839\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [361/500], Step [1/18], Train_Loss: 0.0389, Valid_Loss: 0.2834\n",
      "Epoch [362/500], Step [1/18], Train_Loss: 0.0358, Valid_Loss: 0.2711\n",
      "Epoch [363/500], Step [1/18], Train_Loss: 0.0293, Valid_Loss: 0.2703\n",
      "Epoch [364/500], Step [1/18], Train_Loss: 0.0342, Valid_Loss: 0.2968\n",
      "Epoch [365/500], Step [1/18], Train_Loss: 0.0314, Valid_Loss: 0.3301\n",
      "Epoch [366/500], Step [1/18], Train_Loss: 0.0326, Valid_Loss: 0.2988\n",
      "Epoch [367/500], Step [1/18], Train_Loss: 0.0322, Valid_Loss: 0.2659\n",
      "Epoch [368/500], Step [1/18], Train_Loss: 0.0321, Valid_Loss: 0.2978\n",
      "Epoch [369/500], Step [1/18], Train_Loss: 0.0327, Valid_Loss: 0.2766\n",
      "Epoch [370/500], Step [1/18], Train_Loss: 0.0296, Valid_Loss: 0.2894\n",
      "Epoch [371/500], Step [1/18], Train_Loss: 0.0253, Valid_Loss: 0.2806\n",
      "Epoch [372/500], Step [1/18], Train_Loss: 0.0223, Valid_Loss: 0.2808\n",
      "Epoch [373/500], Step [1/18], Train_Loss: 0.0324, Valid_Loss: 0.2757\n",
      "Epoch [374/500], Step [1/18], Train_Loss: 0.0266, Valid_Loss: 0.2778\n",
      "Epoch [375/500], Step [1/18], Train_Loss: 0.0314, Valid_Loss: 0.3298\n",
      "Epoch [376/500], Step [1/18], Train_Loss: 0.0242, Valid_Loss: 0.2568\n",
      "Epoch [377/500], Step [1/18], Train_Loss: 0.0272, Valid_Loss: 0.2971\n",
      "Epoch [378/500], Step [1/18], Train_Loss: 0.0246, Valid_Loss: 0.2715\n",
      "Epoch [379/500], Step [1/18], Train_Loss: 0.0380, Valid_Loss: 0.2645\n",
      "Epoch [380/500], Step [1/18], Train_Loss: 0.0193, Valid_Loss: 0.2877\n",
      "Epoch [381/500], Step [1/18], Train_Loss: 0.0188, Valid_Loss: 0.2878\n",
      "Epoch [382/500], Step [1/18], Train_Loss: 0.0228, Valid_Loss: 0.2999\n",
      "Epoch [383/500], Step [1/18], Train_Loss: 0.0280, Valid_Loss: 0.2591\n",
      "Epoch [384/500], Step [1/18], Train_Loss: 0.0305, Valid_Loss: 0.2843\n",
      "Epoch [385/500], Step [1/18], Train_Loss: 0.0287, Valid_Loss: 0.2758\n",
      "Epoch [386/500], Step [1/18], Train_Loss: 0.0199, Valid_Loss: 0.2815\n",
      "Epoch [387/500], Step [1/18], Train_Loss: 0.0234, Valid_Loss: 0.2444\n",
      "Epoch [388/500], Step [1/18], Train_Loss: 0.0246, Valid_Loss: 0.2474\n",
      "Epoch [389/500], Step [1/18], Train_Loss: 0.0205, Valid_Loss: 0.3005\n",
      "Epoch [390/500], Step [1/18], Train_Loss: 0.0386, Valid_Loss: 0.2631\n",
      "Epoch [391/500], Step [1/18], Train_Loss: 0.0235, Valid_Loss: 0.2683\n",
      "Epoch [392/500], Step [1/18], Train_Loss: 0.0171, Valid_Loss: 0.2775\n",
      "Epoch [393/500], Step [1/18], Train_Loss: 0.0227, Valid_Loss: 0.2720\n",
      "Epoch [394/500], Step [1/18], Train_Loss: 0.0327, Valid_Loss: 0.2592\n",
      "Epoch [395/500], Step [1/18], Train_Loss: 0.0263, Valid_Loss: 0.2872\n",
      "Epoch [396/500], Step [1/18], Train_Loss: 0.0178, Valid_Loss: 0.2721\n",
      "Epoch [397/500], Step [1/18], Train_Loss: 0.0248, Valid_Loss: 0.2678\n",
      "Epoch [398/500], Step [1/18], Train_Loss: 0.0258, Valid_Loss: 0.2536\n",
      "Epoch [399/500], Step [1/18], Train_Loss: 0.0226, Valid_Loss: 0.2755\n",
      "Epoch [400/500], Step [1/18], Train_Loss: 0.0224, Valid_Loss: 0.2644\n",
      "Epoch [401/500], Step [1/18], Train_Loss: 0.0232, Valid_Loss: 0.2616\n",
      "Epoch [402/500], Step [1/18], Train_Loss: 0.0283, Valid_Loss: 0.2567\n",
      "Epoch [403/500], Step [1/18], Train_Loss: 0.0273, Valid_Loss: 0.2531\n",
      "Epoch [404/500], Step [1/18], Train_Loss: 0.0322, Valid_Loss: 0.2536\n",
      "Epoch [405/500], Step [1/18], Train_Loss: 0.0262, Valid_Loss: 0.2470\n",
      "Epoch [406/500], Step [1/18], Train_Loss: 0.0295, Valid_Loss: 0.2440\n",
      "Epoch [407/500], Step [1/18], Train_Loss: 0.0327, Valid_Loss: 0.2504\n",
      "Epoch [408/500], Step [1/18], Train_Loss: 0.0286, Valid_Loss: 0.2615\n",
      "Epoch [409/500], Step [1/18], Train_Loss: 0.0226, Valid_Loss: 0.2683\n",
      "Epoch [410/500], Step [1/18], Train_Loss: 0.0217, Valid_Loss: 0.2654\n",
      "Epoch [411/500], Step [1/18], Train_Loss: 0.0314, Valid_Loss: 0.2531\n",
      "Epoch [412/500], Step [1/18], Train_Loss: 0.0378, Valid_Loss: 0.2593\n",
      "Epoch [413/500], Step [1/18], Train_Loss: 0.0303, Valid_Loss: 0.2457\n",
      "Epoch [414/500], Step [1/18], Train_Loss: 0.0241, Valid_Loss: 0.2705\n",
      "Epoch [415/500], Step [1/18], Train_Loss: 0.0224, Valid_Loss: 0.2651\n",
      "Epoch [416/500], Step [1/18], Train_Loss: 0.0270, Valid_Loss: 0.2618\n",
      "Epoch [417/500], Step [1/18], Train_Loss: 0.0219, Valid_Loss: 0.2772\n",
      "Epoch [418/500], Step [1/18], Train_Loss: 0.0202, Valid_Loss: 0.2688\n",
      "Epoch [419/500], Step [1/18], Train_Loss: 0.0194, Valid_Loss: 0.2553\n",
      "Epoch [420/500], Step [1/18], Train_Loss: 0.0239, Valid_Loss: 0.2547\n",
      "Epoch [421/500], Step [1/18], Train_Loss: 0.0192, Valid_Loss: 0.2869\n",
      "Epoch [422/500], Step [1/18], Train_Loss: 0.0164, Valid_Loss: 0.2635\n",
      "Epoch [423/500], Step [1/18], Train_Loss: 0.0203, Valid_Loss: 0.2629\n",
      "Epoch [424/500], Step [1/18], Train_Loss: 0.0179, Valid_Loss: 0.2556\n",
      "Epoch [425/500], Step [1/18], Train_Loss: 0.0187, Valid_Loss: 0.2560\n",
      "Epoch [426/500], Step [1/18], Train_Loss: 0.0202, Valid_Loss: 0.2557\n",
      "Epoch [427/500], Step [1/18], Train_Loss: 0.0193, Valid_Loss: 0.2795\n",
      "Epoch [428/500], Step [1/18], Train_Loss: 0.0159, Valid_Loss: 0.2617\n",
      "Epoch [429/500], Step [1/18], Train_Loss: 0.0209, Valid_Loss: 0.2745\n",
      "Epoch [430/500], Step [1/18], Train_Loss: 0.0158, Valid_Loss: 0.2761\n",
      "Epoch [431/500], Step [1/18], Train_Loss: 0.0209, Valid_Loss: 0.2508\n",
      "Epoch [432/500], Step [1/18], Train_Loss: 0.0145, Valid_Loss: 0.2887\n",
      "Epoch [433/500], Step [1/18], Train_Loss: 0.0155, Valid_Loss: 0.2846\n",
      "Epoch [434/500], Step [1/18], Train_Loss: 0.0195, Valid_Loss: 0.2910\n",
      "Epoch [435/500], Step [1/18], Train_Loss: 0.0136, Valid_Loss: 0.2901\n",
      "Epoch [436/500], Step [1/18], Train_Loss: 0.0161, Valid_Loss: 0.2662\n",
      "Epoch [437/500], Step [1/18], Train_Loss: 0.0159, Valid_Loss: 0.2896\n",
      "Epoch [438/500], Step [1/18], Train_Loss: 0.0192, Valid_Loss: 0.2836\n",
      "Epoch [439/500], Step [1/18], Train_Loss: 0.0152, Valid_Loss: 0.2870\n",
      "Epoch [440/500], Step [1/18], Train_Loss: 0.0104, Valid_Loss: 0.2660\n",
      "Epoch [441/500], Step [1/18], Train_Loss: 0.0127, Valid_Loss: 0.2820\n",
      "Epoch [442/500], Step [1/18], Train_Loss: 0.0183, Valid_Loss: 0.2772\n",
      "Epoch [443/500], Step [1/18], Train_Loss: 0.0164, Valid_Loss: 0.2718\n",
      "Epoch [444/500], Step [1/18], Train_Loss: 0.0114, Valid_Loss: 0.2992\n",
      "Epoch [445/500], Step [1/18], Train_Loss: 0.0167, Valid_Loss: 0.2909\n",
      "Epoch [446/500], Step [1/18], Train_Loss: 0.0146, Valid_Loss: 0.2711\n",
      "Epoch [447/500], Step [1/18], Train_Loss: 0.0154, Valid_Loss: 0.2690\n",
      "Epoch [448/500], Step [1/18], Train_Loss: 0.0174, Valid_Loss: 0.2866\n",
      "Epoch [449/500], Step [1/18], Train_Loss: 0.0109, Valid_Loss: 0.2807\n",
      "Epoch [450/500], Step [1/18], Train_Loss: 0.0162, Valid_Loss: 0.2778\n",
      "Epoch [451/500], Step [1/18], Train_Loss: 0.0185, Valid_Loss: 0.2678\n",
      "Epoch [452/500], Step [1/18], Train_Loss: 0.0159, Valid_Loss: 0.3164\n",
      "Epoch [453/500], Step [1/18], Train_Loss: 0.0130, Valid_Loss: 0.2815\n",
      "Epoch [454/500], Step [1/18], Train_Loss: 0.0154, Valid_Loss: 0.2825\n",
      "Epoch [455/500], Step [1/18], Train_Loss: 0.0119, Valid_Loss: 0.2917\n",
      "Epoch [456/500], Step [1/18], Train_Loss: 0.0149, Valid_Loss: 0.2862\n",
      "Epoch [457/500], Step [1/18], Train_Loss: 0.0172, Valid_Loss: 0.2885\n",
      "Epoch [458/500], Step [1/18], Train_Loss: 0.0144, Valid_Loss: 0.2888\n",
      "Epoch [459/500], Step [1/18], Train_Loss: 0.0127, Valid_Loss: 0.2890\n",
      "Epoch [460/500], Step [1/18], Train_Loss: 0.0126, Valid_Loss: 0.2893\n",
      "Epoch [461/500], Step [1/18], Train_Loss: 0.0104, Valid_Loss: 0.2840\n",
      "Epoch [462/500], Step [1/18], Train_Loss: 0.0144, Valid_Loss: 0.3015\n",
      "Epoch [463/500], Step [1/18], Train_Loss: 0.0133, Valid_Loss: 0.2896\n",
      "Epoch [464/500], Step [1/18], Train_Loss: 0.0159, Valid_Loss: 0.2894\n",
      "Epoch [465/500], Step [1/18], Train_Loss: 0.0265, Valid_Loss: 0.2710\n",
      "Epoch [466/500], Step [1/18], Train_Loss: 0.0189, Valid_Loss: 0.2877\n",
      "Epoch [467/500], Step [1/18], Train_Loss: 0.0141, Valid_Loss: 0.2760\n",
      "Epoch [468/500], Step [1/18], Train_Loss: 0.0210, Valid_Loss: 0.2867\n",
      "Epoch [469/500], Step [1/18], Train_Loss: 0.0131, Valid_Loss: 0.2798\n",
      "Epoch [470/500], Step [1/18], Train_Loss: 0.0136, Valid_Loss: 0.2736\n",
      "Epoch [471/500], Step [1/18], Train_Loss: 0.0121, Valid_Loss: 0.3033\n",
      "Epoch [472/500], Step [1/18], Train_Loss: 0.0122, Valid_Loss: 0.3036\n",
      "Epoch [473/500], Step [1/18], Train_Loss: 0.0179, Valid_Loss: 0.2665\n",
      "Epoch [474/500], Step [1/18], Train_Loss: 0.0194, Valid_Loss: 0.2649\n",
      "Epoch [475/500], Step [1/18], Train_Loss: 0.0160, Valid_Loss: 0.3066\n",
      "Epoch [476/500], Step [1/18], Train_Loss: 0.0129, Valid_Loss: 0.2786\n",
      "Epoch [477/500], Step [1/18], Train_Loss: 0.0154, Valid_Loss: 0.2824\n",
      "Epoch [478/500], Step [1/18], Train_Loss: 0.0156, Valid_Loss: 0.2875\n",
      "Epoch [479/500], Step [1/18], Train_Loss: 0.0160, Valid_Loss: 0.2912\n",
      "Epoch [480/500], Step [1/18], Train_Loss: 0.0124, Valid_Loss: 0.2943\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [481/500], Step [1/18], Train_Loss: 0.0103, Valid_Loss: 0.2983\n",
      "Epoch [482/500], Step [1/18], Train_Loss: 0.0141, Valid_Loss: 0.3091\n",
      "Epoch [483/500], Step [1/18], Train_Loss: 0.0108, Valid_Loss: 0.3084\n",
      "Epoch [484/500], Step [1/18], Train_Loss: 0.0179, Valid_Loss: 0.2985\n",
      "Epoch [485/500], Step [1/18], Train_Loss: 0.0116, Valid_Loss: 0.2905\n",
      "Epoch [486/500], Step [1/18], Train_Loss: 0.0139, Valid_Loss: 0.3000\n",
      "Epoch [487/500], Step [1/18], Train_Loss: 0.0119, Valid_Loss: 0.3123\n",
      "Epoch [488/500], Step [1/18], Train_Loss: 0.0100, Valid_Loss: 0.3030\n",
      "Epoch [489/500], Step [1/18], Train_Loss: 0.0118, Valid_Loss: 0.2926\n",
      "Epoch [490/500], Step [1/18], Train_Loss: 0.0133, Valid_Loss: 0.2852\n",
      "Epoch [491/500], Step [1/18], Train_Loss: 0.0088, Valid_Loss: 0.2854\n",
      "Epoch [492/500], Step [1/18], Train_Loss: 0.0180, Valid_Loss: 0.3190\n",
      "Epoch [493/500], Step [1/18], Train_Loss: 0.0133, Valid_Loss: 0.2711\n",
      "Epoch [494/500], Step [1/18], Train_Loss: 0.0086, Valid_Loss: 0.3080\n",
      "Epoch [495/500], Step [1/18], Train_Loss: 0.0118, Valid_Loss: 0.3044\n",
      "Epoch [496/500], Step [1/18], Train_Loss: 0.0134, Valid_Loss: 0.3016\n",
      "Epoch [497/500], Step [1/18], Train_Loss: 0.0164, Valid_Loss: 0.2966\n",
      "Epoch [498/500], Step [1/18], Train_Loss: 0.0083, Valid_Loss: 0.2999\n",
      "Epoch [499/500], Step [1/18], Train_Loss: 0.0156, Valid_Loss: 0.3006\n",
      "Epoch [500/500], Step [1/18], Train_Loss: 0.0109, Valid_Loss: 0.3012\n",
      "done!\n"
     ]
    }
   ],
   "source": [
    "# Train the model\n",
    "total_step = int(len(x_train_conts) / batch_size)\n",
    "for epoch in range(epochs):\n",
    "    for i, (x_conts,x_cats,y) in enumerate(train_loader):\n",
    "        # Forward pass\n",
    "        outputs = model(x_cats,x_conts)\n",
    "        loss = criterion(outputs, y[:,None].float())\n",
    "        \n",
    "        # Backward and optimize\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        \n",
    "        if (i) % 100 == 0:\n",
    "            for i2, (x2_conts,x2_cats,y2) in enumerate(valid_loader):\n",
    "                loss2 = criterion(outputs, y2[:,None].float())\n",
    "                break\n",
    "            print ('Epoch [{}/{}], Step [{}/{}], Train_Loss: {:.4f}, Valid_Loss: {:.4f}' \n",
    "                   .format(epoch+1, epochs, i+1, total_step, loss.item(), loss2.item()))\n",
    "print(\"done!\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For some reason this is not even close what I got from Kaggle."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loss: tensor(0.1172)\n"
     ]
    }
   ],
   "source": [
    "# Test the model\n",
    "model.eval()\n",
    "total = 0\n",
    "nums = 0\n",
    "with torch.no_grad():\n",
    "    for x_conts, x_cats, y in test_loader:\n",
    "        outputs = model(x_cats,x_conts)\n",
    "        \n",
    "        y = y[:,None].float().reshape((-1,1))\n",
    "        loss = criterion(outputs, y.float())\n",
    "        nums += 1\n",
    "        total += loss\n",
    "\n",
    "    print('Loss:',total/nums)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
